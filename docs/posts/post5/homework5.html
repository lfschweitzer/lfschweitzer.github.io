<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>My Awesome CSCI 0451 Blog -  Implementing Logistic Regression </title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Me!</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title"><u> Implementing Logistic Regression </u></h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="implementing-logistic-regression" class="level1">
<h1><u> Implementing Logistic Regression </u></h1>
<div id="cell-1" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> logistic <span class="im">import</span> LogisticRegression, GradientDescentOptimizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Link to source code (<a href="https://github.com/lfschweitzer/lfschweitzer.github.io/blob/23d98daa9a6f335274641d964a13be5ba28f66f9/posts/post5/logistic.py">logistic.py</a>)</p>
<p>For this blog post I implemented logistic regression and performed several experiments on my model. The first experiment I conducted was to see vanilla gradient descent. Vanilla gradient descent is when beta=0 and we were able to know that the model works because we saw loss decrease monotonically. The second experiment was to understand the benefits of momentum. By increasing the beta value we were able to see loss decrease faster than the vanilla gradient descent case. Then, finally, we conducted an experiment to see the potential harms of overfitting our data. By reaching 100% accuracy on training data, we could see the drawback directly by a lower accuracy rate on training data. Overall, I was able to learn more about implementing machine learning models and how to test their functionality. I was able to concretely understand the benefits of momentum and the drawbacks of overfitting.</p>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p>Before doing any experiments, I had to generate data for a classification problem.</p>
<div id="cell-7" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classification_data(n_points <span class="op">=</span> <span class="dv">300</span>, noise <span class="op">=</span> <span class="fl">0.2</span>, p_dims <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.arange(n_points) <span class="op">&gt;=</span> <span class="bu">int</span>(n_points<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fl">1.0</span><span class="op">*</span>y</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> y[:, <span class="va">None</span>] <span class="op">+</span> torch.normal(<span class="fl">0.0</span>, noise, size <span class="op">=</span> (n_points,p_dims))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.cat((X, torch.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>))), <span class="dv">1</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Plot the data</p>
<div id="cell-9" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_classification_data(X, y, ax):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    markers <span class="op">=</span> [<span class="st">"o"</span> , <span class="st">","</span>]</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> y <span class="op">==</span> targets[i]</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[ix,<span class="dv">0</span>], X[ix,<span class="dv">1</span>], s <span class="op">=</span> <span class="dv">20</span>,  c <span class="op">=</span> y[ix], facecolors <span class="op">=</span> <span class="st">"none"</span>, edgecolors <span class="op">=</span> <span class="st">"darkgrey"</span>, cmap <span class="op">=</span> <span class="st">"BrBG"</span>, vmin <span class="op">=</span> <span class="op">-</span><span class="dv">2</span>, vmax <span class="op">=</span> <span class="dv">2</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>, marker <span class="op">=</span> markers[i])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$x_1$"</span>, ylabel <span class="op">=</span> <span class="vs">r"$x_2$"</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># p_dims is 2</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> classification_data(noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plot_classification_data(X, y, ax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="homework5_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Code tp graph a straight line</p>
<div id="cell-11" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_line(w, x_min, x_max, ax, <span class="op">**</span>kwargs):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    w_ <span class="op">=</span> w.flatten()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.linspace(x_min, x_max, <span class="dv">101</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="op">-</span>(w_[<span class="dv">0</span>]<span class="op">*</span>x <span class="op">+</span> w_[<span class="dv">2</span>])<span class="op">/</span>w_[<span class="dv">1</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> ax.plot(x, y, <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="vanilla-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="vanilla-gradient-descent">Vanilla Gradient Descent</h3>
<p>The first experiment that I performed was vanilla gradient descent: When p_dim = 2, when alpha is sufficiently small and beta=0.</p>
<p>Want to see:</p>
<ol type="1">
<li>Gradient descent for logistic regression converges to a weight vector w that looks visually correct
<ul>
<li>show this by plot the decision boundary with the data</li>
</ul></li>
<li>Loss decreases monotonically: A monotonic function is a function which is either entirely nonincreasing or nondecreasing.
<ul>
<li>show this by plotting the loss over iterations</li>
</ul></li>
</ol>
<p>First implement a training loop with graphs with a dividing line to visualize our progress.</p>
<div id="cell-16" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a Logistic Regression </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression() </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer(LR)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize for main loop</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>loss_vec_van <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for vanilla gradient descent, alpha must be sufficiently small and beta must be 0</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    opt.step(X, y, alpha <span class="op">=</span> <span class="fl">0.01</span>, beta <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> LR.loss(X, y).item()</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    loss_vec_van.append(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_accuracy(X, y):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> LR.predict(X)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    correct_preds <span class="op">=</span> (predictions <span class="op">==</span> y).<span class="bu">float</span>()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> torch.mean(correct_preds)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>find_accuracy(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.9933333396911621</code></pre>
</div>
</div>
<p>Plot the loss over time over the 2000 iterations.</p>
<div id="cell-19" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_loss(loss, label<span class="op">=</span> <span class="st">""</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    plt.style.use(<span class="st">'seaborn-v0_8-whitegrid'</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    plt.plot(loss, color <span class="op">=</span> <span class="st">"blue"</span>, label<span class="op">=</span>label)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    plt.scatter(torch.arange(<span class="bu">len</span>(loss)), loss, color <span class="op">=</span> <span class="st">"slategrey"</span>, s<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Perceptron Iteration (Updates Only)"</span>, ylabel <span class="op">=</span> <span class="st">"loss"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"Final loss: </span><span class="sc">{</span>loss[<span class="bu">len</span>(loss)<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>plot_loss(loss_vec_van)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="homework5_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that the loss is decreasing monotonically over time through this graph of the loss. The negative slope shows us that the loss is in fact decreasing over time. In other words, our machine learning model is learning!</p>
<p>Plot the final line separating the data</p>
<div id="cell-22" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plot_classification_data(X, y, ax)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>draw_line(LR.w, x_min<span class="op">=-</span><span class="dv">1</span>, x_max<span class="op">=</span><span class="dv">2</span>, ax<span class="op">=</span>ax, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f"loss = </span><span class="sc">{</span>loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="homework5_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see that the dividing line almost perfectly divides the classes. In time, we could see the logistic regression training could become perfectly accurate.</p>
</section>
</section>
<section id="benefits-of-momentum" class="level2">
<h2 class="anchored" data-anchor-id="benefits-of-momentum">Benefits of momentum</h2>
<p>Our next experiment was to see the benefits of momentum. On the same data, gradient descent with momentum (e.g.&nbsp;beta=0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta=0).</p>
<p>We want to see:</p>
<ol type="1">
<li>A model that learns at a faster rate
<ul>
<li>show loss decreasing at a faster rate than when beta was 0</li>
</ul></li>
</ol>
<p>First implement a training loop with graphs with a dividing line to visualize our progress.</p>
<div id="cell-28" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a Logistic Regression </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression() </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer(LR)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> classification_data(noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize for main loop</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>loss_vec_mom <span class="op">=</span> []</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to see the benefits of momentum, alpha must be sufficiently small and beta must be 0.9</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    opt.step(X, y, alpha <span class="op">=</span> <span class="fl">0.01</span>, beta <span class="op">=</span> <span class="fl">0.9</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> LR.loss(X, y).item()            </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    loss_vec_mom.append(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Plot the loss over time over the 2000 iterations.</p>
<div id="cell-30" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plot_loss(loss_vec_mom, <span class="st">'Momentum'</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_vec_van, color <span class="op">=</span> <span class="st">"green"</span>, label<span class="op">=</span><span class="st">'Vanilla'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(torch.arange(<span class="bu">len</span>(loss_vec_van)), loss_vec_van, color <span class="op">=</span> <span class="st">"slategrey"</span>, s<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Perceptron Iteration (Updates Only)"</span>, ylabel <span class="op">=</span> <span class="st">"loss"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="homework5_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The faster declining slope of the loss shows that the larger beta value does in fact increase the learning speed of the machine learning model.</p>
<div id="cell-32" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plot_classification_data(X, y, ax)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>draw_line(LR.w, x_min<span class="op">=-</span><span class="dv">1</span>, x_max<span class="op">=</span><span class="dv">2</span>, ax<span class="op">=</span>ax, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f"loss = </span><span class="sc">{</span>loss<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="homework5_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see the benefits of increase of momentum by viewing the improved dividing line. The increase in the beta value allows our logistic regression to improve at a much faster rate then when beta=0. We know that because with the same number of iterations, the loss decreased more, or in other words, the model learned to classify at a faster rate.</p>
<div id="cell-34" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>find_accuracy(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 1.0</code></pre>
</div>
</div>
<p>A perfect accuracy rate!</p>
</section>
<section id="overfitting" class="level2">
<h2 class="anchored" data-anchor-id="overfitting">Overfitting</h2>
<p>Our final experiment was to show the danger of overfitting. To show this I need to generate some data where p_dim &gt; n_points and create an instance where the same logistic regression model has a 100% accuracy rate on training data.</p>
<p>Want to see:</p>
<ol type="1">
<li>Perfect accuracy for training data</li>
<li>Less accurate classification for testing data with the exact same parameters</li>
</ol>
<p>For overfitting, we need to generate data where p_dim &gt; n_points.</p>
<div id="cell-40" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> classification_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.5</span>, p_dims <span class="op">=</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Goal to achieve 100% accuracy with the training data.</p>
<div id="cell-42" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a Logistic Regression </span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression() </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> GradientDescentOptimizer(LR)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize for main loop</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>loss_vec <span class="op">=</span> []</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>):</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    opt.step(X_train, y_train, alpha <span class="op">=</span> <span class="fl">0.01</span>, beta <span class="op">=</span> <span class="fl">0.9</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> LR.loss(X_train, y_train).item()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    loss_vec.append(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-43" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss_vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.6321858763694763, 0.54594886302948, 0.46694833040237427, 0.4111320972442627, 0.37654781341552734, 0.3555326759815216, 0.34188082814216614, 0.33193260431289673, 0.3237716257572174, 0.31643998622894287, 0.3094692528247833, 0.3026372790336609, 0.2958459258079529, 0.289059579372406, 0.2822745144367218, 0.27550220489501953, 0.26876121759414673, 0.2620723843574524, 0.2554562985897064, 0.2489321529865265, 0.24251705408096313, 0.23622561991214752, 0.23007024824619293, 0.22406093776226044, 0.2182054966688156, 0.21250976622104645, 0.20697788894176483, 0.20161227881908417, 0.19641421735286713, 0.19138361513614655, 0.18651944398880005, 0.1818198561668396, 0.17728225886821747, 0.17290352284908295, 0.16867999732494354, 0.16460773348808289, 0.16068248450756073, 0.1568998098373413, 0.15325510501861572, 0.1497437208890915, 0.14636091887950897, 0.1431020200252533, 0.13996239006519318, 0.13693739473819733, 0.13402248919010162, 0.13121330738067627, 0.12850548326969147, 0.12589487433433533, 0.12337743490934372, 0.12094920873641968, 0.11860643327236176, 0.11634549498558044, 0.11416284739971161, 0.11205515265464783, 0.11001919955015182, 0.10805190354585648, 0.1061502993106842, 0.10431157052516937, 0.10253303498029709, 0.10081210732460022, 0.09914635866880417, 0.09753336757421494, 0.0959709957242012, 0.09445705264806747, 0.09298951923847198, 0.09156645089387894, 0.09018602967262268, 0.08884651213884354, 0.08754615485668182, 0.08628343790769577, 0.08505680412054062, 0.08386481553316116, 0.08270610123872757, 0.08157934248447418, 0.08048328757286072, 0.07941672950983047, 0.0783785805106163, 0.07736771553754807, 0.07638312131166458, 0.07542379200458527, 0.0744888186454773, 0.07357729226350784, 0.07268834114074707, 0.0718211829662323, 0.07097500562667847, 0.07014907896518707, 0.06934269517660141, 0.06855516880750656, 0.06778585910797119, 0.06703413277864456, 0.06629940867424011, 0.06558109819889069, 0.0648786723613739, 0.06419160962104797, 0.06351940333843231, 0.06286157667636871, 0.06221768260002136, 0.06158725917339325, 0.060969896614551544, 0.060365188866853714, 0.05977275222539902, 0.05919221043586731, 0.05862320959568024, 0.05806540325284004, 0.05751846358180046, 0.05698205903172493, 0.05645591393113136, 0.05593971163034439, 0.055433157831430435, 0.0549359992146492, 0.05444798618555069, 0.05396883934736252, 0.053498316556215286, 0.05303620174527168, 0.05258224532008171, 0.05213623866438866, 0.05169800668954849, 0.05126729980111122, 0.05084393173456192, 0.05042772367596626, 0.05001849681138992, 0.049616046249866486, 0.049220241606235504, 0.04883088171482086, 0.04844784736633301, 0.04807092994451523, 0.04770001769065857, 0.04733497276902199, 0.046975623816251755, 0.046621862798929214, 0.04627354070544243, 0.04593054950237274, 0.04559275135397911, 0.045260027050971985, 0.04493226855993271, 0.04460935294628143, 0.044291187077760696, 0.04397765174508095, 0.04366863891482353, 0.04336406663060188, 0.04306383058428764, 0.042767833918333054, 0.04247599467635155, 0.042188216000795364, 0.04190440475940704, 0.04162450134754181, 0.0413484089076519, 0.04107605293393135, 0.0408073291182518, 0.04054221510887146, 0.040280602872371674, 0.04002242162823677, 0.03976762294769287, 0.039516109973192215, 0.03926786035299301, 0.0390227809548378, 0.038780808448791504, 0.03854188695549965, 0.03830597549676895, 0.03807298094034195, 0.03784286975860596, 0.03761560469865799, 0.03739110752940178, 0.03716932609677315, 0.03695021569728851, 0.036733731627464294, 0.03651982918381691, 0.03630845248699188, 0.036099549382925034, 0.03589310869574547, 0.03568904474377632, 0.0354873426258564, 0.03528796136379242, 0.035090841352939606, 0.03489596024155617, 0.034703273326158524, 0.03451274335384369, 0.034324340522289276, 0.03413801267743111, 0.03395373746752739, 0.03377147018909454, 0.033591195940971375, 0.0334128774702549, 0.033236462622880936, 0.03306193649768829, 0.03288926184177399, 0.03271842375397682, 0.03254937380552292, 0.032382089644670486, 0.032216548919677734, 0.03205271437764168, 0.031890567392110825, 0.03173007443547249, 0.031571220606565475, 0.031413983553647995, 0.03125831484794617, 0.031104205176234245, 0.030951641499996185, 0.03080056607723236, 0.030650997534394264, 0.030502891167998314, 0.030356230214238167, 0.03021099418401718, 0.030067168176174164, 0.029924700036644936, 0.029783600941300392, 0.029643850401043892, 0.0295054130256176, 0.029368290677666664, 0.029232440516352654, 0.029097864404320717, 0.028964534401893616, 0.028832437470555305, 0.02870153822004795, 0.028571855276823044, 0.028443343937397003, 0.02831599861383438, 0.028189804404973984, 0.028064750134944916, 0.027940794825553894, 0.02781795524060726, 0.02769620157778263, 0.027575520798563957, 0.02745589055120945, 0.027337322011590004, 0.027219774201512337, 0.02710324339568615, 0.02698771469295025, 0.02687317691743374, 0.026759613305330276, 0.026647020131349564, 0.02653536945581436, 0.026424655690789223, 0.026314882561564445, 0.02620602585375309, 0.026098061352968216, 0.025991003960371017, 0.02588481642305851, 0.02577950991690159, 0.02567504718899727, 0.025571448728442192, 0.025468681007623672, 0.02536674588918686, 0.02526562660932541, 0.025165317580103874, 0.02506580390036106, 0.024967089295387268, 0.024869145825505257, 0.024771977216005325, 0.024675562977790833, 0.024579904973506927, 0.02448500320315361, 0.024390822276473045, 0.024297358468174934, 0.024204635992646217, 0.02411261759698391, 0.024021293967962265, 0.02393067069351673, 0.02384072355926037, 0.023751448839902878, 0.023662859573960304, 0.02357492409646511, 0.023487651720643044, 0.02340102754533291, 0.02331501990556717, 0.02322966977953911, 0.023144932463765144, 0.023060815408825874, 0.02297731675207615, 0.022894416004419327, 0.022812116891145706, 0.022730404511094093, 0.022649282589554787, 0.022568751126527786, 0.022488780319690704, 0.022409377619624138, 0.02233053930103779, 0.022252243012189865, 0.02217451110482216, 0.02209731563925743, 0.02202066034078598, 0.02194453775882721, 0.02186894416809082, 0.021793873980641365, 0.021719321608543396, 0.021645262837409973, 0.021571718156337738, 0.021498674526810646, 0.02142612263560295, 0.0213540680706501, 0.021282492205500603, 0.021211396902799606, 0.021140772849321365, 0.021070614457130432, 0.02100093849003315, 0.02093171328306198, 0.020862938836216927, 0.020794620737433434, 0.020726757124066353, 0.02065933309495449, 0.020592346787452698, 0.020525790750980377, 0.02045966312289238, 0.020393969491124153, 0.020328689366579056, 0.020263826474547386, 0.020199386402964592, 0.02013535052537918, 0.020071720704436302, 0.020008495077490807, 0.0199456624686718, 0.019883232191205025, 0.01982118748128414, 0.019759520888328552, 0.0196982529014349, 0.019637348130345345, 0.01957683265209198, 0.019516687840223312, 0.019456904381513596, 0.019397489726543427, 0.019338442012667656, 0.019279753789305687, 0.019221417605876923, 0.01916344091296196, 0.01910579949617386, 0.019048510119318962, 0.018991561606526375, 0.01893496699631214, 0.018878696486353874, 0.018822763115167618, 0.018767166882753372, 0.018711885437369347, 0.018656926229596138, 0.01860230416059494, 0.018547993153333664, 0.018494004383683205, 0.018440334126353264, 0.0183869618922472, 0.018333904445171356, 0.018281150609254837, 0.018228696659207344, 0.018176542595028877, 0.018124694004654884, 0.018073130398988724, 0.018021874129772186, 0.017970891669392586, 0.01792021468281746, 0.01786980591714382, 0.017819685861468315, 0.01776985451579094, 0.017720287665724754, 0.017671002075076103, 0.017621999606490135, 0.01757325604557991, 0.01752479560673237, 0.017476581037044525, 0.017428649589419365, 0.0173809677362442, 0.01733354665338993, 0.01728639379143715, 0.017239492386579514, 0.01719285175204277, 0.017146455124020576, 0.017100315541028976, 0.017054399475455284, 0.01700875535607338, 0.016963351517915726, 0.016918182373046875, 0.01687326282262802, 0.016828574240207672, 0.01678411476314068, 0.016739899292588234, 0.016695907339453697, 0.01665216125547886, 0.01660863310098648, 0.016565334051847458, 0.016522258520126343, 0.016479410231113434, 0.016436776146292686, 0.016394367441534996, 0.016352178528904915, 0.016310207545757294, 0.01626843772828579, 0.016226893290877342, 0.016185570508241653, 0.016144437715411186, 0.01610352098941803, 0.016062812879681587, 0.016022315248847008, 0.015982016921043396, 0.01594192534685135, 0.015902021899819374, 0.015862317755818367, 0.015822822228074074, 0.01578352600336075, 0.0157444067299366, 0.015705490484833717, 0.015666775405406952, 0.015628241002559662, 0.015589895658195019, 0.015551748685538769, 0.015513773076236248, 0.0154759855940938, 0.015438389033079147, 0.015400970354676247, 0.015363730490207672, 0.015326670370995998, 0.015289786271750927, 0.015253091230988503, 0.015216554515063763, 0.015180213376879692, 0.01514403335750103, 0.015108024701476097, 0.015072180889546871, 0.015036518685519695, 0.015001014806330204, 0.014965695329010487, 0.014930525794625282, 0.014895523898303509, 0.014860681258141994, 0.014826004393398762, 0.014791494235396385, 0.01475713774561882, 0.014722946099936962, 0.014688901603221893, 0.014655028469860554, 0.014621304348111153, 0.01458772923797369, 0.01455431617796421, 0.014521058648824692, 0.014487946406006813, 0.014454979449510574, 0.014422166161239147, 0.014389502815902233, 0.014356991276144981, 0.014324618503451347, 0.014292394742369652, 0.014260312542319298, 0.014228378422558308, 0.01419658400118351, 0.014164933934807777, 0.01413342822343111, 0.014102053828537464, 0.014070829376578331, 0.014039737172424793, 0.014008785597980022, 0.013977966271340847, 0.01394727360457182, 0.013916726224124432, 0.013886302709579468, 0.013856020756065845, 0.013825858943164349, 0.01379583403468132, 0.01376593392342329, 0.01373616699129343, 0.013706529513001442, 0.013677009381353855, 0.013647629879415035, 0.013618371449410915, 0.01358923688530922, 0.013560228049755096, 0.013531351462006569, 0.013502590358257294, 0.01347396057099104, 0.01344542857259512, 0.013417042791843414, 0.013388756662607193, 0.013360598124563694, 0.013332551345229149, 0.013304629363119602, 0.013276818208396435, 0.013249123468995094, 0.013221549801528454, 0.01319408044219017, 0.013166730292141438, 0.013139490969479084, 0.013112365268170834, 0.013085351325571537, 0.013058451004326344, 0.013031650334596634, 0.013004976324737072, 0.01297840941697359, 0.012951935641467571, 0.012925594113767147, 0.012899339199066162, 0.012873198837041855, 0.012847164645791054, 0.01282123290002346, 0.012795411050319672, 0.012769683264195919, 0.012744065374135971, 0.012718546204268932, 0.012693122960627079, 0.012667804956436157, 0.01264259684830904, 0.012617474421858788, 0.012592455372214317, 0.012567522935569286, 0.012542708776891232, 0.012517980299890041, 0.012493360787630081, 0.012468835338950157, 0.012444393709301949, 0.01242005918174982, 0.01239581499248743, 0.012371676973998547, 0.012347630225121975, 0.01232367753982544, 0.012299811467528343, 0.012276042252779007, 0.012252364307641983, 0.012228768318891525, 0.012205281294882298, 0.012181868776679039, 0.012158547528088093, 0.012135314755141735, 0.012112176045775414, 0.012089119292795658, 0.012066146358847618, 0.012043263763189316, 0.012020468711853027, 0.011997759342193604, 0.011975135654211044, 0.011952594853937626, 0.011930140666663647, 0.011907768435776234, 0.01188548095524311, 0.011863275431096554, 0.011841153725981712, 0.011819116771221161, 0.011797167360782623, 0.011775285005569458, 0.011753490194678307, 0.011731789447367191, 0.0117101538926363, 0.011688610538840294, 0.011667128652334213, 0.011645745486021042, 0.011624443344771862, 0.01160319522023201, 0.011582042090594769, 0.011560957878828049, 0.01153995469212532, 0.011519024148583412, 0.011498169042170048, 0.011477386578917503, 0.011456677690148354, 0.011436046101152897, 0.011415486223995686, 0.011394995264708996, 0.01137457974255085, 0.011354237794876099, 0.01133397314697504, 0.011313777416944504, 0.011293652467429638, 0.011273598298430443, 0.01125362142920494, 0.011233700439333916, 0.01121386419981718, 0.011194088496267796, 0.011174392886459827, 0.011154755018651485, 0.011135194450616837, 0.011115697212517262, 0.01109626516699791, 0.011076919734477997, 0.011057626456022263, 0.011038408614695072, 0.011019251309335232, 0.011000161990523338, 0.01098113413900137, 0.010962189175188541, 0.010943304747343063, 0.010924475267529488, 0.01090571191161871, 0.0108870230615139, 0.010868392884731293, 0.010849814862012863, 0.010831318795681, 0.010812871158123016, 0.010794485919177532, 0.010776162147521973, 0.010757910087704659, 0.010739706456661224, 0.01072157546877861, 0.010703501291573048, 0.010685477405786514, 0.010667518712580204, 0.010649628937244415, 0.010631787590682507, 0.010614011436700821, 0.010596293024718761, 0.010578641667962074, 0.010561032220721245, 0.010543491691350937, 0.010526001453399658, 0.010508588515222073, 0.010491209104657173, 0.010473902337253094, 0.01045665517449379, 0.010439455509185791, 0.01042230986058712, 0.010405225679278374, 0.010388200171291828, 0.010371226817369461, 0.010354317724704742, 0.010337469168007374, 0.010320655070245266, 0.010303903371095657, 0.010287213139235973, 0.010270577855408192, 0.010253986343741417, 0.010237468406558037, 0.010220992378890514, 0.010204566642642021, 0.01018819771707058, 0.010171879082918167, 0.010155607014894485, 0.010139381512999535, 0.01012322586029768, 0.010107104666531086, 0.010091041214764118, 0.010075031779706478, 0.010059075430035591, 0.010043160989880562, 0.010027306154370308, 0.010011478327214718, 0.009995723143219948, 0.009980004280805588, 0.009964343160390854, 0.00994871836155653, 0.00993315503001213, 0.009917634539306164, 0.009902155958116055, 0.00988673698157072, 0.009871364571154118, 0.009856029413640499, 0.009840752929449081, 0.00982552021741867, 0.009810325689613819, 0.009795181453227997, 0.009780089370906353, 0.009765038266777992, 0.00975004956126213, 0.009735087864100933, 0.009720183908939362, 0.009705307893455029, 0.009690502658486366, 0.009675716049969196, 0.009660996496677399, 0.009646312333643436, 0.009631676599383354, 0.00961708091199398, 0.009602529928088188, 0.009588027372956276, 0.009573563002049923, 0.009559150785207748, 0.009544778615236282, 0.009530451148748398, 0.009516163729131222, 0.009501921944320202, 0.00948772206902504, 0.009473569691181183, 0.009459458291530609, 0.009445391595363617, 0.009431363083422184, 0.009417375549674034, 0.00940343365073204, 0.00938953086733818, 0.009375656954944134, 0.009361838921904564, 0.009348059073090553, 0.009334315545856953, 0.009320612996816635, 0.009306945838034153, 0.0092933289706707, 0.009279744699597359, 0.009266204200685024, 0.009252700954675674, 0.009239221923053265, 0.009225801564753056, 0.009212415665388107, 0.009199066087603569, 0.009185751900076866, 0.009172480553388596, 0.009159239940345287, 0.009146049618721008, 0.009132884442806244, 0.00911976769566536, 0.009106677956879139, 0.00909363105893135, 0.009080621413886547, 0.009067649021744728, 0.00905471108853817, 0.009041811339557171, 0.009028945118188858, 0.009016122668981552, 0.009003332816064358, 0.008990569971501827, 0.00897784810513258, 0.008965173736214638, 0.008952523581683636, 0.008939908817410469, 0.00892733410000801, 0.008914790116250515, 0.008902287110686302, 0.008889815770089626, 0.008877377025783062, 0.008864969946444035, 0.008852600120007992, 0.00884027685970068, 0.008827977813780308, 0.008815716952085495, 0.008803489618003368, 0.00879129208624363, 0.00877912063151598, 0.008767003193497658, 0.008754909969866276, 0.00874285213649273, 0.008730831556022167, 0.008718826808035374, 0.008706871420145035, 0.008694950491189957, 0.008683058433234692, 0.008671201765537262, 0.008659368380904198, 0.008647572249174118, 0.008635813370347023, 0.008624082431197166, 0.008612392470240593, 0.008600733242928982, 0.008589090779423714, 0.008577490225434303, 0.00856592133641243, 0.00855437945574522, 0.008542864583432674, 0.008531386964023113, 0.008519940078258514, 0.008508517406880856, 0.008497137576341629, 0.008485783822834492, 0.008474458009004593, 0.008463162928819656, 0.008451894856989384, 0.008440655656158924, 0.008429447188973427, 0.008418269455432892, 0.008407127112150192, 0.008395998738706112, 0.008384913206100464, 0.008373851887881756, 0.008362820371985435, 0.008351817727088928, 0.008340839296579361, 0.008329905569553375, 0.00831898208707571, 0.008308092132210732, 0.008297234773635864, 0.008286396972835064, 0.008275594562292099, 0.008264819160103798, 0.008254077285528183, 0.008243346586823463, 0.008232660591602325, 0.008221988566219807, 0.008211351931095123, 0.00820075161755085, 0.008190164342522621, 0.008179602213203907, 0.008169073611497879, 0.008158567361533642, 0.008148100227117538, 0.008137643337249756, 0.008127223700284958, 0.008116829209029675, 0.008106449618935585, 0.008096108213067055, 0.008085792884230614, 0.008075499907135963, 0.008065234869718552, 0.008054993115365505, 0.008044783025979996, 0.008034590631723404, 0.008024433627724648, 0.008014298044145107, 0.008004182949662209, 0.007994096726179123, 0.007984032854437828, 0.00797400251030922, 0.007963990792632103, 0.007954010739922523, 0.007944053038954735, 0.007934113964438438, 0.007924200966954231, 0.007914314977824688, 0.007904455997049809, 0.007894627749919891, 0.007884809747338295, 0.007875025272369385, 0.007865259423851967, 0.007855523377656937, 0.007845809683203697, 0.007836123928427696, 0.007826465182006359, 0.007816817611455917, 0.007807205431163311, 0.00779760954901576, 0.007788049057126045, 0.007778484374284744, 0.007768979296088219, 0.00775946956127882, 0.007750004064291716, 0.00774055439978838, 0.00773111917078495, 0.0077217137441039085, 0.00771233718842268, 0.007702984381467104, 0.007693651132285595, 0.0076843430288136005, 0.00767504470422864, 0.007665778510272503, 0.007656540255993605, 0.007647320162504911, 0.007638123817741871, 0.007628941908478737, 0.007619778625667095, 0.007610655389726162, 0.007601537276059389, 0.007592449896037579, 0.0075833844020962715, 0.0075743370689451694, 0.00756530836224556, 0.007556305732578039, 0.007547320332378149, 0.007538357749581337, 0.007529424503445625, 0.007520493119955063, 0.007511596195399761, 0.007502713240683079, 0.007493860088288784, 0.007485029753297567, 0.007476206868886948, 0.0074674105271697044, 0.007458633743226528, 0.007449883967638016, 0.007441150024533272, 0.007432440295815468, 0.007423750124871731, 0.00741506926715374, 0.007406417280435562, 0.007397782988846302, 0.007389177102595568, 0.00738058565184474, 0.007372008636593819, 0.007363453507423401, 0.007354919798672199, 0.007346412632614374, 0.007337910123169422, 0.007329440210014582, 0.007320990785956383, 0.007312551606446505, 0.007304131984710693, 0.007295734714716673, 0.007287361193448305, 0.007279008626937866, 0.00727067282423377, 0.007262344937771559, 0.007254048716276884, 0.007245772052556276, 0.00723749864846468, 0.007229255978018045, 0.007221028208732605, 0.0072128199972212315, 0.007204629946500063, 0.0071964627131819725, 0.007188311778008938, 0.0071801794692873955, 0.007172064855694771, 0.007163971196860075, 0.007155886385589838, 0.007147825788706541, 0.007139780092984438, 0.0071317595429718494, 0.0071237520314753056, 0.007115754298865795, 0.007107792422175407, 0.00709983054548502, 0.007091896142810583, 0.007083976175636053, 0.007076086942106485, 0.007068194914609194, 0.007060328498482704, 0.007052483502775431, 0.0070446552708745, 0.007036834955215454, 0.007029032800346613, 0.0070212483406066895, 0.007013499271124601, 0.007005749270319939, 0.0069980197586119175, 0.0069902981631457806, 0.006982605438679457, 0.006974933668971062, 0.006967259105294943, 0.006959629710763693, 0.00695199565961957, 0.0069443839602172375, 0.006936789024621248, 0.006929212249815464, 0.006921641994267702, 0.006914105266332626, 0.006906573195010424, 0.006899069994688034, 0.006891563069075346, 0.0068840812891721725, 0.006876628380268812, 0.006869177334010601, 0.006861748639494181, 0.006854341831058264, 0.006846931762993336, 0.0068395561538636684, 0.006832183804363012, 0.006824831943958998, 0.0068175047636032104, 0.006810185499489307, 0.006802876014262438, 0.006795595865696669, 0.006788317114114761, 0.006781070958822966, 0.006773828063160181, 0.006766598671674728, 0.0067593930289149284, 0.006752203684300184, 0.006745017599314451, 0.006737858522683382, 0.006730711553245783, 0.0067235855385661125, 0.00671647023409605, 0.006709371227771044, 0.006702282000333071, 0.006695212330669165, 0.0066881561651825905, 0.006681123748421669, 0.006674094125628471, 0.00666708592325449, 0.006660084240138531, 0.006653104443103075, 0.006646147929131985, 0.0066391960717737675, 0.0066322628408670425, 0.006625339388847351, 0.006618433631956577, 0.006611551623791456, 0.006604666821658611, 0.006597810424864292, 0.0065909577533602715, 0.006584125105291605, 0.006577301770448685, 0.006570497527718544, 0.006563716102391481, 0.006556930020451546, 0.006550164427608252, 0.006543423049151897, 0.006536679342389107, 0.006529959850013256, 0.006523248739540577, 0.006516547873616219, 0.006509872153401375, 0.006503203883767128, 0.006496544927358627, 0.006489913444966078, 0.006483284756541252, 0.006476664450019598, 0.006470063701272011, 0.006463479250669479, 0.006456906907260418, 0.00645034946501255, 0.006443800404667854, 0.006437267176806927, 0.006430747453123331, 0.006424237508326769, 0.00641774432733655, 0.00641126511618495, 0.0064048017375171185, 0.006398343946784735, 0.006391898263245821, 0.006385458633303642, 0.006379057187587023, 0.006372648291289806, 0.006366255227476358, 0.006359873805195093, 0.00635350588709116, 0.00634716497734189, 0.006340819410979748, 0.006334496662020683, 0.006328175775706768, 0.006321880966424942, 0.006315586157143116, 0.006309300661087036, 0.006303041707724333, 0.006296785082668066, 0.00629054382443428, 0.006284322123974562, 0.006278109736740589, 0.006271897349506617, 0.0062657129019498825, 0.006259528920054436, 0.006253358907997608, 0.006247201468795538, 0.006241063587367535, 0.006234921049326658, 0.006228810176253319, 0.006222703494131565, 0.006216606590896845, 0.006210525054484606, 0.006204449106007814, 0.006198389455676079, 0.006192343775182962, 0.006186303682625294, 0.006180279888212681, 0.006174270994961262, 0.006168270017951727, 0.006162281613796949, 0.006156293209642172, 0.006150331348180771, 0.006144368089735508, 0.006138421129435301, 0.00613249558955431, 0.00612657843157649, 0.006120665464550257, 0.006114768795669079, 0.006108880043029785, 0.006103006191551685, 0.006097135134041309, 0.006091285962611437, 0.0060854461044073105, 0.006079604849219322, 0.006073791533708572, 0.006067984271794558, 0.0060621825978159904, 0.006056395824998617, 0.006050623022019863, 0.00604486046358943, 0.006039108615368605, 0.0060333614237606525, 0.006027625408023596, 0.006021920591592789, 0.006016196217387915, 0.0060105049051344395, 0.006004809867590666, 0.0059991334564983845, 0.005993464030325413, 0.005987806245684624, 0.005982164293527603, 0.005976525600999594, 0.0059709008783102036, 0.005965291988104582, 0.005959686823189259, 0.0059540909714996815, 0.0059485044330358505, 0.00594294024631381, 0.005937371402978897, 0.0059318202547729015, 0.005926279351115227, 0.005920758470892906, 0.0059152268804609776, 0.005909724626690149, 0.005904217250645161, 0.00589873269200325, 0.005893254186958075, 0.005887782666832209, 0.005882325116544962, 0.0058768694289028645, 0.005871439352631569, 0.005866007413715124, 0.00586058059707284, 0.005855179857462645, 0.005849792156368494, 0.005844388157129288, 0.005839010700583458, 0.0058336383663117886, 0.00582827627658844, 0.005822929088026285, 0.005817583296447992, 0.00581225473433733, 0.005806939210742712, 0.005801622290164232, 0.0057963235303759575, 0.005791032686829567, 0.005785746965557337, 0.005780484061688185, 0.0057752192951738834, 0.005769957322627306, 0.005764720495790243, 0.005759489722549915, 0.005754262208938599, 0.005749034229665995, 0.005743836052715778, 0.005738634616136551, 0.005733453202992678, 0.0057282764464616776, 0.005723109934478998, 0.005717944353818893, 0.005712800193578005, 0.005707651376724243, 0.0057025207206606865, 0.005697401240468025, 0.0056922887451946735, 0.0056871771812438965, 0.005682083778083324, 0.005676996894180775, 0.005671919789165258, 0.005666845012456179, 0.005661793984472752, 0.005656741093844175, 0.005651701241731644, 0.00564667209982872, 0.005641646217554808, 0.005636631045490503, 0.005631629843264818, 0.005626639351248741, 0.0056216479279100895, 0.00561666302382946, 0.005611702334135771, 0.005606742575764656, 0.00560178654268384, 0.005596845876425505, 0.005591909401118755, 0.005586979910731316, 0.005582073703408241, 0.005577149335294962, 0.005572255235165358, 0.005567364860326052, 0.005562480539083481, 0.005557611119002104, 0.005552751012146473, 0.005547887180000544, 0.005543038249015808, 0.005538203287869692, 0.005533367861062288, 0.005528547335416079, 0.005523739382624626, 0.0055189295671880245, 0.005514134652912617, 0.005509349517524242, 0.0055045620538294315, 0.005499794613569975, 0.00549502857029438, 0.005490281619131565, 0.005485532805323601, 0.005480789579451084, 0.005476066377013922, 0.005471348762512207, 0.005466642323881388, 0.005461929831653833, 0.005457228980958462, 0.005452550016343594, 0.005447869189083576, 0.0054431878961622715, 0.0054385303519666195, 0.005433876533061266, 0.005429230164736509, 0.0054245926439762115, 0.005419962573796511, 0.005415339954197407, 0.005410728044807911, 0.00540612218901515, 0.005401517730206251, 0.005396927706897259, 0.0053923423402011395, 0.005387773737311363, 0.005383205600082874, 0.005378652364015579, 0.005374101456254721, 0.005369556602090597, 0.005365016870200634, 0.005360497161746025, 0.005355967674404383, 0.005351455882191658, 0.005346951074898243, 0.0053424593061208725, 0.005337970796972513, 0.005333492066711187, 0.005329008214175701, 0.005324546713382006, 0.00532008521258831, 0.005315631628036499, 0.005311188753694296, 0.005306754261255264, 0.005302324425429106, 0.0052979011088609695, 0.005293484777212143, 0.005289080087095499, 0.005284677259624004, 0.005280282348394394, 0.00527589488774538, 0.005271515343338251, 0.00526713952422142, 0.005262781400233507, 0.005258422344923019, 0.005254075396806002, 0.005249727983027697, 0.005245399661362171, 0.005241063889116049, 0.0052367388270795345, 0.0052324216812849045, 0.005228129681199789, 0.005223819520324469, 0.005219532176852226, 0.005215245299041271, 0.005210963077843189, 0.005206692963838577, 0.0052024321630597115, 0.005198170430958271, 0.005193918477743864, 0.0051896777004003525, 0.005185437388718128, 0.005181211046874523, 0.005176984239369631, 0.00517276581376791, 0.005168545059859753, 0.0051643457263708115, 0.005160157103091478, 0.005155961494892836, 0.005151780322194099, 0.005147603806108236, 0.005143437534570694, 0.0051392726600170135, 0.0051351082511246204, 0.0051309652626514435, 0.005126830190420151, 0.005122697912156582, 0.0051185633055865765, 0.005114436615258455, 0.005110322963446379, 0.0051062144339084625, 0.005102108232676983, 0.005098007153719664, 0.005093925166875124, 0.005089845508337021, 0.005085765849798918, 0.0050816889852285385, 0.005077630281448364, 0.005073575768619776, 0.005069522652775049, 0.005065477918833494, 0.005061434116214514, 0.005057408008724451, 0.005053379572927952, 0.005049360450357199, 0.005045352969318628, 0.005041341297328472, 0.0050373440608382225, 0.005033353343605995, 0.005029367748647928, 0.0050253779627382755, 0.005021410994231701, 0.00501744169741869, 0.005013474728912115, 0.005009527318179607, 0.005005568265914917, 0.00500162597745657, 0.004997698124498129, 0.004993761423975229, 0.004989834036678076, 0.004985918290913105, 0.0049820151180028915, 0.00497809424996376, 0.004974205046892166, 0.004970300942659378, 0.004966409876942635, 0.004962530918419361, 0.004958650562912226, 0.004954781383275986, 0.004950924310833216, 0.004947055131196976, 0.004943206440657377, 0.004939368460327387, 0.004935517441481352, 0.004931692034006119, 0.004927861504256725, 0.004924037493765354, 0.004920220468193293, 0.004916407633572817, 0.004912601318210363, 0.004908810369670391, 0.004905004985630512, 0.004901228006929159, 0.004897444974631071, 0.004893667530268431, 0.004889901727437973, 0.00488613685593009, 0.004882373847067356, 0.004878619220107794, 0.00487488554790616, 0.00487113744020462, 0.004867395851761103, 0.004863671492785215, 0.004859951324760914, 0.004856236279010773, 0.004852517507970333, 0.004848811775445938, 0.004845110233873129, 0.0048414175398647785, 0.004837726708501577, 0.004834035877138376, 0.004830355755984783, 0.004826684482395649, 0.004823018331080675, 0.004819354508072138, 0.004815701860934496, 0.004812054801732302, 0.004808403085917234, 0.004804764408618212, 0.004801125731319189, 0.0047974968329072, 0.004793873522430658, 0.004790253005921841, 0.0047866408713161945, 0.004783038981258869, 0.004779432900249958, 0.004775843117386103, 0.004772244021296501, 0.00476866913959384, 0.004765084479004145, 0.0047615026123821735, 0.004757935181260109, 0.0047543710097670555, 0.004750810097903013, 0.004747259430587292, 0.004743713885545731, 0.004740170668810606, 0.004736622795462608, 0.004733091685920954, 0.004729574546217918, 0.00472604762762785, 0.00472252955660224, 0.004719017539173365, 0.004715515300631523, 0.004712010733783245, 0.004708512686192989, 0.004705023020505905, 0.004701533820480108, 0.004698052536696196, 0.004694573115557432, 0.004691111855208874, 0.004687637556344271, 0.0046841795556247234, 0.004680733196437359, 0.004677274730056524, 0.00467383349314332, 0.004670395981520414, 0.004666960332542658, 0.004663525149226189, 0.004660104401409626, 0.0046566808596253395, 0.004653260577470064, 0.004649855196475983, 0.004646451212465763, 0.0046430532820522785, 0.004639656748622656, 0.004636259283870459, 0.004632881842553616, 0.004629493225365877, 0.004626121371984482, 0.004622756969183683, 0.0046193962916731834, 0.004616024438291788, 0.004612663760781288, 0.004609316121786833, 0.0046059731394052505, 0.004602630622684956, 0.004599294159561396, 0.004595962353050709, 0.004592637065798044, 0.004589308053255081, 0.004585996735841036, 0.004582685884088278, 0.004579376429319382, 0.0045760697685182095, 0.00457276776432991, 0.0045694694854319096, 0.004566192626953125, 0.0045629022642970085, 0.004559623543173075, 0.0045563518069684505, 0.004553078208118677, 0.004549814388155937, 0.0045465570874512196, 0.004543295595794916, 0.004540049470961094, 0.004536798689514399, 0.004533551167696714, 0.004530313890427351, 0.00452708313241601, 0.004523853771388531, 0.00452063512057066, 0.004517417401075363, 0.004514193627983332, 0.004510989412665367, 0.004507784731686115, 0.004504580516368151, 0.0045013874769210815, 0.0044981930404901505, 0.00449501583352685, 0.004491819068789482, 0.004488644655793905, 0.004485469777137041, 0.004482298623770475, 0.004479134455323219, 0.004475975409150124, 0.004472816362977028, 0.004469659645110369, 0.004466510843485594, 0.004463372752070427, 0.004460235126316547, 0.0044571044854819775, 0.0044539677910506725, 0.004450846929103136, 0.004447723273187876, 0.004444603808224201, 0.004441496916115284, 0.004438383504748344, 0.004435283597558737, 0.004432182293385267, 0.004429085645824671, 0.0044259922578930855, 0.004422910511493683, 0.004419831093400717, 0.004416751675307751, 0.004413683898746967, 0.004410609137266874, 0.0044075362384319305, 0.004404481966048479, 0.00440142210572958, 0.00439836923032999, 0.004395319148898125, 0.004392280708998442, 0.004389237612485886, 0.004386201035231352, 0.004383170045912266, 0.004380148835480213, 0.004377120640128851, 0.0043740957044065, 0.004371087532490492, 0.004368075169622898, 0.004365071188658476, 0.004362056963145733, 0.004359060898423195, 0.004356070421636105, 0.004353085998445749, 0.004350095055997372, 0.0043471152894198895, 0.004344133194535971, 0.0043411655351519585, 0.004338193219155073, 0.0043352218344807625, 0.004332258831709623, 0.004329306073486805, 0.004326350521296263, 0.004323399625718594, 0.004320451989769936, 0.0043175132013857365, 0.004314575344324112, 0.0043116407468914986, 0.004308707546442747, 0.004305779002606869, 0.0043028718791902065, 0.004299950785934925, 0.0042970385402441025, 0.004294118843972683, 0.004291221499443054, 0.004288309719413519, 0.004285421688109636, 0.0042825196869671345, 0.004279632121324539, 0.004276742227375507, 0.0042738658376038074, 0.004270985256880522, 0.004268107004463673, 0.004265236668288708, 0.004262367729097605, 0.004259510897099972, 0.0042566535994410515, 0.00425378605723381, 0.004250938072800636, 0.004248096141964197, 0.00424524862319231, 0.004242416005581617, 0.004239573609083891, 0.004236741457134485, 0.004233910236507654, 0.004231085069477558, 0.004228259902447462, 0.004225443117320538, 0.004222629591822624, 0.004219826310873032, 0.004217017907649279, 0.0042142136953771114, 0.004211416468024254, 0.004208622500300407, 0.0042058248072862625, 0.0042030359618365765, 0.004200255498290062, 0.004197477828711271, 0.004194700624793768, 0.004191926214843988, 0.004189149010926485, 0.004186390899121761, 0.004183623939752579, 0.004180870018899441, 0.004178110044449568, 0.004175359848886728, 0.0041726152412593365, 0.00416986970230937, 0.004167123697698116, 0.00416438328102231, 0.004161664750427008, 0.004158927593380213, 0.004156201612204313, 0.0041534812189638615, 0.004150761291384697, 0.0041480474174022675, 0.0041453298181295395, 0.0041426243260502815, 0.004139916971325874, 0.004137217067182064, 0.004134512506425381, 0.00413182657212019, 0.004129146225750446, 0.004126453306525946, 0.004123761784285307, 0.004121081903576851, 0.004118398763239384, 0.004115735180675983, 0.004113061353564262, 0.004110397771000862, 0.004107732325792313, 0.00410507433116436, 0.0041024088859558105, 0.004099768586456776, 0.0040971143171191216, 0.004094473551958799, 0.004091829527169466, 0.004089190624654293, 0.004086554050445557, 0.004083916544914246, 0.00408129720017314, 0.004078663885593414, 0.004076039418578148, 0.0040734270587563515, 0.004070809576660395, 0.004068193957209587, 0.004065589979290962, 0.004062987398356199, 0.004060389474034309, 0.0040577854961156845, 0.0040551889687776566, 0.004052594304084778, 0.004050012212246656, 0.004047431517392397, 0.004044839646667242, 0.004042259883135557, 0.004039689898490906, 0.004037117585539818, 0.004034542478621006, 0.004031982272863388, 0.004029418807476759, 0.0040268683806061745, 0.0040243021212518215, 0.004021751694381237, 0.004019209183752537, 0.004016664810478687, 0.004014109261333942, 0.0040115718729794025, 0.004009036812931299, 0.004006507806479931, 0.004003977403044701, 0.004001452121883631, 0.003998929169028997, 0.003996408078819513, 0.003993893042206764, 0.003991379868239164, 0.003988864831626415, 0.003986345138400793, 0.003983853384852409, 0.003981351386755705, 0.003978855442255735, 0.003976365085691214, 0.003973861690610647, 0.003971375059336424, 0.003968888893723488, 0.003966410178691149, 0.003963927272707224, 0.003961457405239344, 0.003958974964916706, 0.003956511151045561, 0.00395404826849699, 0.003951581660658121, 0.003949122037738562, 0.00394666800275445, 0.0039442069828510284, 0.003941755276173353, 0.003939315676689148, 0.003936855588108301, 0.003934422042220831, 0.003931980114430189, 0.003929549362510443, 0.0039271204732358456, 0.003924683202058077, 0.00392225943505764, 0.003919831942766905, 0.003917412832379341, 0.003914997447282076, 0.003912585787475109, 0.00391017273068428, 0.003907752688974142, 0.003905358025804162, 0.0039029524195939302, 0.003900548443198204, 0.0038981614634394646, 0.003895763773471117, 0.003893375163897872, 0.003890985157340765, 0.0038886063266545534, 0.003886212594807148, 0.0038838430773466825, 0.0038814616855233908, 0.003879089606925845, 0.003876726608723402, 0.003874351968988776, 0.003871994325891137, 0.0038696301635354757, 0.003867278341203928, 0.0038649148773401976, 0.0038625660818070173, 0.0038602205459028482, 0.0038578722160309553, 0.0038555311039090157, 0.0038531858008354902, 0.00385085167363286, 0.0038485173135995865, 0.003846182720735669, 0.003843853482976556, 0.003841531928628683, 0.003839207347482443, 0.0038368897512555122, 0.0038345721550285816, 0.0038322568871080875, 0.0038299476727843285, 0.003827629378065467, 0.0038253304082900286, 0.003823026781901717, 0.0038207294419407845, 0.00381843070499599, 0.003816133365035057, 0.003813844872638583, 0.003811563365161419, 0.0038092799950391054, 0.003806987777352333, 0.0038047158159315586, 0.00380242639221251, 0.0038001597858965397, 0.0037978868931531906, 0.00379561772570014, 0.0037933567073196173, 0.0037910949904471636, 0.0037888309452682734, 0.0037865769118070602, 0.00378431030549109, 0.003782061394304037, 0.0037798210978507996, 0.0037775677628815174, 0.003775321878492832, 0.0037730897311121225, 0.003770840121433139, 0.003768606809899211, 0.0037663846742361784, 0.0037641527596861124, 0.003761922474950552, 0.003759693121537566, 0.003757476108148694, 0.003755263052880764, 0.003753042547032237, 0.0037508311215788126, 0.003748619928956032, 0.003746408736333251, 0.003744198475033045, 0.003742000786587596, 0.0037397979758679867, 0.0037375986576080322, 0.003735412610694766, 0.0037332146894186735, 0.0037310225889086723, 0.0037288374733179808, 0.003726641647517681, 0.0037244725972414017, 0.003722286783158779, 0.0037201077211648226, 0.003717942861840129, 0.003715757979080081, 0.00371359009295702, 0.0037114215083420277, 0.0037092568818479776, 0.003707097377628088, 0.003704937407746911, 0.0037027839571237564, 0.003700627014040947, 0.0036984700709581375, 0.00369632663205266, 0.0036941790021955967, 0.0036920341663062572, 0.0036898928228765726, 0.003687744727358222, 0.0036856073420494795, 0.0036834757775068283, 0.0036813414189964533, 0.0036792124155908823, 0.0036770871374756098, 0.0036749625578522682, 0.0036728468257933855, 0.0036707166582345963, 0.0036685995291918516, 0.0036664847284555435, 0.003664376214146614, 0.0036622672341763973, 0.00366016011685133, 0.0036580476444214582, 0.003655947744846344, 0.0036538452841341496, 0.003651744918897748, 0.003649655496701598, 0.003647562814876437, 0.003645468270406127, 0.0036433767527341843, 0.0036412947811186314, 0.00363921164534986, 0.0036371357273310423, 0.003635052125900984, 0.003632980864495039, 0.003630902851000428, 0.003628822974860668, 0.0036267649848014116, 0.00362470094114542, 0.0036226296797394753, 0.003620558651164174, 0.003618500893935561, 0.0036164429038763046, 0.0036143874749541283, 0.0036123355384916067, 0.0036102894227951765, 0.0036082377191632986, 0.0036061941646039486, 0.0036041534040123224, 0.0036021023988723755, 0.003600069787353277, 0.003598029026761651, 0.003595990128815174, 0.0035939591471105814, 0.0035919335205107927, 0.003589906031265855, 0.0035878855269402266, 0.0035858601331710815, 0.0035838382318615913, 0.0035818188916891813, 0.0035798021126538515, 0.003577793948352337, 0.0035757827572524548, 0.003573768539354205, 0.0035717596765607595, 0.003569754073396325, 0.0035677480045706034, 0.0035657586995512247, 0.0035637542605400085, 0.003561762161552906, 0.003559771226719022, 0.0035577828530222178, 0.0035557877272367477, 0.003553805872797966, 0.0035518088843673468, 0.003549835179001093, 0.0035478523932397366, 0.0035458726342767477, 0.0035438938066363335, 0.0035419168416410685, 0.003539949655532837, 0.003537971992045641, 0.0035360122565180063, 0.0035340441390872, 0.003532087430357933, 0.0035301262978464365, 0.003528168424963951, 0.003526208456605673, 0.0035242578014731407, 0.0035222978331148624, 0.0035203557927161455, 0.0035184065345674753, 0.003516459371894598, 0.0035145231522619724, 0.0035125717986375093, 0.003510640002787113, 0.0035086972638964653, 0.003506768262013793, 0.003504831111058593, 0.0035028972197324038, 0.0035009661223739386, 0.0034990482963621616, 0.003497121622785926, 0.0034952061250805855, 0.003493281314149499, 0.0034913658164441586, 0.0034894465934485197, 0.003487532027065754, 0.0034856214188039303, 0.003483711974695325, 0.0034818025305867195, 0.0034799003042280674, 0.0034779894631356, 0.0034760909620672464, 0.0034741919953376055, 0.003472301410511136, 0.0034704015124589205, 0.0034685167483985424, 0.0034666231367737055, 0.0034647295251488686, 0.0034628466237336397, 0.003460964886471629, 0.0034590796567499638, 0.0034571948926895857, 0.003455318743363023, 0.003453437937423587, 0.0034515599254518747, 0.0034496912267059088, 0.003447821829468012, 0.003445947077125311, 0.003444093279540539, 0.003442225744947791, 0.003440363798290491, 0.0034385069739073515, 0.0034366431646049023, 0.003434787504374981, 0.003432936267927289, 0.0034310861956328154, 0.0034292263444513083, 0.003427384188398719, 0.0034255373757332563, 0.003423681017011404, 0.0034218516666442156, 0.0034200057853013277, 0.003418169915676117, 0.0034163363743573427, 0.0034145053941756487, 0.0034126692917197943, 0.0034108469262719154, 0.003409010823816061, 0.003407178446650505, 0.003405358176678419, 0.003403525101020932, 0.0034017146099358797, 0.003399892244488001, 0.0033980750013142824, 0.003396263811737299, 0.003394448198378086, 0.0033926349133253098, 0.0033908365294337273, 0.0033890188205987215, 0.0033872134517878294, 0.0033854120410978794, 0.003383612260222435, 0.0033818092197179794, 0.003380011999979615, 0.003378216875717044, 0.003376421984285116, 0.003374627325683832, 0.0033728491980582476, 0.003371051512658596, 0.0033692698925733566, 0.0033674882724881172, 0.0033657022286206484, 0.0033639203757047653, 0.003362140618264675, 0.0033603694755584, 0.003358586458489299, 0.003356814617291093, 0.003355045337229967, 0.0033532720990478992, 0.0033515049144625664, 0.0033497451804578304, 0.0033479840494692326, 0.0033462224528193474, 0.0033444571308791637, 0.0033427015878260136, 0.0033409392926841974, 0.0033391881734132767, 0.0033374398481100798, 0.003335681278258562, 0.0033339308574795723, 0.003332186955958605, 0.003330445382744074, 0.0033286944963037968, 0.0033269552513957024, 0.003325217869132757, 0.003323483746498823, 0.003321744268760085, 0.003320010844618082, 0.003318273928016424, 0.003316547954455018, 0.0033148217480629683, 0.003313088323920965, 0.0033113574609160423, 0.0033096361439675093, 0.0033079173881560564, 0.003306201659142971, 0.003304474288597703, 0.0033027625177055597, 0.0033010425977408886, 0.0032993298955261707, 0.0032976248767226934, 0.0032959135714918375, 0.0032942062243819237, 0.003292499575763941, 0.0032907919958233833, 0.003289093030616641, 0.0032873910386115313, 0.0032856897450983524, 0.0032839979976415634, 0.0032822939101606607, 0.003280605422332883, 0.0032789146061986685, 0.0032772200647741556, 0.003275532741099596, 0.003273851005360484, 0.0032721564639359713, 0.0032704805489629507, 0.003268797881901264, 0.003267123596742749, 0.003265443490818143, 0.003263761755079031, 0.003262088866904378, 0.003260418539866805, 0.0032587475143373013, 0.003257072065025568, 0.00325540523044765, 0.0032537439838051796, 0.0032520710956305265, 0.0032504114788025618, 0.003248751163482666, 0.0032470927108079195, 0.0032454326283186674, 0.0032437776681035757, 0.0032421236392110586, 0.0032404663506895304, 0.0032388127874583006, 0.0032371801789849997, 0.00323552917689085, 0.0032338828314095736, 0.0032322353217750788, 0.003230588510632515, 0.003228952642530203, 0.0032273083925247192, 0.003225678810849786, 0.0032240282744169235, 0.003222406143322587, 0.0032207639887928963, 0.0032191395293921232, 0.003217506455257535, 0.0032158801332116127, 0.003214254043996334, 0.0032126293517649174, 0.003211007686331868, 0.0032093864865601063, 0.003207767615094781, 0.0032061475794762373, 0.0032045310363173485, 0.003202914958819747, 0.003201304003596306, 0.0031996986363083124, 0.0031980820931494236, 0.0031964778900146484, 0.003194869961589575, 0.003193268086761236, 0.003191662486642599, 0.003190058981999755, 0.003188465489074588, 0.003186862450093031, 0.0031852605752646923, 0.0031836635898798704, 0.003182073589414358, 0.0031804812606424093, 0.003178889863193035, 0.0031772989314049482, 0.003175716381520033, 0.003174128942191601, 0.0031725484877824783, 0.003170952433720231, 0.0031693780329078436, 0.0031677992083132267, 0.003166220150887966, 0.003164642257615924, 0.0031630739104002714]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="homework5_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The logistic regression model has been fit perfectly to the training data.</p>
<div id="cell-45" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>find_accuracy(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 1.0</code></pre>
</div>
</div>
<p>So we achieved 100% accuracy.</p>
<p>Then we must initialize the test data with the same parameters as the training data.</p>
<div id="cell-48" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> classification_data(n_points <span class="op">=</span> <span class="dv">50</span>, noise <span class="op">=</span> <span class="fl">0.5</span>, p_dims <span class="op">=</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-49" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>find_accuracy(X_test, y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.8799999952316284</code></pre>
</div>
</div>
<p>We can see that the logistic regression model has overfit to the training data and cannot classify the test data with the same accuracy. This it the danger of fitting a model too well to training data, as it is now not generalizable to other data.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this blog post, I was able to investigate more about gradient descent and its application in solving the empirical risk minimization problem, specifically focusing on logistic regression. By implementing gradient descent in the logistic regression model, I gained a deeper understanding of how the algorithm works and how it can be customized to suit different classification needs.</p>
<p>Furthermore, I explored a key variant of gradient descent called momentum, which allows logistic regression to achieve faster convergence. Through experiments and analysis, we observed the impact of momentum on the convergence speed on the logistic regression model.</p>
<p>Overall, these experiments taught me valuable lessons in optimization techniques for machine learning models. Through this experience I learned the importance of different parameters though changing the value of beta, and the impact of overfitting on our training data. By combining theory with practical implementation and experimentation, I gained a comprehensive understanding of gradient descent and its variants in the context of logistic regression.</p>
<p>As we continue to learn various machine learning algorithms and optimization techniques, the knowledge and insights gained from this blog post will help me develop practices to build more complex and efficient models in the future.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>