[
  {
    "objectID": "warmups/warmup0314.html",
    "href": "warmups/warmup0314.html",
    "title": "1. Generates the data",
    "section": "",
    "text": "1. Generates the data\n\nimport torch\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\nX\n\n\ny\n\ntensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n\n\n\n\n2. three class definitions\n\nimport torch\n\nclass LinearModel:\n\n    def __init__(self):\n        self.w = None \n\n    def score(self, X):\n        \"\"\"\n        Compute the scores for each data point in the feature matrix X. \n        The formula for the ith entry of s is s[i] = &lt;self.w, x[i]&gt;. \n\n        If self.w currently has value None, then it is necessary to first initialize self.w to a random value. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            s torch.Tensor: vector of scores. s.size() = (n,)\n        \"\"\"\n        if self.w is None: \n            self.w = torch.rand((X.size()[1]))\n        \n        s = torch.matmul(X, self.w)\n        \n        return s\n\n    def predict(self, X):\n        \"\"\"\n        Compute the predictions for each data point in the feature matrix X. The prediction for the ith data point is either 0 or 1. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            y_hat, torch.Tensor: vector predictions in {0.0, 1.0}. y_hat.size() = (n,)\n        \"\"\"\n        s = self.score(X)\n        \n        threshold = 0.5\n        y_hat = torch.where(s &gt;= threshold, torch.tensor(1.0), torch.tensor(0.0))\n        \n        return y_hat\n\nclass Perceptron(LinearModel):\n\n    def loss(self, X, y):\n        \"\"\"\n        Compute the misclassification rate. In the perceptron algorithm, the target vector y is assumed to have labels in {-1, 1}. A point i is classified correctly if its score s_i has the same sign as y_i. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n            y, torch.Tensor: the target vector.  y.size() = (n,). In the perceptron algorithm, the possible labels for y are assumed to be {-1, 1}\n        \"\"\"\n\n        y_hat = self.predict(X)\n        \n        misc = torch.where(y_hat*y &gt; 0, False, True)\n        \n        misc_rate = (1.0*misc).mean()\n        \n        print(misc_rate)\n        \n        return misc_rate\n\n    def grad(self, X, y):\n        pass \n\nclass PerceptronOptimizer:\n\n    def __init__(self, model):\n        self.model = model \n    \n    def step(self, X, y):\n        \"\"\"\n        Compute one step of the perceptron update using the feature matrix X \n        and target vector y. \n        \"\"\"\n        pass\n    \n    \np = Perceptron()\ns = p.score(X)\nl = p.loss(X, y)\nprint(l == 0.5)\n\ntensor(True)"
  },
  {
    "objectID": "warmups/warmup0416.html",
    "href": "warmups/warmup0416.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch\nfrom matplotlib import pyplot as plt \nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef regression_data(n = 100, w = torch.Tensor([-0.7, 0.5]), x_max = 1):\n\n    x = torch.rand(n)*x_max\n    y = x*w[1] + w[0] + 0.05*torch.randn(n)\n    return x, y\n\nx, y = regression_data()\n# print(f\"{x=}\")\n# print(f\"{y=}\")\n\nplt.scatter(x, y, facecolors = \"none\", edgecolors = \"steelblue\")\nlabs = plt.gca().set(xlabel = r\"$x$\", ylabel = r\"$y$\")\n\n\n\n\n\n\n\n\n\nimport random\n\ndef SGD(x, y, alpha, epochs):\n\n    w_0 = 0.5\n    w_1 = 0.5\n    risk_vec = []\n\n    for t in range(1, epochs+1):\n        \n        # Permute the data indices in a random order.\n        indices = list(range(len(x)))\n        random.shuffle(indices)\n\n        # Then, for each data index i (in the permuted order), perform the following two updates:\n        for i in range(len(indices)):\n            \n            y_i = y[i]\n            x_i = x[i]\n           \n            # update w_0 and w_1 based on SGD formula\n            error = y_i - w_1 * x_i - w_0\n            w_0 += ((2 * alpha)/ t)* w_0 * error\n            w_1 += ((2 * alpha)/ t)* w_1 * error * x_i\n\n        # Calculate the risk for the current w_0 and w_1\n        little_risk = (y - w_1 * x - w_0)**2\n        risk = torch.mean(little_risk)\n        risk_vec.append(risk.item())\n        \n    return risk_vec\n\n\nplt.figure(figsize=(10, 5))\n\nalpha = 0.2\n\n# Plotting the empirical risk during a single epoch\nrisk_vec = SGD(x, y, alpha, 2) ## so we get risk after one update! (technically update twice)\nprint(f\"{risk_vec=}\")\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(risk_vec) + 1), risk_vec)\nplt.xlabel('Epoch')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution in a Single Epoch')\n\n# Plotting the empirical risk over 100 epochs\nrisk_vec= SGD(x, y, alpha, 100)\nprint(f\"{risk_vec=}\")\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(risk_vec) + 1), risk_vec)\nplt.xlabel('Epochs')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution over 100 Epochs')\n\nplt.tight_layout()\nplt.show()\n\nrisk_vec=[0.20837946236133575, 0.20835325121879578]\nrisk_vec=[0.20837946236133575, 0.20835325121879578, 0.2083527147769928, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316, 0.2083526998758316]"
  },
  {
    "objectID": "warmups/warmup0408.html",
    "href": "warmups/warmup0408.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import urllib.request\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport torch\nimport numpy as np\n\ndef read_image(url):\n    urllib.request.urlretrieve(url, \"maru.png\")\n    img = Image.open(\"maru.png\")\n    return torch.tensor(np.array(img)/255).float()\n\nurl = \"https://github.com/middlebury-csci-0451/CSCI-0451-s24/blob/main/assets/img/figs/maru.png?raw=true\"\n\nimg = read_image(url)\n\ndef to_greyscale(im):\n    v = torch.tensor([0.2989, 0.5870, 0.1140])\n    return 1 - img[:,:,:3]@v\n\nimg = to_greyscale(img)\n\nplt.imshow(img, cmap = \"Greys\")\nno_ax = plt.gca().axis(\"off\")\n\nImplement kernel convolution for extracting features from images. Your implementation should accept a 2d array X (think of X as representing a greyscale image) and a square convolutional kernel K. Your implementation should operate using pure torch. You can use any zero-padding strategy, but you do need to explain what your strategy is when presenting.\n\nimport torch.nn.functional as F\nimport torch\n\ndef convolve2d(img, kernel):\n    \n    # padding is kernel row size//2\n    padding = kernel.shape[0] // 2\n    \n    # Apply zero-padding to the input image so that we can have 3 neighbors of our edge pixels\n    img_padded = torch.nn.functional.pad(img.float(), (padding, padding, padding, padding), mode='constant', value=0)\n    \n    # Initialize an empty tensor to store the convolution result\n    conv_output = torch.zeros_like(img)\n    \n    # loop through the rows and cols of the image\n    for i in range(padding, img_padded.shape[0] - padding):\n        for j in range(padding, img_padded.shape[1] - padding):\n            \n            # Go through and copy the image\n            region = img_padded[i - padding:i + padding + 1, j - padding:j + padding + 1]\n            \n            # multiply the region with the kernel and sum\n            conv_output[i - padding, j - padding] = torch.sum(region * kernel)\n    \n    return conv_output\n\n\n# from scipy.signal import convolve2d\n\nkernel = torch.tensor([[-1, -1, -1], [-1,  8, -1], [-1, -1, -1]])\n\nconvd = convolve2d(img, kernel)\n\nplt.imshow(convd, cmap = \"Greys\", vmin = 0, vmax = 0.1)\nplt.gca().axis(\"off\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post4/homework4.html",
    "href": "posts/post4/homework4.html",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "For this blog post I implemented the perceptron algorithm. I then ran several experiments to visualize the changes to my model each iteration and see the improvement of my loss. I investigated how the perceptron implementation changed when given linearly separable data vs not and 2 dimension vs more dimensional data. I also implemented mini-batch and ran experiments to see how this impacted algorithm outcomes. Through these steps, I learned the perceptron algorithm works and how that functionality can change to accommodate different data. Ultimately, I learned that the perceptron algorithm works well to address many different data forms by continuously updating the model based on misclassified points.\nLink to source code (perceptron.py)\nIn perceptron.py I implemented 5 functions: score, predict, loss, grad, and step. Here I will go through to briefly explain each function.\n\nscore: Calculate the score by taking the cross product of the data input and the weights.\npredict: Calculates y_hat where y_hat is 1 when the score is greater than or equal to 0, and 0 otherwise.\nloss: Finds loss by calculating mean of misclassified data points.\ngrad: Calculates score. If misclassified, returns gradient of cross product of data input and output \\[\\mathbb{1}\\left[s_i y_{i} &lt; 0 \\right] y_{i} \\mathbf{x}_{i}\\] Otherwise, return 0 gradient.\nstep: Calls and returns loss function. Adds gradient from grad function to model.\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptron_minibatch import Perceptron_mini, PerceptronOptimizer_mini\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nFirst, I used data from warmup to create a plot of linearly separable data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSee if perceptron.py code is functional by run the “minimal training loop” code from this section of the notes and eventually achieve loss = 0 on linearly separable data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThen I showed visualizations of the data, the separating line, and the evolution of the loss function during training.\nI copied the graph from notes in class to show the change of the loss in iterations of the perceptron algorithm.\n\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThis shows that the loss gradually decreases over the iterations and eventually reaches 0 (possible as the data is linearly separable).\nWe can then measure the accuracy of our predictions.\n\ndef find_accuracy(X, y):\n\n    predictions = p.predict(X)\n    \n    # convert predictions from {0, 1} to {-1, 1}\n    predictions = 2*predictions - 1\n    \n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nOur model has perfect accuracy.\n\n\n\nI changed class notes code to include 50 points that overlap the existing classes. This will mean that the data is not necessarily linearly separable like the data before.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data_overlap(n_points=300, noise=0.2, p_dims=2, overlap_points=50):\n   \n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # Add overlapping points within the range of existing points\n    overlap_X = torch.rand(overlap_points, p_dims) * (X.max() - X.min()) + X.min()\n    overlap_X = torch.cat((overlap_X, torch.ones((overlap_X.shape[0], 1))), 1)\n    X = torch.cat((X, overlap_X), dim=0)\n\n    # Convert y from {0, 1} to {-1, 1}\n    y = torch.cat((2 * y - 1, torch.ones(overlap_points, dtype=torch.long)))  # Label the overlapping points as class 1\n\n    return X, y\n\ndef plot_perceptron_data_overlap(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s=20, c=y[ix], facecolors=\"none\", edgecolors=\"darkgrey\", cmap=\"BrBG\", vmin=-2, vmax=2, alpha=0.5, marker=markers[i])\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data_overlap()\nplot_perceptron_data_overlap(X, y, ax)\nplt.show()\n\n\n\n\n\n\n\n\nNext, I reran the perceptron algorithm. The difference with not linearly separable data is that I cannot run the algorithm until the loss is 0 as it will never be 0 for this data. Instead, I ran the data for 1000 iterations based on Pr.Chodrow’s advice in the blog post description. I also including code to visualize model updates.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nX, y = perceptron_data_overlap()\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(0, 1000):\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # make an optimization step -- this is where the update actually happens\n    local_loss = opt.step(x_i, y_i)\n\n    # also add the new loss to loss_vec for plotting below\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nI then reran code to show the updates of the loss.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nHere we can see that the loss decreases over time but does not reach 0 like in the linearly separable case.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9628571271896362\n\n\nClose, but not perfect, accuracy.\n\n\n\nThe only difference here is that I created data with more than 5 dimensions.\n\nX, y = perceptron_data_overlap(n_points = 300, noise = 0.2, p_dims = 5)\n\nI ran the perceptron algorithm for 1000 iterations, without knowing if the data is linearly separable.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_dimen = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    local_loss = opt.step(x_i, y_i)\n    \n    if (local_loss &gt; 0):\n\n        loss = p.loss(X, y).item()\n        loss_vec_dimen.append(loss)\n        \n\nI then visualized the loss over time.\n\nplot_loss(loss_vec_dimen)\n\n\n\n\n\n\n\n\nBecause the visualization of the loss vector shows that data never fully reaches a loss of 0 after 1000 iterations, we can conclude that the data is probably not linearly separable.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9571428298950195\n\n\nClose, but not perfect, accuracy with non-linearly separable data.\n\n\n\nI then modified my perceptron.grad() method so that it accepts a submatrix of the feature matrix X of size k*p. I changed the code from:\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n\n   # if misclassified, calculate update\n    if s*y &lt;= 0:            \n        update_val = X*y\n        return update_val[0,:]\n    else:\n        return torch.zeros_like(self.w)\nto\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n    \n    # choose random learning rate\n    learning_rate = 0.3\n\n    # if misclassified, calculate update\n    misclass = s*y &lt;= 0\n    update_val_row = X*y[:,None]\n    \n    update_val = update_val_row * misclass[:,None]\n    \n    r = learning_rate * torch.mean(update_val, 0)\n    \n    return r\nThis change allowed the grad function to accept submatrices of the feature matrix X instead of just a single point.\n\n\n\nI then performed experiments and create visualizations to demonstrate the following:\nFor linearly separable data, when k = 1, mini-batch perceptron performs similarly to regular perceptron.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nLoss goes to 0 after many iterations. And accuracy is perfect.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nStill with k=1, the perceptron with mini-batch performs similarly to the normal perceptron with overlapping data.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nfor index in range(0, 1500):\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nWe can see that the perceptron reaches a loss close to, but never equal to, zero. We also have nearly perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.954285740852356\n\n\nFor linearly separable data when k = 10, mini-batch perceptron can still find a separating line in 2d.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nwhile loss &gt; 0:\n    \n    # K is 10\n    k = 10\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nHere the loss reaches zero with linearly separable data. We also have 100% accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nWhen k = n (that is, the batch size is the size of the entire data set), mini-batch perceptron can converge even when the data is not linearly separable, provided that the learning rate is small enough.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    # K is n\n    k = n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nEven with overlapping data (not linearly separable) the algorithm converges with k=n at very close to 0 loss. Close to perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9485714435577393\n\n\n\n\n\n\n\nFor a single iteration of the perceptron algorithm the dot product is taken between w and one row of the feature matrix X, as each row represents a data point. The size of the row of the feature matrix is p, the number of dimensions of the data. Therefore a single iteration’s runtime is O(p).\n\n\n\nWith the mini-batch algorithm, we take the dot product between each data point in the batch instead of just one for each iteration. Therefore it is O(kp) for each algorithm iteration.\n\n\n\n\nThrough this blog post, I learned many important lessons in Machine Learning and computing in general. Firstly, I was able to implement the perceptron algorithm and understand the steps that go into updating a machine learning model. Through my experiments, I validates that my implementation were correct. I learned how to analyze the updates of perceptron and see how the algorithm functions on different data. I developed my abilities to visualize data and use those visualizations to understand both the input data and the algorithm itself. Ultimately, I was able to successfully implement and use the perceptron algorithm for linearly separable data, non-linearly separable data, and data with more than 2 dimensions. With mini-batch, I was able to demonstrate the algorithm functioned similarly to a single point with k=1, can still find a separating line in linearly separable data with k=10, and when k=n converge even without linearly separable data."
  },
  {
    "objectID": "posts/post4/homework4.html#abstract",
    "href": "posts/post4/homework4.html#abstract",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "For this blog post I implemented the perceptron algorithm. I then ran several experiments to visualize the changes to my model each iteration and see the improvement of my loss. I investigated how the perceptron implementation changed when given linearly separable data vs not and 2 dimension vs more dimensional data. I also implemented mini-batch and ran experiments to see how this impacted algorithm outcomes. Through these steps, I learned the perceptron algorithm works and how that functionality can change to accommodate different data. Ultimately, I learned that the perceptron algorithm works well to address many different data forms by continuously updating the model based on misclassified points.\nLink to source code (perceptron.py)\nIn perceptron.py I implemented 5 functions: score, predict, loss, grad, and step. Here I will go through to briefly explain each function.\n\nscore: Calculate the score by taking the cross product of the data input and the weights.\npredict: Calculates y_hat where y_hat is 1 when the score is greater than or equal to 0, and 0 otherwise.\nloss: Finds loss by calculating mean of misclassified data points.\ngrad: Calculates score. If misclassified, returns gradient of cross product of data input and output \\[\\mathbb{1}\\left[s_i y_{i} &lt; 0 \\right] y_{i} \\mathbf{x}_{i}\\] Otherwise, return 0 gradient.\nstep: Calls and returns loss function. Adds gradient from grad function to model.\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptron_minibatch import Perceptron_mini, PerceptronOptimizer_mini\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/post4/homework4.html#implement-perceptron-on-linearly-separable-data",
    "href": "posts/post4/homework4.html#implement-perceptron-on-linearly-separable-data",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "First, I used data from warmup to create a plot of linearly separable data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSee if perceptron.py code is functional by run the “minimal training loop” code from this section of the notes and eventually achieve loss = 0 on linearly separable data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThen I showed visualizations of the data, the separating line, and the evolution of the loss function during training.\nI copied the graph from notes in class to show the change of the loss in iterations of the perceptron algorithm.\n\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThis shows that the loss gradually decreases over the iterations and eventually reaches 0 (possible as the data is linearly separable).\nWe can then measure the accuracy of our predictions.\n\ndef find_accuracy(X, y):\n\n    predictions = p.predict(X)\n    \n    # convert predictions from {0, 1} to {-1, 1}\n    predictions = 2*predictions - 1\n    \n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nOur model has perfect accuracy."
  },
  {
    "objectID": "posts/post4/homework4.html#implement-perceptron-on-data-that-is-not-linearly-separable",
    "href": "posts/post4/homework4.html#implement-perceptron-on-data-that-is-not-linearly-separable",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "I changed class notes code to include 50 points that overlap the existing classes. This will mean that the data is not necessarily linearly separable like the data before.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data_overlap(n_points=300, noise=0.2, p_dims=2, overlap_points=50):\n   \n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # Add overlapping points within the range of existing points\n    overlap_X = torch.rand(overlap_points, p_dims) * (X.max() - X.min()) + X.min()\n    overlap_X = torch.cat((overlap_X, torch.ones((overlap_X.shape[0], 1))), 1)\n    X = torch.cat((X, overlap_X), dim=0)\n\n    # Convert y from {0, 1} to {-1, 1}\n    y = torch.cat((2 * y - 1, torch.ones(overlap_points, dtype=torch.long)))  # Label the overlapping points as class 1\n\n    return X, y\n\ndef plot_perceptron_data_overlap(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s=20, c=y[ix], facecolors=\"none\", edgecolors=\"darkgrey\", cmap=\"BrBG\", vmin=-2, vmax=2, alpha=0.5, marker=markers[i])\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data_overlap()\nplot_perceptron_data_overlap(X, y, ax)\nplt.show()\n\n\n\n\n\n\n\n\nNext, I reran the perceptron algorithm. The difference with not linearly separable data is that I cannot run the algorithm until the loss is 0 as it will never be 0 for this data. Instead, I ran the data for 1000 iterations based on Pr.Chodrow’s advice in the blog post description. I also including code to visualize model updates.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nX, y = perceptron_data_overlap()\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(0, 1000):\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # make an optimization step -- this is where the update actually happens\n    local_loss = opt.step(x_i, y_i)\n\n    # also add the new loss to loss_vec for plotting below\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nI then reran code to show the updates of the loss.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nHere we can see that the loss decreases over time but does not reach 0 like in the linearly separable case.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9628571271896362\n\n\nClose, but not perfect, accuracy."
  },
  {
    "objectID": "posts/post4/homework4.html#implement-perceptron-on-data-with-more-than-2-dimensions",
    "href": "posts/post4/homework4.html#implement-perceptron-on-data-with-more-than-2-dimensions",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "The only difference here is that I created data with more than 5 dimensions.\n\nX, y = perceptron_data_overlap(n_points = 300, noise = 0.2, p_dims = 5)\n\nI ran the perceptron algorithm for 1000 iterations, without knowing if the data is linearly separable.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_dimen = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    local_loss = opt.step(x_i, y_i)\n    \n    if (local_loss &gt; 0):\n\n        loss = p.loss(X, y).item()\n        loss_vec_dimen.append(loss)\n        \n\nI then visualized the loss over time.\n\nplot_loss(loss_vec_dimen)\n\n\n\n\n\n\n\n\nBecause the visualization of the loss vector shows that data never fully reaches a loss of 0 after 1000 iterations, we can conclude that the data is probably not linearly separable.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9571428298950195\n\n\nClose, but not perfect, accuracy with non-linearly separable data."
  },
  {
    "objectID": "posts/post4/homework4.html#implement-minibatch",
    "href": "posts/post4/homework4.html#implement-minibatch",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "I then modified my perceptron.grad() method so that it accepts a submatrix of the feature matrix X of size k*p. I changed the code from:\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n\n   # if misclassified, calculate update\n    if s*y &lt;= 0:            \n        update_val = X*y\n        return update_val[0,:]\n    else:\n        return torch.zeros_like(self.w)\nto\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n    \n    # choose random learning rate\n    learning_rate = 0.3\n\n    # if misclassified, calculate update\n    misclass = s*y &lt;= 0\n    update_val_row = X*y[:,None]\n    \n    update_val = update_val_row * misclass[:,None]\n    \n    r = learning_rate * torch.mean(update_val, 0)\n    \n    return r\nThis change allowed the grad function to accept submatrices of the feature matrix X instead of just a single point."
  },
  {
    "objectID": "posts/post4/homework4.html#minibatch-perceptron-experiments",
    "href": "posts/post4/homework4.html#minibatch-perceptron-experiments",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "I then performed experiments and create visualizations to demonstrate the following:\nFor linearly separable data, when k = 1, mini-batch perceptron performs similarly to regular perceptron.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nLoss goes to 0 after many iterations. And accuracy is perfect.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nStill with k=1, the perceptron with mini-batch performs similarly to the normal perceptron with overlapping data.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nfor index in range(0, 1500):\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nWe can see that the perceptron reaches a loss close to, but never equal to, zero. We also have nearly perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.954285740852356\n\n\nFor linearly separable data when k = 10, mini-batch perceptron can still find a separating line in 2d.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nwhile loss &gt; 0:\n    \n    # K is 10\n    k = 10\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nHere the loss reaches zero with linearly separable data. We also have 100% accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nWhen k = n (that is, the batch size is the size of the entire data set), mini-batch perceptron can converge even when the data is not linearly separable, provided that the learning rate is small enough.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    # K is n\n    k = n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nEven with overlapping data (not linearly separable) the algorithm converges with k=n at very close to 0 loss. Close to perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9485714435577393"
  },
  {
    "objectID": "posts/post4/homework4.html#discussion-question",
    "href": "posts/post4/homework4.html#discussion-question",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "For a single iteration of the perceptron algorithm the dot product is taken between w and one row of the feature matrix X, as each row represents a data point. The size of the row of the feature matrix is p, the number of dimensions of the data. Therefore a single iteration’s runtime is O(p).\n\n\n\nWith the mini-batch algorithm, we take the dot product between each data point in the batch instead of just one for each iteration. Therefore it is O(kp) for each algorithm iteration."
  },
  {
    "objectID": "posts/post4/homework4.html#conclusion",
    "href": "posts/post4/homework4.html#conclusion",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "Through this blog post, I learned many important lessons in Machine Learning and computing in general. Firstly, I was able to implement the perceptron algorithm and understand the steps that go into updating a machine learning model. Through my experiments, I validates that my implementation were correct. I learned how to analyze the updates of perceptron and see how the algorithm functions on different data. I developed my abilities to visualize data and use those visualizations to understand both the input data and the algorithm itself. Ultimately, I was able to successfully implement and use the perceptron algorithm for linearly separable data, non-linearly separable data, and data with more than 2 dimensions. With mini-batch, I was able to demonstrate the algorithm functioned similarly to a single point with k=1, can still find a separating line in linearly separable data with k=10, and when k=n converge even without linearly separable data."
  },
  {
    "objectID": "posts/post2/hw2.html",
    "href": "posts/post2/hw2.html",
    "title": " ‘Optimal’ Decision-Making ",
    "section": "",
    "text": "In this blog post I trained a Logistic Regression function to predict the outcome of a loan. Through iterative comparison, I found that age, loan percent income, and person home ownership were the optimal features to predict loan status. Using these features to train my model, I then measured the optimal threshold by finding the highest possible gain for the bank. For these calculations I used the cost functions given by Professor Phil that depend on interest rate and loan amount. I then evaluated my model to see how the predictions changed based on different characteristics and calculated fairness based on my chosen definition.\nI found that the bank stood to gain $7931919.35735091 in total, or $1384.0375776218652 per loan. This was based on an optimal threshold of 2.857653939857805. My trained model predicted that older people and people with lower incomes were less likely to be given a loan. It also predicted default rate as highest for debt consolidation, then medical expenses, personal, education, and loan intent. Based on my choice of Error Rate Parity as a definition of model fairness, I determined that my model made fair decisions based on medical expense loan intent."
  },
  {
    "objectID": "posts/post2/hw2.html#introduction",
    "href": "posts/post2/hw2.html#introduction",
    "title": " ‘Optimal’ Decision-Making ",
    "section": "",
    "text": "In this blog post I trained a Logistic Regression function to predict the outcome of a loan. Through iterative comparison, I found that age, loan percent income, and person home ownership were the optimal features to predict loan status. Using these features to train my model, I then measured the optimal threshold by finding the highest possible gain for the bank. For these calculations I used the cost functions given by Professor Phil that depend on interest rate and loan amount. I then evaluated my model to see how the predictions changed based on different characteristics and calculated fairness based on my chosen definition.\nI found that the bank stood to gain $7931919.35735091 in total, or $1384.0375776218652 per loan. This was based on an optimal threshold of 2.857653939857805. My trained model predicted that older people and people with lower incomes were less likely to be given a loan. It also predicted default rate as highest for debt consolidation, then medical expenses, personal, education, and loan intent. Based on my choice of Error Rate Parity as a definition of model fairness, I determined that my model made fair decisions based on medical expense loan intent."
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html",
    "href": "posts/finalProj/AnalysisDoc.html",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport seaborn as sns\nallergies = pd.read_csv('allergies.csv')\ncancer = pd.read_csv('cancer.csv')\ncareplans = pd.read_csv('careplans.csv')\nclaims = pd.read_csv('claims.csv')\nconditions = pd.read_csv('conditions.csv')\ndiabetes = pd.read_csv('diabetes.csv')\nencounters = pd.read_csv('encounters.csv') \netc = pd.read_csv('etc.csv')\nheart = pd.read_csv('heart.csv')\nimmunizations = pd.read_csv('immunizations.csv')\nlungs = pd.read_csv('lungs.csv')\nmedications = pd.read_csv('medications.csv')\nobservations = pd.read_csv('observations.csv')\npatients = pd.read_csv('patients.csv')\npregnancy = pd.read_csv('pregnancy.csv')\nprocedures = pd.read_csv('procedures.csv')"
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html#exploring-general-trends-of-the-dataset",
    "href": "posts/finalProj/AnalysisDoc.html#exploring-general-trends-of-the-dataset",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Exploring general trends of the dataset",
    "text": "Exploring general trends of the dataset\n\nCareplans visualization\n\n# careplans viz \ncareplanCounts = careplans.groupby('DESCRIPTION').size().reset_index(name='count')\ncareplanCounts = careplanCounts.sort_values(by='count', ascending=False)\ntop10Careplans = careplanCounts.head(10)\nprint(top10Careplans)\n\nplt.figure(figsize=(10, 6))\nplt.bar(top10Careplans['DESCRIPTION'], top10Careplans['count'], color='skyblue')\nplt.xlabel('Careplan')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Careplans')\nplt.xticks(rotation=45, ha='right')\nplt.grid()\nplt.show()\n\n                              DESCRIPTION  count\n67                      Stress management    830\n34                       Exercise therapy    719\n22  Deep breathing and coughing exercises    641\n57       Recommendation to avoid exercise    641\n60                    Respiratory therapy    641\n6                     Antenatal education    609\n62                 Routine antenatal care    609\n53               Pregnancy diet education    609\n27                          Diabetic diet    498\n26          Diabetes self management plan    498\n\n\n\n\n\n\n\n\n\n\n\nVisualizing conditions dataset\n\nconditionsCounts = conditions.groupby('DESCRIPTION').size().reset_index(name='count')\nconditionsCounts = conditionsCounts.sort_values(by='count', ascending=False)\ntop10Conditions = conditionsCounts.head(10)\nprint(top10Conditions)\n\nplt.figure(figsize=(10, 6))\nplt.grid()\nplt.bar(top10Conditions['DESCRIPTION'], top10Conditions['count'], color='skyblue')\nplt.xlabel('Condition')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Conditions')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n#note that there is some matchup between top 10 careplans and top 10 conditions. this is good bc is expected behavior \n\n                              DESCRIPTION  count\n116            Viral sinusitis (disorder)   1125\n3      Acute viral pharyngitis (disorder)    602\n2             Acute bronchitis (disorder)    508\n86                            Prediabetes    458\n52                           Hypertension    373\n75                       Normal pregnancy    339\n20           Chronic sinusitis (disorder)    329\n79                           Otitis media    202\n110  Streptococcal sore throat (disorder)    146\n85                         Polyp of colon    108\n\n\n\n\n\n\n\n\n\nSinusitis seems like the most common condition in our dataset, which makes sense given its link to common colds.\n\n\nVisualizing encounters\n\n#encounters viz \nencountersCounts = encounters.groupby('DESCRIPTION').size().reset_index(name='count')\nencountersCounts = encountersCounts.sort_values(by='count', ascending=False)\ntop10Encounters = encountersCounts.head(10)\nprint(top10Encounters)\n\nplt.figure(figsize=(10, 6))\nplt.grid()\nplt.bar(top10Encounters['DESCRIPTION'], top10Encounters['count'], color='skyblue')\nplt.xlabel('Encounter Type')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Encounters')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n                    DESCRIPTION  count\n22         Outpatient Encounter   8629\n17        Encounter for symptom   2852\n24  Patient encounter procedure   1524\n27               Prenatal visit   1383\n23         Outpatient procedure   1281\n5    Consultation for treatment    899\n15     Encounter for 'check-up'    751\n13     Emergency room admission    694\n16        Encounter for problem    628\n6           Death Certification    461\n\n\n\n\n\n\n\n\n\nThe most common encounter was outpatient encounter, which makes sense as it includes general doctor visits like annuals and check-ups.\n\n\nExamining the distribution of patient identities\n\n#what the 'average' patient looks like in terms of race and gender\n\n#find most frequent race to represent average\npatientRaceCount = patients.groupby('race').size().reset_index(name='raceCount')\npatientRaceCount = patientRaceCount.sort_values(by='raceCount', ascending=False)\ntop5FreqRace = patientRaceCount.head(5)\nprint(top5FreqRace)\n\n#repeat for gender\npatientGenderCount = patients.groupby('gender').size().reset_index(name='genderCount')\npatientGenderCount = patientGenderCount.sort_values(by='genderCount', ascending=False)\ntop5FreqGender = patientGenderCount.head(5)\nprint(top5FreqGender)\n\n       race  raceCount\n3     white       1085\n2  hispanic        155\n1     black        129\n0     asian         93\n  gender  genderCount\n1      M          741\n0      F          721\n\n\nWhite patients are the most common racial group in our dataset, which may make sense for Massachussetts although this could be overrepresentation in comparison to actual demographic proportions. Male and female patients are somewhat evenly split.\n\n#visualization for what the 'average' patient looks like in terms of body stat characteristics\nobservationsRadar = observations.drop(columns = ['DATE', 'PATIENT', 'ENCOUNTER', 'CODE', 'UNITS'])\nobservationsRadar = observationsRadar.dropna()\nobservationsRadar['VALUE'] = pd.to_numeric(observationsRadar['VALUE'], errors='coerce')\naverageVals = observationsRadar.groupby('DESCRIPTION')['VALUE'].mean().reset_index()\naverageVals = averageVals[averageVals['DESCRIPTION'].isin(['Body Height', 'Body Weight', 'Body Mass Index', 'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Quality adjusted life years'])]\naverageVals\n\n\n\n\n\n\n\n\nDESCRIPTION\nVALUE\n\n\n\n\n3\nBody Height\n145.246010\n\n\n4\nBody Mass Index\n27.837863\n\n\n5\nBody Weight\n65.032186\n\n\n16\nDiastolic Blood Pressure\n84.485896\n\n\n40\nQuality adjusted life years\n41.901321\n\n\n45\nSystolic Blood Pressure\n129.587277\n\n\n\n\n\n\n\n\n#CREATION OF RADAR CHART OBTAINED FROM ONLINE CODE \ndescriptions = averageVals['DESCRIPTION'].tolist()\nvalues = averageVals['VALUE'].tolist()\n\n#to close the circle\nvalues.append(values[0])\n\nnum_vars = len(descriptions)\n\n#compute angle for each axis\nangles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n#need to \"complete the loop\" and append the start value to the end since plot is a cirlce\nvalues += values[:1]\nangles += angles[:1]\nvalues.pop()\n\n#plot\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\nax.fill(angles, values, color='skyblue', alpha=0.7)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(descriptions, fontsize=12)\n\nax.set_yticklabels([])\n\n#add values to each point\nfor angle, value in zip(angles[:-1], values[:-1]):\n    ax.text(angle, value, str(round(value, 2)), ha='center', va='bottom', fontsize=10)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis represents the average vital signs of our patients."
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html#pulmonary-diseases-eda",
    "href": "posts/finalProj/AnalysisDoc.html#pulmonary-diseases-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Pulmonary Diseases EDA",
    "text": "Pulmonary Diseases EDA\n\nPULMONARY DISEASES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(lungs, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Pulmonary Conditions by Race')\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt seems as though black patients are the most likely to get acute bronchitis while asian patients are most likely to get chronic sinusitus. This is important as analyzing these conditions together mike give us more mixed results in total.\n\n\nPULMONARY DISEASES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(lungs, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Pulmonary Conditions by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAgain, these vary by different diseases. However, we can see that Dominican and Mexican patients are most likely to be diagnosed with Acute bronchitis while Asian Indian patients are far more likely to get Chronic sinusitis.\n\n\nPULMONARY DISEASES BY BIRTHPLACE\nCreating a dataframe of populations of all the towns present in our data (all in Massachusetts), in order to calculate adjusted prevalance.\n\npopulations = pd.DataFrame()\nunique_birthplaces = lungs['birthplace'].unique()\npopulations['birthplace'] = unique_birthplaces\npopulations['pop'] = [37819, 101727, 35313, 93682, 3785, 53896, 87954,\n                       62698, 60803, 5025, 18317, 23662, 1734, 79762, 205319, 1641,\n                       25334, 650706, 6362, 31531, 59922, 6802, 58528, 87381, 19872,\n                       17612, 54980, 25121, 24747, 17489, 35022, 64065, 22992, 6196, 15946, 27395,\n                       44722, 25905, 9806, 11657, 65399, 8485, 15988, 113608, 28854, 4301, 104826, \n                       118488, 29155, 13885, 12265, 23315, 1677, 28501, 67153, 49350, 29327, 10293,\n                       29349, 8270, 154064, 19808, 43310, 29862, 22325, 16593, 6388, 6379, 17619, 14939, \n                       16720, 40971, 31747, 43646, 25050, 38637, 1802, 15702, 15101, 29195, 18448, 16732,\n                       16516, 11048, 7754, 11066, 19948, 4688, 100891, 33792, 18181, 11115, 37286, 100682,\n                       36229, 14570, 46601, 10911, 11625, 17669, 13697, 20902, 10667, 19163, 22666, 10580,\n                       32158, 11777, 1861, 2215, 6347, 64712, 23629, 34307, 752, 9547, 16705, 13866, 18510,\n                       43784, 35744, 21478, 70963, 42844, 40535, 16296, 16127, 7973, 31296, 26123, 3265, 13320,\n                       53241, 3234, 7839, 14749, 8055, 24498, 9640, 42235, 1730, 5798, 37973, 8316, 23923, 18662,\n                       10084, 6975, 15827, 49532, 11964, 136913, 6279, 7214, 17806, 41248, 6358, 10874, 19063, 6569,\n                       23184, 11753, 11386, 2985, 16450, 9182, 7764, 21374, 11802, 41502, 4111, 16053, 6183, 27003,\n                       15710, 6125, 8471, 4963, 29836, 1029, 15227, 10000, 12337, 1793, 491, 4678, 11988, 8168, 28950,\n                       1566, 12904, 57410, 6850, 13427, 11327, 6532, 917, 17456, 25209, 717, 17182, 15168, 12777, \n                       8153, 14313, 13435, 1264, 7884, 9230, 14180, 3056, 12418, 14382, 31248, 10169, 8541, 16188, \n                       36500, 31388, 17027, 16094, 13911, 28385, 9395, 11261, 1489, 12925, 27999, 5943, 16693, 5346,\n                       9811, 27400, 4871, 24296, 12133, 6346, 31635, 11688, 1245, 5284, 5966, 1458, 780, 27135, 316,\n                       5429, 17765, 12629, 3390, 6952, 7144, 2180, 7649, 4907, 5139, 4852, 5125, 5135, 2901, 5398, 4519]\n\n\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\nconditions_summed = lungs.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatalungs = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatalungs[f'prevalence_{condition_column}'] = prevdatalungs[condition_column] / prevdatalungs['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatalungs[prevdatalungs[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn many of these conditions, it’s worth nothing that one town in particular has a much higher prevelance. Unfortunately, with the vast number of towns and conditions there are so many it is difficult to accurately visualize. However, Hancock is extremely high for Sinusitus. This could be notable as Hancock ranked as the 287th richest town in Mass out of 341, so it is less wealthy compares to others in our dataset."
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html#cancer-eda",
    "href": "posts/finalProj/AnalysisDoc.html#cancer-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Cancer EDA",
    "text": "Cancer EDA\n\nCANCER BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(cancer, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Cancer by Race')\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAsian patients seem to dominate many categories.\n\n\nCANCER BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(cancer, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cancer by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCANCER PREVALANCE BY BIRTHPLACE\n\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\nconditions_summed = cancer.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatacanc = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatacanc[f'prevalence_{condition_column}'] = prevdatacanc[condition_column] / prevdatacanc['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatacanc[prevdatacanc[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor some cancer types, very few places record incidents. It is harder to draw conclusion for different conditions with this lack of data."
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html#diabetes-eda",
    "href": "posts/finalProj/AnalysisDoc.html#diabetes-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Diabetes EDA",
    "text": "Diabetes EDA\n\nDIABETES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(diabetes, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Diabetes by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDIABETES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(diabetes, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Diabetes by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe don’t see significant race of ethnicity disparities for the different diabetes conditions.\n\n\nDIABETES PREVALENCE BY BIRTHPLACE\n\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\nconditions_summed = diabetes.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatadiab = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatadiab[f'prevalence_{condition_column}'] = prevdatadiab[condition_column] / prevdatadiab['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatadiab[prevdatadiab[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere there is a mix of some towns dominating prevelance, and some diabetes cormorbidities being generally quite rare."
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html#cardiovascular-diseases-eda",
    "href": "posts/finalProj/AnalysisDoc.html#cardiovascular-diseases-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Cardiovascular Diseases EDA",
    "text": "Cardiovascular Diseases EDA\n\nCARDIOVASCULAR DISEASES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Cardiovascular Diseases by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\nHispanic patients seem slightly more common in our heart condition dataset.\n\n\nCARDIOVASCULAR DISEASES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Diseases by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCARDIOVASCULAR DISEASE PREVALENCE BY BIRTHPLACE\n\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\nconditions_summed = heart.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdataheart = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdataheart[f'prevalence_{condition_column}'] = prevdataheart[condition_column] / prevdataheart['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdataheart[prevdataheart[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharlemont and Russell seem to stand out with high levels of heart defects, with a wealth rating of 306 and 280 out of 341 respectively."
  },
  {
    "objectID": "posts/finalProj/AnalysisDoc.html#pregnancy-complicationsn-eda",
    "href": "posts/finalProj/AnalysisDoc.html#pregnancy-complicationsn-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Pregnancy Complicationsn EDA",
    "text": "Pregnancy Complicationsn EDA\n\nPREGNANCY COMPLICATIONS BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(pregnancy, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Pregnancy Complications by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\nGenerally non-white patients seem to have generally higher risks here.\n\n\nPREGNANCY COMPLICATIONS BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(pregnancy, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Pregnancy Complications by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWest-Indian patients seem to dominate a few of these conditions.\n\n\nPREGNANCY COMPLICATIONS BY BIRTHPLACE\n\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\nconditions_summed = pregnancy.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatapreg = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatapreg[f'prevalence_{condition_column}'] = prevdatapreg[condition_column] / prevdatapreg['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatapreg[prevdatapreg[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor many here as well there are few towns reporting incidents of these conditions, which could make analysis harder in terms of generalizing trends."
  },
  {
    "objectID": "posts/finalProj/ModelDoc.html",
    "href": "posts/finalProj/ModelDoc.html",
    "title": "Final Project Results, Model Creation Document",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport seaborn as sns\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n\nReading in our Data\n\nconditions_diabetes = pd.read_csv('conditions_diabetes.csv')\nconditions_pregnancy = pd.read_csv('conditions_pregnancy.csv')\nconditions_cancer = pd.read_csv('conditions_cancer.csv')\nconditions_heart = pd.read_csv('conditions_heart.csv')\nconditions_lungs = pd.read_csv('conditions_lungs.csv')\n\nobservations = pd.read_csv('observations_pivot.csv')\npatients = pd.read_csv('patient_clean.csv')\n\nNote: All of our datasets are grouped by related diseases (for example diabetes and comorbitidies such as diabetic retinopathy), for the rest of the post, when we say “diabetes” or “pregnancy complications,” we are talking about diabetes and all present comorbidites, or a grouping of pregnancy complications such as pre/ante eclampsia and misscarriage.\n\n\nDiabetes Modeling & Analysis\n\n1. Prepping Data\nIn order to prep our data for modelling we label encoded each of the qualitative variables (keeping track so we could decode them again later). We created a function in order to do this easily multiple times.\n\nle = LabelEncoder()\n\n# our data-prepping function for modeling\ndef prep_data(df):\n    \n    # label encode all quantitative vars\n    df[\"race\"] = le.fit_transform(df[\"race\"]) \n    race_code = {code: race for code, race in enumerate(le.classes_)}\n\n    df[\"ethnicity\"] = le.fit_transform(df[\"ethnicity\"])\n    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n\n    df[\"gender\"] = le.fit_transform(df[\"gender\"])\n    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n\n    df[\"birthplace\"] = le.fit_transform(df[\"birthplace\"])\n    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n\n    df[\"curr_town\"] = le.fit_transform(df[\"curr_town\"]) \n    curr_code = {code: bp for code, bp in enumerate(le.classes_)}\n    \n    # split data into test and train\n    train, test = train_test_split(df, test_size=0.2, random_state=42)\n    \n    X_train = train.drop(columns=['y'])\n    y_train = train['y']\n    \n    X_test = test.drop(columns=['y'])\n    y_test = test['y']\n    \n    # return split x, y, and all of the code tracking dicts\n    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code\n\n\nnp.random.seed(300)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_diabetes)\n\n\n\n2. Finding optimal model\nNext, we created a function we could reuse that identifies the best performing model on our data from the options random forest, SVC, logistic regression, and decision trees. The best model is what we use to predict the probability that each person has a certain disease (for our purposes, their risk score).\n\n# our model-finding function\ndef train_model(X_train, y_train):\n    \n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n    LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n\n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n\n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n    DTCScore  = grid_search.best_score_\n    bestDTCDepth = grid_search.best_params_\n\n\n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n\n    RFCScore  = grid_search.best_score_\n    bestRFCDepth = grid_search.best_params_\n\n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train, y_train)\n\n    SVMScore  = grid_search.best_score_   \n\n\n    print(\"best LR :\", LRScore)\n    print(\"best DTC:\", DTCScore)\n    print(\"best max depth: \", bestDTCDepth)\n    print(\"best RFC: \", RFCScore)\n    print(\"best max depth: \", bestRFCDepth)\n    print(\"best SVM: \", SVMScore)\n\n    # store the scores of each model\n    max_score = 0\n    max_model = \"\"\n    if LRScore &gt; max_score:\n        max_score = LRScore\n        max_model = \"LR\"\n    if DTCScore &gt; max_score:\n        max_score = DTCScore\n        max_model = \"DTC\"\n    if RFCScore &gt; max_score:\n        max_score = RFCScore\n        max_model = \"RFC\"\n    if SVMScore &gt; max_score:\n        max_score = SVMScore\n        max_model = \"SVM\"\n\n    print(\"best score overall is: \", max_score, \" with model: \", max_model)\n\n\n    \n# run model finding function on our diabetes data\nnp.random.seed(500)\ntrain_model(X_train, y_train)\n\nbest LR : 0.9050401672719269\nbest DTC: 0.9178790213124979\nbest max depth:  {'max_depth': 3}\nbest RFC:  0.9153112505043837\nbest max depth:  {'max_depth': 5}\nbest SVM:  0.9016066908770772\nbest score overall is:  0.9178790213124979  with model:  DTC\n\n\nThe results of our function should that the decision tree classifier is the best model possible, with an accuracy of 91.78%. Our accuracies tend generally lower considering the limited information we allowed the model to have, as we really wanted to see what the model would do when it predicted on identity factors such as race, ethnicity, and birthplace, and not how it would predict given information on the specific procedures and allergies a patient had.\n\n\n3. Create Risk Scores\nPredict probabilities for all our entries using the best model we found.\n\ndtc = DecisionTreeClassifier(max_depth=3)\ndtc.fit(X_train, y_train)\npred_prob = dtc.predict_proba(X_test)\n\nFor ease we created a risk finding function that can be used across factors and disease probabilities.\n\ndef find_risk(code, col, probs):\n    # finds the corresponding subset of our probability data\n    indices = (X_test[col] == code)\n    prob_subset = probs[indices]\n    # finds the average of this subset\n    av_prob = np.mean(prob_subset[:, 1]) \n    return av_prob   \n\n\n\n4. Compare Across Race, Gender, Ethnicity\nNext, we find the average risk score for different demographic characteristics: Race, Gender, and Ethnicity.\n\nRace\n\ndiabetesRaceRisk = []\n\n# find risk for each race (after finding on their code from the label encoder)\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    diabetesRaceRisk.append(newRow)\n\n# print summary table\ndiabetesRaceRisk = pd.DataFrame(diabetesRaceRisk)\ndiabetesRaceRisk = diabetesRaceRisk.sort_values(by='risk', ascending=False)\ndiabetesRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.479592\n\n\n2\nhispanic\n0.340659\n\n\n3\nwhite\n0.312536\n\n\n1\nblack\n0.256158\n\n\n\n\n\n\n\nOur model tells us that the most susceptible group to diabetes is Asian, then Hispanic and White, with Black being the least susceptible. These results were interesting in that they do indeed indicate that there may be a difference according to race, and made us think of how we could explore demographic information about Massachussetts (where our data is “from”), to understand whether these trends are reflective of larger trends.\n\n\nGender\n\ndiabetesGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    diabetesGenderRisk.append(newRow)\n\ndiabetesGenderRisk = pd.DataFrame(diabetesGenderRisk)\ndiabetesGenderRisk = diabetesGenderRisk.sort_values(by='risk', ascending=False)\ndiabetesGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.375356\n\n\n1\nM\n0.263908\n\n\n\n\n\n\n\nOur model tells us that women are slightly more likely to experience diabetes (or comorbidities) than men, which is in line with medical research we’ve seen.\n\n\nEthnicity\n\nav_risk_eth = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    av_risk_eth.append(new_row)\n\nav_risk_eth_df = pd.DataFrame(av_risk_eth)\nav_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\nav_risk_eth_df\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n2\nasian_indian\n0.714286\n\n\n13\npolish\n0.558405\n\n\n9\ngerman\n0.492674\n\n\n12\nmexican\n0.428571\n\n\n1\namerican\n0.428571\n\n\n14\nportuguese\n0.397959\n\n\n6\nenglish\n0.369491\n\n\n17\nscottish\n0.333333\n\n\n15\npuerto_rican\n0.329670\n\n\n5\ndominican\n0.328571\n\n\n0\nafrican\n0.318681\n\n\n11\nitalian\n0.314127\n\n\n3\ncentral_american\n0.306122\n\n\n7\nfrench\n0.285714\n\n\n8\nfrench_canadian\n0.261905\n\n\n4\nchinese\n0.244898\n\n\n18\nswedish\n0.205128\n\n\n16\nrussian\n0.200000\n\n\n10\nirish\n0.182902\n\n\n19\nwest_indian\n0.000000\n\n\n\n\n\n\n\nThis table gives us lots of information about risk by ethnicity, most interestingly perhaps, it agrees with our race finding that Asian people are more likely to experience diabetes, in that our most at risk ethnicity was Asian Indian. However, Chinese and West Indian, the two other Asian ethnicities in the datasest are at the bottom of the risk hierarchy, which made us consider that the risk of Asian Indian people specifically, and alone, was what was driving our race findings.\n\n\n\n5. Compare Across Wealthier & Poorer Towns of Residence/Birthplace\nIn order to compare outcomes across towns of varying socioeconomic status, we compiled a list of the richest and poorest towns present in our dataset (using Census data).\n\n# richest towns in Mass\nrichTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n\n# poorest towns in Mass\npoorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]\n\nCreate a df with all the information for the rich and poor towns\n\ndef find_town_info_row(town, bp_code_swapped, townCounts_df, code_name):\n    code = bp_code_swapped[town]\n    \n    if not townCounts_df[townCounts_df[code_name] == code].empty:\n        count = townCounts_df[townCounts_df[code_name] == code]['count'].values[0]\n    else:\n        count = 0\n    \n    new_row = {code_name: town, 'code': code, 'count': count}\n    \n    new_row_df = pd.DataFrame([new_row])\n    \n    return new_row_df\n\n\ndef find_town_info_all(counts, code_name):\n    \n    townCounts_df = pd.merge(X_test, counts, on=code_name)\n    town_info_rich = pd.DataFrame(columns=[code_name, 'code', 'count'])\n    town_info_poor = pd.DataFrame(columns=[code_name, 'code', 'count'])\n\n    bp_code_swapped = {value: key for key, value in bp_code.items()}\n\n    for town in richTowns:\n        \n        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n        town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n\n    for town in poorTowns:\n        \n        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n        town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n        \n    return town_info_rich, town_info_poor\n\nbirthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n\ntown_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')\n\nWe proceed with the following code to get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns.\n\ndef get_towns_by_sum_pop(town_info, code_name):\n    \n    townsUsed = set()\n    peopleCount = 0\n\n    for index, row in town_info.iterrows():\n        \n        if peopleCount &gt; 65:\n            break\n        \n        name = row[code_name]\n        count = row['count']\n        townsUsed.add(name)\n        peopleCount += count\n    \n    return townsUsed, peopleCount\n\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')\n\n\n\nBirthplace\n\ndef get_av_prob_bp(townsUsed, code_name, bp_code):\n    \n    town_codes = []\n    bp_code_swapped = {value: key for key, value in bp_code.items()}\n\n\n    for town_full in townsUsed:\n        town_codes.append(bp_code_swapped[town_full])\n        \n    indices = X_test[code_name].isin(town_codes)\n    prob_subset = pred_prob[indices]\n    av_prob = np.mean(prob_subset[:, 1]) \n\n    return av_prob\n\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.33305156382079454 av_poor_prob:  0.31947027331642713\n\n\nWe find that there is not much difference in the average risk of diabetes when comparing poor and rich birthplace towns.\n\n\nCurrent Town of Residence\nCreate a dataframe with the information for rich and poor towns. Then get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns.\n\ncurr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\n\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')\n\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n#HERE\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.25274725274725274 av_poor_prob:  0.2827087442472057\n\n\nIn this comparison, we find that people currently residing in rich towns have slightly lower rates of diabetes than those residing in poorer towns.\n\n\n\nPregnancy Analysis\nWe repeated the same exact process as above for each of our condition subsets.\nFinding the best model:\n\nnp.random.seed(567)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_pregnancy)\n\n\nnp.random.seed(567)\ntrain_model(X_train, y_train) \n\nbest LR : 0.9538094714060378\nbest DTC: 0.9632185172957705\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.9632185172957705\nbest max depth:  {'max_depth': 1}\nbest SVM:  0.9632185172957705\nbest score overall is:  0.9632185172957705  with model:  DTC\n\n\n\nCompute Average Risk scores\nPredict probabilities for all our entries using the best model we found\n\nDTC = DecisionTreeClassifier(max_depth=1)\nDTC.fit(X_train, y_train)\npred_prob = DTC.predict_proba(X_test)\n\n\n\nRace\n\npregRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    pregRaceRisk.append(newRow)\n\npregRaceRisk = pd.DataFrame(pregRaceRisk)\npregRaceRisk = pregRaceRisk.sort_values(by='risk', ascending=False)\npregRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n1\nblack\n0.051395\n\n\n2\nhispanic\n0.038217\n\n\n0\nasian\n0.037262\n\n\n3\nwhite\n0.034260\n\n\n\n\n\n\n\nHere we can see that being black gives a patient a little less than double the risk of pregnancy issues than being white. Hispanics have the second highest rate of pregnancy complications.\n\n\nGender\n\npregGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    pregGenderRisk.append(newRow)\n\npregGenderRisk = pd.DataFrame(pregGenderRisk)\npregGenderRisk = pregGenderRisk.sort_values(by='risk', ascending=False)\npregGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.074523\n\n\n1\nM\n0.000000\n\n\n\n\n\n\n\nThis result may seem a bit redundant or silly, it makes sense as generally people identified as male do not get pregnant.\n\n\nEthnicity\n\nav_risk_eth = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    av_risk_eth.append(new_row)\n\nav_risk_eth_df = pd.DataFrame(av_risk_eth)\nav_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n\n\nav_risk_eth_df\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n5\ndominican\n0.074523\n\n\n17\nscottish\n0.074523\n\n\n1\namerican\n0.054199\n\n\n3\ncentral_american\n0.053231\n\n\n19\nwest_indian\n0.049682\n\n\n8\nfrench_canadian\n0.049682\n\n\n12\nmexican\n0.049682\n\n\n14\nportuguese\n0.047908\n\n\n4\nchinese\n0.042585\n\n\n7\nfrench\n0.039746\n\n\n11\nitalian\n0.038269\n\n\n6\nenglish\n0.036060\n\n\n0\nafrican\n0.034395\n\n\n2\nasian_indian\n0.031939\n\n\n15\npuerto_rican\n0.031529\n\n\n18\nswedish\n0.029809\n\n\n10\nirish\n0.025262\n\n\n13\npolish\n0.024841\n\n\n9\ngerman\n0.023289\n\n\n16\nrussian\n0.014905\n\n\n\n\n\n\n\nHere we see that our finding that Black patients are more likely to experience pregnancy-related complications is driven largely by Dominican patients.\n\n\nBirthplace\n\nbirthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')\n\n\nnp.random.seed(234)\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.038981469137448335 av_poor_prob:  0.03668844154112784\n\n\n\n\nCurrent Town of Residence\n\ncurr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')\n\n\nnp.random.seed(234)\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.04586055192640981 av_poor_prob:  0.03630627027507444\n\n\nThis finding was somewhat surprising to us, in that wealthier towns were found to have higher risks of pregnancy complications. We discuss the potential implications of this result in our results section.\n\n\n\nCancer Analysis\n\nnp.random.seed(2)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_cancer)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\nnp.random.seed(500)\ntrain_model(X_train, y_train)\n\nbest LR : 0.9486775980338212\nbest DTC: 0.9546641722607386\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.9546641722607386\nbest max depth:  {'max_depth': 1}\nbest SVM:  0.9546641722607386\nbest score overall is:  0.9546641722607386  with model:  DTC\n\n\nOnce again we find that the model with the best score is DTC, The Decision Tree Classifier, with about 98% accuracy.\n\nDTC = DecisionTreeClassifier(max_depth=1)\nDTC.fit(X_train, y_train)\npred_prob = DTC.predict_proba(X_test)\n\n\nRace\n\ncancerRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    cancerRaceRisk.append(newRow)\n\ncancerRaceRisk = pd.DataFrame(cancerRaceRisk)\ncancerRaceRisk = cancerRaceRisk.sort_values(by='risk', ascending=False)\ncancerRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n3\nwhite\n0.051942\n\n\n2\nhispanic\n0.051650\n\n\n1\nblack\n0.046859\n\n\n0\nasian\n0.034009\n\n\n\n\n\n\n\nWe find across the board cancer rates are somewhat even, but that at the extremes white patients have almost a 52% risk of being classified with cancer,and Asian patients have around a 34% risk.\n\n\nGender\n\ncancerGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    cancerGenderRisk.append(newRow)\n\ncancerGenderRisk = pd.DataFrame(cancerGenderRisk)\ncancerGenderRisk = cancerGenderRisk.sort_values(by='risk', ascending=False)\ncancerGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n1\nM\n0.053825\n\n\n0\nF\n0.047147\n\n\n\n\n\n\n\nWomen are slightly less likely to have cancer.\n\n\nEthnicity\n\ncancerEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    cancerEthRisk.append(new_row)\n\ncancerEthRisk = pd.DataFrame(cancerEthRisk)\ncancerEthRisk = cancerEthRisk.sort_values(by='risk', ascending=False)\n\ncancerEthRisk\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n9\ngerman\n0.105675\n\n\n15\npuerto_rican\n0.067085\n\n\n0\nafrican\n0.067085\n\n\n4\nchinese\n0.062675\n\n\n11\nitalian\n0.059576\n\n\n6\nenglish\n0.057127\n\n\n13\npolish\n0.049934\n\n\n10\nirish\n0.049557\n\n\n14\nportuguese\n0.048342\n\n\n18\nswedish\n0.045475\n\n\n5\ndominican\n0.045475\n\n\n1\namerican\n0.041827\n\n\n3\ncentral_american\n0.034009\n\n\n7\nfrench\n0.032097\n\n\n2\nasian_indian\n0.005342\n\n\n8\nfrench_canadian\n0.005342\n\n\n12\nmexican\n0.005342\n\n\n19\nwest_indian\n0.005342\n\n\n16\nrussian\n0.005342\n\n\n17\nscottish\n0.005342\n\n\n\n\n\n\n\nOur results for ethnicity largely match the results we found distinguishing by race.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.06399830132085002 av_poor_prob:  0.04238804096017698\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.03621368085712754 av_poor_prob:  0.05164958111475114\n\n\nWe note that for birthplace, people in rich towns are more likely to get diagnosed with cancer as opposed to people from poorer towns. For current town of residence, the opposite is true.\n\n\n\nHeart Analysis\n\nnp.random.seed(210)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_heart)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\nnp.random.seed(20)\ntrain_model(X_train, y_train)\n\n/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nbest LR : 0.87766773045743\nbest DTC: 0.8973478595796193\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.8999156303877335\nbest max depth:  {'max_depth': None}\nbest SVM:  0.8973478595796193\nbest score overall is:  0.8999156303877335  with model:  RFC\n\n\n\nCompute Average Risk scores\nWe found that the best model to predict probabilities for all our entries in this case would be RFC.\n\nRFC = RandomForestClassifier(random_state=0, max_depth=1)\nRFC.fit(X_train, y_train)\n\npred_prob = RFC.predict_proba(X_test)\n\n\n\n4. Compare Across Race, Gender, Ethnicity\n\n\nRace\n\nheartRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    heartRaceRisk.append(newRow)\n\nheartRaceRisk = pd.DataFrame(heartRaceRisk)\nheartRaceRisk = heartRaceRisk.sort_values(by='risk', ascending=False)\nheartRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.510746\n\n\n3\nwhite\n0.502101\n\n\n1\nblack\n0.491353\n\n\n2\nhispanic\n0.491280\n\n\n\n\n\n\n\nWe find that the demographic with the highest likelihood of having heart problems is Asian, but overall the results are fairly even.\n\n\nGender\n\nheartGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    heartGenderRisk.append(newRow)\n\nheartGenderRisk = pd.DataFrame(heartGenderRisk)\nheartGenderRisk = heartGenderRisk.sort_values(by='risk', ascending=False)\nheartGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.508013\n\n\n1\nM\n0.492275\n\n\n\n\n\n\n\nAccording to our results, women and men are equally likely to have heart conditions, which disagrees with real medical trends that show men are much more likely to have these conditions.\n\n\nEthnicity\n\nheartEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    heartEthRisk.append(new_row)\n\nheartEthRisk = pd.DataFrame(heartEthRisk)\nheartEthRisk = heartEthRisk.sort_values(by='risk', ascending=False)\n\nheartEthRisk\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n2\nasian_indian\n0.556735\n\n\n17\nscottish\n0.556144\n\n\n13\npolish\n0.553649\n\n\n12\nmexican\n0.534791\n\n\n14\nportuguese\n0.519123\n\n\n1\namerican\n0.516831\n\n\n9\ngerman\n0.516270\n\n\n16\nrussian\n0.511412\n\n\n18\nswedish\n0.508385\n\n\n6\nenglish\n0.500485\n\n\n0\nafrican\n0.500460\n\n\n7\nfrench\n0.498784\n\n\n8\nfrench_canadian\n0.494611\n\n\n11\nitalian\n0.490876\n\n\n19\nwest_indian\n0.489579\n\n\n10\nirish\n0.489033\n\n\n3\ncentral_american\n0.487097\n\n\n15\npuerto_rican\n0.482365\n\n\n5\ndominican\n0.480579\n\n\n4\nchinese\n0.464757\n\n\n\n\n\n\n\nAgain, here we see very little variation.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.10892307692307693 av_poor_prob:  0.1103076923076923\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.08661538461538462 av_poor_prob:  0.09256410256410255\n\n\nIt seems as if there are not significant differences between the risk of heart diseases between wealthier and less-wealthy birthplace towns or current towns of residence.\n\n\n\nLungs Analysis\n\nnp.random.seed(400)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_lungs)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\ntrain_model(X_train, y_train)\n\nbest LR : 0.5705770147830234\nbest DTC: 0.6107626279300099\nbest max depth:  {'max_depth': 5}\nbest RFC:  0.6210850665786289\nbest max depth:  {'max_depth': 4}\nbest SVM:  0.5971387696709585\nbest score overall is:  0.6210850665786289  with model:  RFC\n\n\n\nCompute Average Risk scores\nWe found that the best model to predict probabilities for all our entries iin this case would be RFC.\n\nRFC = RandomForestClassifier(random_state=0, max_depth=4)\nRFC.fit(X_train, y_train)\n\npred_prob = RFC.predict_proba(X_test)\n\n\n\n4. Compare Across Race, Gender, Ethnicity\n\n\nRace\n\nlungsRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    lungsRaceRisk.append(newRow)\n\nlungsRaceRisk = pd.DataFrame(lungsRaceRisk)\nlungsRaceRisk = lungsRaceRisk.sort_values(by='risk', ascending=False)\nlungsRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.514338\n\n\n1\nblack\n0.509092\n\n\n3\nwhite\n0.507817\n\n\n2\nhispanic\n0.492106\n\n\n\n\n\n\n\nWe find very little variation for risk rates for lung issues for race.\n\n\nGender\n\nlungsGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    lungsGenderRisk.append(newRow)\n\nlungsGenderRisk = pd.DataFrame(lungsGenderRisk)\nlungsGenderRisk = lungsGenderRisk.sort_values(by='risk', ascending=False)\nlungsGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.519215\n\n\n1\nM\n0.493550\n\n\n\n\n\n\n\nAgain, there are pretty even rates for gender and risk of having lung complications.\n\n\nEthnicity\n\nlungsEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    lungsEthRisk.append(new_row)\n\nlungsEthRisk = pd.DataFrame(lungsEthRisk)\nlungsEthRisk = lungsEthRisk.sort_values(by='risk', ascending=False)\n\nlungsEthRisk\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n13\npolish\n0.583031\n\n\n12\nmexican\n0.580761\n\n\n17\nscottish\n0.577455\n\n\n2\nasian_indian\n0.577379\n\n\n18\nswedish\n0.560368\n\n\n0\nafrican\n0.559980\n\n\n16\nrussian\n0.549236\n\n\n1\namerican\n0.548145\n\n\n9\ngerman\n0.521056\n\n\n14\nportuguese\n0.516368\n\n\n6\nenglish\n0.514735\n\n\n11\nitalian\n0.498196\n\n\n8\nfrench_canadian\n0.487928\n\n\n7\nfrench\n0.485426\n\n\n10\nirish\n0.481814\n\n\n15\npuerto_rican\n0.479351\n\n\n5\ndominican\n0.473627\n\n\n3\ncentral_american\n0.463493\n\n\n19\nwest_indian\n0.457941\n\n\n4\nchinese\n0.451298\n\n\n\n\n\n\n\nThis analysis follows the trend of generally even probabilities throughout, in the most extreme cases with polish people having 58% risk and chinese people having 45%.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.4872429058942045 av_poor_prob:  0.5189382015534418\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.5039833131472166 av_poor_prob:  0.5124758651408807\n\n\nThe results for the risk score for patients of different race, gender, ethnicity, birthplace town, and current town of residence are curious as every risk score hovers around a 0.5 and generally even throughout the different demographics. This points to the conclusion that we are maybe investigating too large amount of conditions under lung ailments."
  },
  {
    "objectID": "posts/post6/homework6.html",
    "href": "posts/post6/homework6.html",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom NewtonOptimizer import NewtonOptimizer\nimport sys\nsys.path.append('/Users/lindseyschweitzer/Documents/GitHub/lfschweitzer.github.io/')\n\nfrom posts.post5.logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/post6/homework6.html#data-generation",
    "href": "posts/post6/homework6.html#data-generation",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "Data Generation",
    "text": "Data Generation\nBefore doing any experiments, I had to generate data for a classification problem.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nPlot the data\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\nX, y = classification_data(n_points= 500, noise= 0.5, p_dims= 2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nCode to graph a straight line\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)"
  },
  {
    "objectID": "posts/post6/homework6.html#when-alpha-is-chosen-appropriately-newtons-method-converges-to-the-correct-choice-of-w.",
    "href": "posts/post6/homework6.html#when-alpha-is-chosen-appropriately-newtons-method-converges-to-the-correct-choice-of-w.",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "1. When alpha is chosen appropriately, Newton’s method converges to the correct choice of w.",
    "text": "1. When alpha is chosen appropriately, Newton’s method converges to the correct choice of w.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(5000):\n    \n    opt.step(X, y, alpha = 0.5)\n    \n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n\ndef find_accuracy(X, y):\n\n    predictions = LR.predict(X)\n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 0.8980000019073486\n\n\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nPlot the loss over time over the 2000 iterations.\n\nimport numpy as np\n\ndef plot_loss(loss, label= \"\"):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(loss, color = \"blue\", label=label)\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFrom these visualizations, we can see that the weights chosen lead to high accuracy rates and a line that is pretty good at separating the data. We can also see that the loss converges over time."
  },
  {
    "objectID": "posts/post6/homework6.html#under-at-least-some-circumstances-newtons-method-can-converge-much-faster-than-standard-gradient-descent-in-the-sense-of-decreasing-the-empirical-risk.",
    "href": "posts/post6/homework6.html#under-at-least-some-circumstances-newtons-method-can-converge-much-faster-than-standard-gradient-descent-in-the-sense-of-decreasing-the-empirical-risk.",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "2. Under at least some circumstances, Newton’s method can converge much faster than standard gradient descent, in the sense of decreasing the empirical risk.",
    "text": "2. Under at least some circumstances, Newton’s method can converge much faster than standard gradient descent, in the sense of decreasing the empirical risk.\nFirst lets do standard gradient descent so we can compare it to Newton’s method later.\n\nLR_Stand = LogisticRegression() \nopt = GradientDescentOptimizer(LR_Stand)\n\n# initialize for main loop\nloss_vec_stand = []\n\nfor index in range(2000):\n    \n    # for vanilla gradient descent, alpha must be sufficiently small and beta must be 0\n    opt.step(X, y, alpha = 0.5, beta = 0)\n    loss = LR_Stand.loss(X, y).item()\n    loss_vec_stand.append(loss)\n\n\nfind_accuracy(X, y)\n\nAccuracy: 0.8980000019073486\n\n\nNow, lets show do Newton’s method to compare the decrease in empirical risk.\n\n# Use the same data as before\n\n# initialize a Logistic Regression and use Newton Optimizer\nLR_Newt = LogisticRegression() \nopt = NewtonOptimizer(LR_Newt)\n\nloss_vec_newt = []\n\nfor index in range(2000):\n    \n    opt.step(X, y, alpha = 50)\n    \n    loss = LR_Newt.loss(X, y).item()\n    loss_vec_newt.append(loss)\n\n\nfind_accuracy(X, y)\n\nAccuracy: 0.8980000019073486\n\n\nNow let’s compare the decrease in empirical risk:\n\nplt.plot(loss_vec_stand, label='Standard Gradient Descent')\nplt.plot(loss_vec_newt, color='green', label=\"Newton's Method\")\n\n\n# use log y axis\nplt.semilogy()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nHere we can see that Newton’s method converges at a much faster rate than Standard Gradient Descent with the same data and learning rate."
  },
  {
    "objectID": "posts/post6/homework6.html#if-alpha-is-too-large-newtons-method-fails-to-converge.",
    "href": "posts/post6/homework6.html#if-alpha-is-too-large-newtons-method-fails-to-converge.",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "3. If alpha is too large, Newton’s method fails to converge.",
    "text": "3. If alpha is too large, Newton’s method fails to converge.\n\n# initialize a Logistic Regression and Newton Optimizer\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\n# initialize for main loop\nloss_vec_alpha = []\n\nfor index in range(200):\n    \n    opt.step(X, y, alpha = 1010)\n    \n    loss = LR.loss(X, y).item()\n    loss_vec_alpha.append(loss)\n\n_LinAlgError: linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\n\n\n\nplot_loss(loss_vec_alpha)\n\n\n\n\n\n\n\n\nWith a high learning rate, we can see that Newton’s method does not cause empirical risk to converge, but instead after some time it begins to increase!"
  },
  {
    "objectID": "posts/post6/homework6.html#operation-counting",
    "href": "posts/post6/homework6.html#operation-counting",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "Operation Counting",
    "text": "Operation Counting\nThe goal is to answer these two questions:\n\nDoes Standard Gradient Descent or Newton’s method require less computational units?\nAnd, when p becomes very large, does it ever pay off to use Newton’s Method?\n\nTo answer these questions we make this set of assumptions that methods have these computational costs:\n\nComputing loss –&gt; c\nComputing grad –&gt; 2c\nComputing H –&gt; pc\nInverting pxp –&gt; \\(k_1p^\\gamma\\) where \\(2 \\leq \\gamma &lt;3\\)\nMultiplying by H –&gt; \\(k_2p^{2}\\)\n\nTo make a singular step for  Newton’s Method: \n\nCompute loss: c\nFor \\(t_\\mathrm{nm}\\) steps:\n\nCompute grad: \\(2c\\)\nCompute H : \\(pc\\)\nInvert H: \\(k_1p^\\gamma\\)\nMultiply by H: \\(k_2p^{2}\\)\n\n\nTotal time: \\[O(c + t_\\mathrm{nm} * (2c + pc + k_1p^\\gamma + k_2p^{2}))\\]\nTo make a singular step for  Standard Gradient Descent: \n\nCompute loss: \\(c\\)\nFor \\(t_\\mathrm{gd}\\) steps:\n\nCompute grad: \\(2c\\)\n\n\nTotal time: \\[O(c + t_\\mathrm{gd} * 2c)\\]\nIf \\[O(GD) = c + t_\\mathrm{gd} * 2c\\] and \\[O(NM) = c + t_\\mathrm{nm} * (2c + pc + k_1p^\\gamma + k_2p^{2})\\] then \\[\\frac{t_\\mathrm{nm}}{t_\\mathrm{gd}} = \\frac{2c + pc + k_1p^\\gamma + k_2p^{2}}{2c}\\]\nThis critical ratio determines the relative computational cost between Newton’s method and Standard Gradient Descent as \\(p\\) increases. It can be simplified to \\[1 + \\frac{p}{2} + \\frac{k_1p^\\gamma}{2c} +  \\frac{k_2p^{2}}{2c}\\]\nTherefore as \\(p\\) shows larger, the critical ratio, and thereby the relative computational expense of Newton’s method and Standard Gradient Descent grows polynomially, determined by \\[k_1p^\\gamma + k_2p^{2}\\] In contrast, the computational cost of Standard Gradient does not depend on \\(p\\)."
  },
  {
    "objectID": "posts/post6/homework6.html#conclusion",
    "href": "posts/post6/homework6.html#conclusion",
    "title": " Newton’s Method for Logistic Regression ",
    "section": "Conclusion",
    "text": "Conclusion\nCreating this blog post allowed me to understand the implementation and specific applications of Newton’s method. I was able to visualize how it is able to converge, even at a faster rate than Standard Gradient Descent under certain circumstances. However, I was also able to understand the circumstances in which Newton’s method fails. If alpha is too large Newton’s method will fail to converge. Additionally, Newton’s method has a much higher time complexity which can become significant when p is large.\nThrough the experience of implementing this blog post, I was able to understand how different machine learning methods could be applied depending on the specifics of the data to be trained on. With large amounts of data, like many machine learning problems today, Newton’s method might be too large to compute. Overall, I was able to learn more about Machine learning models and how to apply them to different data."
  },
  {
    "objectID": "posts/post1/homework1.html",
    "href": "posts/post1/homework1.html",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nTo start my exploration into the penguin data, I first sough to understand the data by plotting it\n\nimport seaborn as sns\n\n# body mass vs specie with colors for gender\n\n# replace the column with the first word in each entry\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nsns.catplot(data=train, x=\"Species\", y=\"Body Mass (g)\", hue=\"Sex\")\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis plot shows penguins as points colored by sex, with species as the x-axis and body mass (in grams) on the y-axis. This plot displays that on average, male penguins are larger than female penguins. It also shows a trend of Gentoo species of penguins larger than Chinstrap or Adelie penguins- who are relatively similar in size. This indicates that Body Mass (g) could potentially be a useful indicator in indicating species type. It is also important to note that the separation of gender allows us to make these assertions. Gento male penguins are generally larger than Adelie male penguins and the same follows with female penguins of these same two species. However, Gentoo female penguins are roughly the same size as Adelie male penguins. Therefore gender might be a useful tool in predicting penguin species.\n\nsns.catplot(data=train, kind=\"violin\", x=\"Delta 13 C (o/oo)\", y=\"Island\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis plot shows the penguins of different species with different colors graphed on Delta 13 C (o/oo) and Island location. Delta 13(o/oo) is an interesting indicator as it is widely used as an indicator of diet especially in reference to vegetation. This therefore shows that there could be a large difference in diet between Chinstrap and the other two species of penguins. Adelie penguins are present in all three islands whereas Gentoo and Chinstrap penguins are isolated to one of the three. Therefore Island could be a helpful indicator of species type, but probably would not be indicative by itself.\n\ntable = train.groupby(['Species', 'Sex']).aggregate({'Culmen Length (mm)' : ['min', 'max','mean']})\nprint(table)\n\n                 Culmen Length (mm)                 \n                                min   max       mean\nSpecies   Sex                                       \nAdelie    FEMALE               34.0  42.2  37.426415\n          MALE                 34.6  46.0  40.404918\nChinstrap FEMALE               40.9  58.0  46.722581\n          MALE                 49.0  55.8  51.334615\nGentoo    .                    44.5  44.5  44.500000\n          FEMALE               40.9  50.5  45.455102\n          MALE                 44.4  55.9  49.006818\n\n\nThis table indicates that on average male penguins have longer Culmen Length than their female counterparts. Male Gentoo penguins have longer Culmens then male Chinstraps who have longer Culmens than male Adelie penguins while the order for female penguins descends for Chinstrap, Gentoo, then Adelie. These distinctions could serve useful in training our model to differentiate species. It also serves to note that the gender distinction matters when comparing species.\n\n\n\nI then attempted to find three features of the data (one qualitative and two quantitative) and train a model on these features that achieves 100% testing accuracy.\nFirst I did data preparation to prepare the quantitative columns of the data\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nI completed an exhaustive iterative search to find the best markers. I used cross validation one each combination of one qualitative and two quantitative column with different models keeping track of the best score for each model. In terms of models I used a Logistic Regression, Decision Tree Classifier, SVC, and Random Forrest Classifier. For the Decision Tree Classifier and the Random Forrest Classifier I used grid search across the amount of parameters to identify the optimal maximum depth. For SVC I used grid search to find the optimal value for gamma across a wide array of numbers. With the best markers of each model, I also saved the columns of the best score.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\",'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)',]\n\nbestLRScore = 0.0\nbestLRCols=[]\ncol_combos = []\n\n\nbestDTCScore = 0.0\nbestDTCCols=[]\n\nbestSVMScore = 0.0\nbestSVMCols=[]\n\nbestRFCScore = 0.0\nbestRFCCols=[]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  \n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n        \n    LRScore = cross_val_score(LR, X_train[cols], y_train, cv=5).mean()\n    \n    # keep track of best Logistic Regression Score\n    if LRScore &gt;= bestLRScore :\n      bestLRScore = LRScore\n      bestLRCols = cols\n        \n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n    \n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCScore  = grid_search.best_score_\n        \n    if(DTCScore &gt; bestDTCScore):\n      bestDTCScore = DTCScore\n      bestDTCCols = cols\n      bestDTCDepth = grid_search.best_params_\n      \n    \n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    RFCScore  = grid_search.best_score_\n    \n    # keep track of best RFC Score  \n    if(RFCScore &gt; bestRFCScore):\n      bestRFCScore = RFCScore\n      bestRFCCols = cols\n      bestRFCDepth = grid_search.best_params_\n    \n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    SVMScore  = grid_search.best_score_   \n    \n    # keep track of best SVM Score\n    if(SVMScore &gt; bestSVMScore):\n      bestSVMScore = SVMScore\n      bestSVMCols = cols\n        \n\n  \nprint(\"best LR\",bestLRCols, \":\", bestLRScore)\nprint(\"best DTC\",bestDTCCols, \":\", bestDTCScore)\nprint(\"best max depth:\", bestDTCDepth)\nprint(\"best RFC\", bestRFCCols, \":\", bestRFCScore)\nprint(\"best max depth:\", bestRFCDepth)\nprint(\"best SVM\", bestSVMCols, \":\", bestSVMScore)\n\nbest LR ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9883107088989442\nbest DTC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9765460030165913\nbest max depth: {'max_depth': 7}\nbest RFC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9844645550527904\nbest max depth: {'max_depth': 5}\nbest SVM ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9805429864253394\n\n\nFrom my iterative search through the features and the different models I was able to find that the best score was Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex. This model with these features had a 0.9883107088989442% classification rate. This made sense as I noted in my data exploration that sex was an important distinction to make when comparing different features to classify teh species.\n\n\n\nNext I prepared the test data by shorting my Species column and identifying selected columns of the best fitting models- Culmen Length (mm), Culmen Depth (mm), and Sex for Logistic Regression. On my test data I achieved 100% testing accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\n\nselected_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\n\nX_train_selected = X_train[selected_cols]\nX_test_selected = X_test[selected_cols]\n\nLR = LogisticRegression()\nLR.fit(X_train_selected, y_train)\ntestScore = LR.score(X_test_selected, y_test)\n\nprint(testScore)\n\n1.0\n\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nI visualized how my model worked on the training and test data by plotting the data and displaying the decision regions of my model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n        X.columns[0] : XX,\n        X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train_selected, y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test_selected, y_test)\n\n\n\n\n\n\n\n\n\n\n\nFinally, to visualize the successful identification of my model with my three chosen characteristics I used a confusion matrix on the testing data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_selected)\nC = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_test_pred, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nThis is a perfect confusion matrix as the model has 100% testing accuracy in classifying the penguins. This result is exactly what we want.\n\n\n\nThis blog post taught me many things. Firstly, it allowed me to gain practice exploring and understanding data. I was able to explore the Palmer Penguin data set by creating tables and graphs. Then I was able to use the knowledge gained by the exploration to train my own classifier. I was able to become familiar with various different machine learning models through this assignment: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, and Support Vector Machines. Through this process, I learned about grid search and how to find optimal parameters. I then got practice training a Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex and finally testing it on our test data to get a 100% accuracy rate. Not only did I become more familiar with this particular data set, but I was able to learn skills in how to analyze data and build and test Machine Learning Models."
  },
  {
    "objectID": "posts/post1/homework1.html#introduction",
    "href": "posts/post1/homework1.html#introduction",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/post1/homework1.html#explore",
    "href": "posts/post1/homework1.html#explore",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "To start my exploration into the penguin data, I first sough to understand the data by plotting it\n\nimport seaborn as sns\n\n# body mass vs specie with colors for gender\n\n# replace the column with the first word in each entry\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nsns.catplot(data=train, x=\"Species\", y=\"Body Mass (g)\", hue=\"Sex\")\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis plot shows penguins as points colored by sex, with species as the x-axis and body mass (in grams) on the y-axis. This plot displays that on average, male penguins are larger than female penguins. It also shows a trend of Gentoo species of penguins larger than Chinstrap or Adelie penguins- who are relatively similar in size. This indicates that Body Mass (g) could potentially be a useful indicator in indicating species type. It is also important to note that the separation of gender allows us to make these assertions. Gento male penguins are generally larger than Adelie male penguins and the same follows with female penguins of these same two species. However, Gentoo female penguins are roughly the same size as Adelie male penguins. Therefore gender might be a useful tool in predicting penguin species.\n\nsns.catplot(data=train, kind=\"violin\", x=\"Delta 13 C (o/oo)\", y=\"Island\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis plot shows the penguins of different species with different colors graphed on Delta 13 C (o/oo) and Island location. Delta 13(o/oo) is an interesting indicator as it is widely used as an indicator of diet especially in reference to vegetation. This therefore shows that there could be a large difference in diet between Chinstrap and the other two species of penguins. Adelie penguins are present in all three islands whereas Gentoo and Chinstrap penguins are isolated to one of the three. Therefore Island could be a helpful indicator of species type, but probably would not be indicative by itself.\n\ntable = train.groupby(['Species', 'Sex']).aggregate({'Culmen Length (mm)' : ['min', 'max','mean']})\nprint(table)\n\n                 Culmen Length (mm)                 \n                                min   max       mean\nSpecies   Sex                                       \nAdelie    FEMALE               34.0  42.2  37.426415\n          MALE                 34.6  46.0  40.404918\nChinstrap FEMALE               40.9  58.0  46.722581\n          MALE                 49.0  55.8  51.334615\nGentoo    .                    44.5  44.5  44.500000\n          FEMALE               40.9  50.5  45.455102\n          MALE                 44.4  55.9  49.006818\n\n\nThis table indicates that on average male penguins have longer Culmen Length than their female counterparts. Male Gentoo penguins have longer Culmens then male Chinstraps who have longer Culmens than male Adelie penguins while the order for female penguins descends for Chinstrap, Gentoo, then Adelie. These distinctions could serve useful in training our model to differentiate species. It also serves to note that the gender distinction matters when comparing species."
  },
  {
    "objectID": "posts/post1/homework1.html#model",
    "href": "posts/post1/homework1.html#model",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "I then attempted to find three features of the data (one qualitative and two quantitative) and train a model on these features that achieves 100% testing accuracy.\nFirst I did data preparation to prepare the quantitative columns of the data\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nI completed an exhaustive iterative search to find the best markers. I used cross validation one each combination of one qualitative and two quantitative column with different models keeping track of the best score for each model. In terms of models I used a Logistic Regression, Decision Tree Classifier, SVC, and Random Forrest Classifier. For the Decision Tree Classifier and the Random Forrest Classifier I used grid search across the amount of parameters to identify the optimal maximum depth. For SVC I used grid search to find the optimal value for gamma across a wide array of numbers. With the best markers of each model, I also saved the columns of the best score.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\",'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)',]\n\nbestLRScore = 0.0\nbestLRCols=[]\ncol_combos = []\n\n\nbestDTCScore = 0.0\nbestDTCCols=[]\n\nbestSVMScore = 0.0\nbestSVMCols=[]\n\nbestRFCScore = 0.0\nbestRFCCols=[]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  \n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n        \n    LRScore = cross_val_score(LR, X_train[cols], y_train, cv=5).mean()\n    \n    # keep track of best Logistic Regression Score\n    if LRScore &gt;= bestLRScore :\n      bestLRScore = LRScore\n      bestLRCols = cols\n        \n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n    \n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCScore  = grid_search.best_score_\n        \n    if(DTCScore &gt; bestDTCScore):\n      bestDTCScore = DTCScore\n      bestDTCCols = cols\n      bestDTCDepth = grid_search.best_params_\n      \n    \n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    RFCScore  = grid_search.best_score_\n    \n    # keep track of best RFC Score  \n    if(RFCScore &gt; bestRFCScore):\n      bestRFCScore = RFCScore\n      bestRFCCols = cols\n      bestRFCDepth = grid_search.best_params_\n    \n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    SVMScore  = grid_search.best_score_   \n    \n    # keep track of best SVM Score\n    if(SVMScore &gt; bestSVMScore):\n      bestSVMScore = SVMScore\n      bestSVMCols = cols\n        \n\n  \nprint(\"best LR\",bestLRCols, \":\", bestLRScore)\nprint(\"best DTC\",bestDTCCols, \":\", bestDTCScore)\nprint(\"best max depth:\", bestDTCDepth)\nprint(\"best RFC\", bestRFCCols, \":\", bestRFCScore)\nprint(\"best max depth:\", bestRFCDepth)\nprint(\"best SVM\", bestSVMCols, \":\", bestSVMScore)\n\nbest LR ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9883107088989442\nbest DTC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9765460030165913\nbest max depth: {'max_depth': 7}\nbest RFC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9844645550527904\nbest max depth: {'max_depth': 5}\nbest SVM ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9805429864253394\n\n\nFrom my iterative search through the features and the different models I was able to find that the best score was Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex. This model with these features had a 0.9883107088989442% classification rate. This made sense as I noted in my data exploration that sex was an important distinction to make when comparing different features to classify teh species."
  },
  {
    "objectID": "posts/post1/homework1.html#test-the-models-on-the-test-data",
    "href": "posts/post1/homework1.html#test-the-models-on-the-test-data",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Next I prepared the test data by shorting my Species column and identifying selected columns of the best fitting models- Culmen Length (mm), Culmen Depth (mm), and Sex for Logistic Regression. On my test data I achieved 100% testing accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\n\nselected_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\n\nX_train_selected = X_train[selected_cols]\nX_test_selected = X_test[selected_cols]\n\nLR = LogisticRegression()\nLR.fit(X_train_selected, y_train)\ntestScore = LR.score(X_test_selected, y_test)\n\nprint(testScore)\n\n1.0\n\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "posts/post1/homework1.html#plotting-decision-regions",
    "href": "posts/post1/homework1.html#plotting-decision-regions",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "I visualized how my model worked on the training and test data by plotting the data and displaying the decision regions of my model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n        X.columns[0] : XX,\n        X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train_selected, y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test_selected, y_test)"
  },
  {
    "objectID": "posts/post1/homework1.html#confusion-matrix",
    "href": "posts/post1/homework1.html#confusion-matrix",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Finally, to visualize the successful identification of my model with my three chosen characteristics I used a confusion matrix on the testing data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_selected)\nC = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_test_pred, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nThis is a perfect confusion matrix as the model has 100% testing accuracy in classifying the penguins. This result is exactly what we want."
  },
  {
    "objectID": "posts/post1/homework1.html#discussion",
    "href": "posts/post1/homework1.html#discussion",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "This blog post taught me many things. Firstly, it allowed me to gain practice exploring and understanding data. I was able to explore the Palmer Penguin data set by creating tables and graphs. Then I was able to use the knowledge gained by the exploration to train my own classifier. I was able to become familiar with various different machine learning models through this assignment: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, and Support Vector Machines. Through this process, I learned about grid search and how to find optimal parameters. I then got practice training a Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex and finally testing it on our test data to get a 100% accuracy rate. Not only did I become more familiar with this particular data set, but I was able to learn skills in how to analyze data and build and test Machine Learning Models."
  },
  {
    "objectID": "posts/finalProj/DataCleaning.html",
    "href": "posts/finalProj/DataCleaning.html",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder"
  },
  {
    "objectID": "posts/finalProj/DataCleaning.html#read-in-data",
    "href": "posts/finalProj/DataCleaning.html#read-in-data",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Read in data",
    "text": "Read in data\n\nallergies = pd.read_csv('allergies.csv')\ncareplans = pd.read_csv('careplans.csv')\nconditions = pd.read_csv('conditions.csv')\nencounters = pd.read_csv('encounters.csv') ##NOT USING RN, DO WE NEED?\nimmunizations = pd.read_csv('immunizations.csv')\nmedications = pd.read_csv('medications.csv')\nobservations = pd.read_csv('observations.csv')\npatients = pd.read_csv('patients.csv')\nprocedures = pd.read_csv('procedures.csv')\n\n\nlen(patients[patients['ethnicity'] == 'scottish']['ethnicity'])\n\n27"
  },
  {
    "objectID": "posts/finalProj/DataCleaning.html#clean-up-dataframes-have-one-row-per-patient",
    "href": "posts/finalProj/DataCleaning.html#clean-up-dataframes-have-one-row-per-patient",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Clean up dataframes: have one row per patient",
    "text": "Clean up dataframes: have one row per patient\n\n## ALLERGIES\nallergies_pivot = pd.get_dummies(allergies['DESCRIPTION'])\nallergies_pivot['PATIENT'] = allergies['PATIENT']\nallergies_pivot = allergies_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## CAREPLANS\ncareplans_pivot = pd.get_dummies(careplans['DESCRIPTION'])\ncareplans_pivot['PATIENT'] = careplans['PATIENT']\ncareplans_pivot = careplans_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## CONDITIONS\nconditions_pivot = pd.get_dummies(conditions['DESCRIPTION'])\nconditions_pivot['PATIENT'] = conditions['PATIENT']\nconditions_pivot = conditions_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## IMMUNIZATIONS\nimmunizations_pivot = pd.get_dummies(immunizations['DESCRIPTION'])\nimmunizations_pivot['PATIENT'] = immunizations['PATIENT']\nimmunizations_pivot = immunizations_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## MEDICATIONS\nmedications_pivot = pd.get_dummies(medications['DESCRIPTION'])\nmedications_pivot['PATIENT'] = medications['PATIENT']\nmedications_pivot = medications_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## OBSERVATIONS\nobservations['VALUE'] = pd.to_numeric(observations['VALUE'], errors='coerce')\n\n# Pivot table with mean aggregation\nobservations_pivot= observations.pivot_table(index=observations.index, columns='DESCRIPTION', values='VALUE', fill_value=0, aggfunc='mean')\nobservations_pivot['PATIENT'] = observations['PATIENT']\nobservations_pivot = observations_pivot.groupby('PATIENT').sum().reset_index()\n\nobservations_pivot.to_csv('observations_pivot.csv', index=False)\n\n\n## PROCEDURES\nprocedures_pivot = pd.get_dummies(procedures['DESCRIPTION'])\nprocedures_pivot['PATIENT'] = procedures['PATIENT']\nprocedures_pivot = procedures_pivot.groupby('PATIENT').sum().reset_index()\n\n\nREMOVE MA US from Patient info (they are all from mass)\n\n#import re\nplaces = patients['birthplace']\ncleaned_places = [place.replace(\" MA US\", \"\") if \" MA US\" in place else place for place in places]\n\n\npatients['birthplace'] = cleaned_places\n\n\ntown_names = patients['birthplace'].unique().tolist()\n#some towns do not appear in birthplace but appear in current addresses, I modified the function below to print these towns and create this list, then\n# reverted to the function so it wouldn't have a long output\ntowns_not_in_bp = ['Sandwich', 'Uxbridge', 'Holland', 'Monson', 'Wendell', 'Wayland', 'Rochester', 'Belchertown', 'Lanesborough', 'Hatfield',\n                  'Georgetown', 'Lakeville', 'Princeton', 'Blackstone', 'Hinsdale', 'Harvard', 'Chesterfield', 'Wellfleet', 'Northfield', 'Hubbardston',\n                  'Windsor', 'Wales', 'Sandisfield', 'Bolton', 'Truro', 'Southwick', 'Sheffield', 'Scituate', 'Halifax', 'Nahant', 'Stockbridge', 'Berlin']\n                \nfor town in towns_not_in_bp:\n    town_names.append(town)\n\n\ndef extract_town_name(address, town_list):\n    # Check if any part of the address matches any town name in the list\n    for town in town_list:\n        if town in address:\n            return town\n    print(address)\n    return None\n\npatients['curr_town'] = \"\"\n\nfor index, address in enumerate(patients['address']):\n    # Extract the town name\n    town_name = extract_town_name(address, town_names)\n    \n    # Assign the extracted town name to the corresponding entry in the 'curr_town' column\n    patients.at[index, 'curr_town'] = town_name\n\n\npatients.to_csv('patient_clean.csv', index=False)"
  },
  {
    "objectID": "posts/finalProj/DataCleaning.html#add-suffixes-to-columns",
    "href": "posts/finalProj/DataCleaning.html#add-suffixes-to-columns",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Add suffixes to columns",
    "text": "Add suffixes to columns\n\npatients = patients.rename(columns={'patient': 'PATIENT'})\n\ndef add_suffix(df, suffix):\n    renamed_columns = {}\n    for col_name in df.columns:\n        if col_name != 'PATIENT':\n            renamed_columns[col_name] = col_name + '_' + suffix\n        else:\n            renamed_columns[col_name] = col_name\n    return df.rename(columns=renamed_columns)\n\nallergies_clean = add_suffix(allergies_pivot, 'ALLERGIES')\ncareplans_clean = add_suffix(careplans_pivot, 'CAREPLANS')\nconditions_clean = add_suffix(conditions_pivot, 'CONDITIONS')\nimmunizations_clean = add_suffix(immunizations_pivot, 'IMMUNIZATIONS')\nmedications_clean = add_suffix(medications_pivot, 'MEDICATIONS')\nobservations_pivot = add_suffix(observations_pivot, 'OBSERVATIONS')\nprocedures_clean = add_suffix(procedures_pivot, 'PROCEDURES')"
  },
  {
    "objectID": "posts/finalProj/DataCleaning.html#prep-data",
    "href": "posts/finalProj/DataCleaning.html#prep-data",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Prep Data",
    "text": "Prep Data\n\nle = LabelEncoder()\n\n# our data-prepping function for modeling\ndef prep_data(patients, conditions, illness_descriptions, observations):\n\n    # make patients column match others for merging, drop unnecessary information and NA vals\n    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n    patients = patients.drop(columns=['birthdate', 'marital','deathdate','ssn', 'address', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n    patients = patients.dropna()\n    conditions = conditions.dropna()\n\n    # merge datasets (patient info and corresponding conditions)\n    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n\n    # create y\n    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n    merged_df = merged_df.drop(columns=illness_descriptions)\n    \n    # return split x, y, and all of the code tracking dicts\n    return merged_df"
  },
  {
    "objectID": "posts/finalProj/DataCleaning.html#merge-datasets",
    "href": "posts/finalProj/DataCleaning.html#merge-datasets",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Merge datasets",
    "text": "Merge datasets\nSeparate conditions dataframe based on disease group\n\n## DIABETES\nillness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n# dataset to be used for analysis\nsubset_conditions.to_csv('conditions_diabetes.csv', index=False)\n\n\n## PREGNANCY\nillness_descriptions = ['PATIENT','Miscarriage in first trimester_CONDITIONS','Miscarriage in second trimester_CONDITIONS',\n                        'Complication occuring during pregnancy_CONDITIONS','Preeclampsia_CONDITIONS', 'Antepartum eclampsia_CONDITIONS',\n                        'Tubal pregnancy_CONDITIONS', 'Congenital uterine anomaly_CONDITIONS', 'Blighted ovum_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n# dataset to be used for analysis\nsubset_conditions.to_csv('conditions_pregnancy.csv', index=False)\n\n\n## CANCER\nillness_descriptions = ['PATIENT','Non-small cell lung cancer (disorder)_CONDITIONS', 'Non-small cell carcinoma of lung  TNM stage 4 (disorder)_CONDITIONS',\n                        'Primary small cell malignant neoplasm of lung  TNM stage 4 (disorder)_CONDITIONS','Non-small cell carcinoma of lung  TNM stage 2 (disorder)_CONDITIONS',\n                        'Non-small cell lung cancer (disorder)_CONDITIONS', 'Suspected lung cancer (situation)_CONDITIONS', 'Malignant tumor of colon_CONDITIONS',\n                        'Overlapping malignant neoplasm of colon_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n\nsubset_conditions.to_csv('conditions_cancer.csv', index=False)\n\n\n## HEART\nillness_descriptions = ['PATIENT','Coronary Heart Disease_CONDITIONS', 'History of cardiac arrest (situation)_CONDITIONS', 'Cardiac Arrest_CONDITIONS',\n                        'History of myocardial infarction (situation)_CONDITIONS', 'Myocardial Infarction_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\nsubset_conditions.to_csv('conditions_heart.csv', index=False)\n\n\n## LUNGS\nillness_descriptions = ['PATIENT','Asthma_CONDITIONS', 'Pulmonary emphysema (disorder)_CONDITIONS', 'Seasonal allergic rhinitis_CONDITIONS', \n                        'Acute bronchitis (disorder)_CONDITIONS', 'Chronic obstructive bronchitis (disorder)_CONDITIONS',\n                        'Childhood asthma_CONDITIONS', 'Perennial allergic rhinitis with seasonal variation_CONDITIONS',\n                        'Perennial allergic rhinitis_CONDITIONS', 'Acute bacterial sinusitis (disorder)_CONDITIONS', 'Chronic sinusitis (disorder)_CONDITIONS',\n                        'Sinusitis (disorder)_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\nsubset_conditions.to_csv('conditions_lungs.csv', index=False)"
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html",
    "href": "posts/finalProj/PostHomePage.html",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "",
    "text": "In this project we explore the implementation of a machine learning model designed to make predictions concerning health outcomes. We aim to train and analyze the results of these models. We ultimately hope to identify bias from the risk scores, while acknowledging that differences in illness rates can naturally vary and are not necessarily indicative of bias. We use two model types (decision trees and random forests), found to be the optimal performers for our data, to generate risk scores for every patient in a synthetic medical data set created by Synthea [SyntheaData]. We chose six condition “groups” to study, combining data from multiple related conditions into one has/does not have target feature. These groups included pulmonary diseases, diabetes and comorbidities, cardiovascular diseases, pregnancy and pregnancy complications, and cancer. We then interpreted risk factors for these categories across race, ethnicity, birthplace, and current town of residence. We then compared and analyzed risk scores across identities and illness categories. A significant portion of our results showed that patients of colors, and patients born in or residing in less wealthy towns have higher risk factors for some conditions, pointing to the influence of environmental and social inequity factors. Some results, however, were more evenly spaced and harder to interpret. Our findings reveal disparities in risk factors among different demographics, emphasizing the impact of environmental and social inequities on health outcomes and the need for further investigation and analysis."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#abstract",
    "href": "posts/finalProj/PostHomePage.html#abstract",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "",
    "text": "In this project we explore the implementation of a machine learning model designed to make predictions concerning health outcomes. We aim to train and analyze the results of these models. We ultimately hope to identify bias from the risk scores, while acknowledging that differences in illness rates can naturally vary and are not necessarily indicative of bias. We use two model types (decision trees and random forests), found to be the optimal performers for our data, to generate risk scores for every patient in a synthetic medical data set created by Synthea [SyntheaData]. We chose six condition “groups” to study, combining data from multiple related conditions into one has/does not have target feature. These groups included pulmonary diseases, diabetes and comorbidities, cardiovascular diseases, pregnancy and pregnancy complications, and cancer. We then interpreted risk factors for these categories across race, ethnicity, birthplace, and current town of residence. We then compared and analyzed risk scores across identities and illness categories. A significant portion of our results showed that patients of colors, and patients born in or residing in less wealthy towns have higher risk factors for some conditions, pointing to the influence of environmental and social inequity factors. Some results, however, were more evenly spaced and harder to interpret. Our findings reveal disparities in risk factors among different demographics, emphasizing the impact of environmental and social inequities on health outcomes and the need for further investigation and analysis."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#introduction",
    "href": "posts/finalProj/PostHomePage.html#introduction",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Introduction",
    "text": "Introduction\nFor our project, we aim to explore the relationship between diseases and social factors such as sex, race, and town, and how these may reflect societal and environmental inequities. We understand from academic literature like “Diabetes Complications in Racial and Ethnic Minority Populations in the USA” [Haw2021] and “The Black-White Disparity in Pregnancy-Related Mortality from 5 Conditions: Differences in Prevalence and Case-Fatality Rates” [Tucker2007] that there exist severe inequities in risk rates across different identity groups. Building on this literature, we are interested in building a machine learning model that reveal these patterns in the US healthcare system.\nOur approach is to identify the most accurate predictive model for our dataset, then use this model to generate risk likelihood scores and evaluate the relationship between different diseases and characteristics indicative of societal inequalities. We will then analyze the implications of these risk factors for inequitable, identity-based risk factors in health outcomes and complications. Our project consists of three documents, one in which we clean our original data, one in which we explore this data visually, one in which we build and explore our models, and the final one, this one, in which we describe our motivations, background, results, and analysis. Taking the general trends we witness in our data visualization document, we carried out the second half of our project; building a model that predicts risk scores.\nWe were inspired by Obermeyer and colleagues work [Obermeyer2019] in analyzing the bias present in machine learning models used to guide healthcare decisions by using health costs as a proxy measure for health. This model used pre-existing bias in our healthcare system to make decisions that further marginalized oppressed identity groups. Our ultimate goal is that our models will bring to light existing inequities. Comparing the risk scores, we analyze whether trends emerged in terms of socioeconomic status (which we measure by the proxy of town of birth and residence), race, gender, and ethnicity. We then used Barocas et al.’s paper [Barocas2023] to analyze the implications of our findings in terms of the three definitions of fairness. Finally, we referenced articles like “Explanatory learner models: Why machine learning (alone) is not the answer” [Rosé2019] to analyze the potential negative impact of relying on machine learning models in important decision contexts and proposed solutions to this dilemma."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#values-statement",
    "href": "posts/finalProj/PostHomePage.html#values-statement",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Values Statement",
    "text": "Values Statement\nThe motivation behind our project was to uncover potential inequities in the manifestations of certain conditions, for example does a persons race or socioeconomic status predispose them to certain conditions more than others. Our goal was to identify potential societal and environmental factors that unjustly, or disproportionately contribute to disparities in health outcomes. Our focus on this project stems from a desire to understand and address societal and evironmental inequities that contribute to disparities in health outcomes, and our personal commitments to promoting equity and social justice in healthcare.\nThe primary potential users of our project would include researchers, policymakers, and public health organizations interested in understanding and addressing health inequities. However, the project’s findings and potential implications could also affect the communities we study, especially those that we find experience disparities in health outcomes due to social determinants.\nIf our research were to be taken out of context by researchers and health professionals, and taken to be a study of biological predisposition, and not of the manifestation of social factors, our results may reinforce assumptions about health outcomes by race and ethnicity in the medical field, enforcing harmful stereotypes or leading to further marginalization of certain groups. Additionally, if the data or models have inherent biases, they could perpetuate or amplify existing disparities.\nWith proper usage and implementation though, we hope our results would positively impact public health programs and initiatives that work in preventative measures in the most at-risk communities. With our data, we hope that these measures would more easily identify communities in which to center efforts and awareness campaigns, by shedding light on health inequities and informing efforts to address them."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#material-methods",
    "href": "posts/finalProj/PostHomePage.html#material-methods",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Material & Methods",
    "text": "Material & Methods\n\nOur Data\nOur project utilizes a synthetic data set created for an Introduction to Biomedical Data Science Textbook, [SyntheaData]. The data was created using Synthea, a synthetic patient generator that models the medical history of synthetic patients. Synthea’s mission is “to output high-quality synthetic, realistic but not real, patient data and associated health records covering every aspect of healthcare.” This allowed for much easier access than real patient data, as well as alleviating any privacy concerns that would arise from using real patient data. The link to the data can be found here: https://data.world/siyeh/synthetic-medical-data.\nOur dataset was originally quite large, with over 200 million entries. After thorough data cleaning and preprocessing, the data was then transformed to multiple CSV documents, generally with the format of each row representing a different patient with one-hot-encoded values for multiple disease conditions.\nWhile using synthetic data has its benefits, it is essential to acknowledge certain inherent limitations. Firstly, despite efforts to create diverse and representative synthetic patients, there may still be discrepancies in representing certain demographic groups or medical conditions accurately. Certain rare or uncommon medical conditions may be underrepresented in the dataset due to the limitations of the modeling and analyses processes. This data is generated to represent patients from Massachusetts, so contains a population representative of this state which is primarily White, wealthy, and educated and does not accurately represent the diversity of the rest of the Unites States. Therefore, any generalization of results must proceed with caution. Thus while this synthetic dataset serves as a valuable resource for educational purposes, researchers and practitioners should approach its use with an understanding of its limitations.\nAfter cleaning our data, we performed exploratory data analysis in order to visualize out dataset, the results of which are found in this extension of our materials and methods section, our exploratory data analysis section.\n\n\nOur Approach\nSince our original dataset was quite large, a thorough procedure of data cleaning and preprocessing was needed, as well as an evaluation of which parts and features of our data should be actively used as predictors for our models. We subset our data into different CSV files, each entry to a given CSV corresponding to the different category of condition. This allowed for our models to be trained more concisely and efficiently, as well as increased the interpretability of results. This extension of methods and materials shows our data cleaning process in more depth.\nMultiple models were trained using cross-validation for each analysis of a condition group, including a logistic regression model, a decision tree classifier, a random forest classifier, and a support vector machine. These models were then evaluated for best score for a specific condition group. The best model, i.e. the one returning the highest cross-validated accuracy, was chosen as the predictive model for our general risk scores. We then trained this optimal model on our training dataset, and created predictions for our testing data that represented the probability of each entry being 1 (having a certain condition) or 0 (not having a certain condition). A risk score could then be anything between 0.00 and 1.00, where 0.50 would represent a 50% probability that the given patient has a condition. The models ran on our own personal devices, on the ML-0451 class kernel."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#results",
    "href": "posts/finalProj/PostHomePage.html#results",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Results",
    "text": "Results\nThe first extension of our results section is this document, in which we run our model and generate the risk scores and comparisons we discuss in further depth here.\nThe results of our risk score predictions varied widely. It is important to remember that the foundation of our project is based on interpreting our model’s perceptions of different group’s likelihoods of having a certain condition, and so they might reasonably disagree with actual trends in condition prevalence. With this in mind, in this section we will delve more deeply into the findings of our model.\nFirst, we wanted to inspect prevalence by race, to understand whether one racial group was more often assigned higher risk scores than others across conditions.\n\nResults by Race:\n\n# importing results for visualization\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nrace_risk = pd.DataFrame()\n\nrace_risk['Race'] = ['White', 'Black', 'Hispanic', 'Asian']\nrace_risk['Diabetes'] = [0.312536, 0.256158, 0.340659, 0.479592]\nrace_risk['Pregnancy'] = [0.034260, 0.051395, 0.038217, 0.037262]\nrace_risk['Cancer'] = [0.051942, 0.046859, 0.051650, 0.034009]\nrace_risk['Heart'] = [0.502101, 0.491353, 0.491280, 0.510746]\nrace_risk['Lung'] = [0.507817, 0.509092, 0.492106, 0.514338]\n\nprint(race_risk)\nprint('Fig. 1. Table of predicted risk score values by race for each condition subset.')\n\n       Race  Diabetes  Pregnancy    Cancer     Heart      Lung\n0     White  0.312536   0.034260  0.051942  0.502101  0.507817\n1     Black  0.256158   0.051395  0.046859  0.491353  0.509092\n2  Hispanic  0.340659   0.038217  0.051650  0.491280  0.492106\n3     Asian  0.479592   0.037262  0.034009  0.510746  0.514338\nFig. 1. Table of predicted risk score values by race for each condition subset.\n\n\n\n# plotting\ntransposed_race_risk = race_risk.set_index('Race').T\ntransposed_race_risk.plot(kind=\"bar\")\n\nplt.title('Bar Chart of Health Risks by Race')\nplt.xlabel('Race')\nplt.ylabel('Risk Score %')\n# scale y axis by 100 to show percentage\nplt.yscale('linear')  \nplt.ylim(0, 1) \n\nplt.xticks(rotation=45)\nplt.show()\n\nprint('Fig. 2. Bar chart of predicted risk score values by race for each condition subset. Risk scores are given in percentage.')\n\n\n\n\n\n\n\n\nFig. 2. Bar chart of predicted risk score values by race for each condition subset. Risk scores are given in percentage.\n\n\nFrom this table and plot we can see that risk scores are generally evenly-distributed across racial categories, meaning our model does not take one racial identity as a strong risk factor for all diseases indiscriminately.\nWhat was interesting as we generated our predictions was that both heart and lung disease both predicted near equal risk scores, all around 50%, for all racial groups. This finding disagrees with real medical literature, which shows that Black patients are statistically more likely to experience any pulmonary defects [LungConditionsbyRace]. Part of what could explain our high risk for lung disease across the board, and the lack of differentiation by race we would expect to see, could be due to the fact that we included a large variety of pulmonary diseases, including seasonal allergies and acute bacterial sinusitis. The latter of these, a bacterial sinus infection, is an incredibly common condition; one in 100 common colds lead to sinusitis [Sinusitis]. Especially as we ignore the fact that our data is simulated to have occurred over a long period of time (years), it is entirely reasonable to expect that, over a certain period of time, anyone may have a 50% risk of experiencing symptoms of sinusitis.\nIn terms of heart conditions, once again our model does not match trends in medical findings about cardiovascular outcomes by race, which also finds Black patients at higher risk for cardiovascular conditions [HeartOutcomes]. Unlike our lung conditions subset, the cardiovascular conditions identified are indicators of larger health issues, and not common complications of everyday illnesses. The fact that our model assigns equal scores regardless of race has large implications for the potential applications of a model like ours, built on synthetic data as ours was. If a model systematically underestimates the risk of groups compared to each other, for example our model would indicate treating all patients similarly regardless of their race, doctors and systems implementing our system may then systemically under-diagnose and treat Black patients, as they are unaware of their true increased risk compared to other racial groups.\nIn terms of diabetes, our model predicted that Asian patients are more likely to experience diabetes, prediabetes or a diabetes-related comorbidity. This agrees with medical findings, that even at lower BMIs, Asian patients are at higher risk for type-2 diabetes [CDCDiabetes]. Our model also predicted that Black patients are at the highest risk for pregnancy complications, which again agrees with prevailing medical literature [BlackMaternalHealth]. This is a promising result as it shows that our model may have a positive usage if implemented in healthcare settings, as it can draw attention to differentiated risks based on risk for diabetes and pregnancy complications.\n\n\nResults by Ethnicity:\nOur results for ethnicity were harder to interpret than our results by race. This following figure shows to top two most at risk ethnic groups and bottom two least at risk ethnic groups for each condition subset. We found that those in the middle often hovered around the same risk scores, and so did not provide as much valuable information for the diagnostic process.\n\ncancer = pd.DataFrame()\npreg = pd.DataFrame()\nheart = pd.DataFrame()\ndia = pd.DataFrame()\nlungs = pd.DataFrame()\n\n\ncancer['Ethnicity'] = ['German', 'Puerto Rican', 'Russian', 'Scottish']\ncancer['Risk'] = [0.105675, 0.067085, 0.005342, 0.005342]\n\npreg['Ethnicity'] = ['Dominican', 'Scottish', 'German', \"Russian\"]\npreg['Risk'] = [0.074523, 0.074523, 0.023289, 0.014905]\n\n\nheart['Ethnicity'] = ['Asian Indian', 'Scottish', 'Dominican', 'Chinese']\nheart['Risk'] = [0.556735, 0.556144, 0.480579, 0.464757]\n\ndia['Ethnicity'] = ['Asian Indian', 'Polish', 'Russian', 'Irish']\ndia['Risk'] = [0.714286, 0.558405, 0.200000, 0.182902]\n\nlungs['Ethnicity'] = ['Polish', 'Mexican', 'West Indian', 'Chinese']\nlungs['Risk'] = [0.583031, 0.580761, 0.457941, 0.451298]\n\nfig, axs = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n\nfor i, (df, title) in enumerate(zip([cancer, preg, heart, dia, lungs], ['Cancer', 'Pregnancy', 'Heart', 'Diabetes', 'Lungs'])):\n    axs[i].bar(df['Ethnicity'], df['Risk'], color='skyblue')\n    axs[i].set_title(title)\n    axs[i].tick_params(axis='x', rotation=45)\n\n# scale y axis by 100 to show percentage\nplt.yscale('linear')  \nplt.ylim(0, 1) \n\nplt.tight_layout()\nplt.show()\n\nprint('Fig. 3. The risk score, shown in percentage on the y-axis, by the top and bottom two most at-risk ethnic groups based on condition subset.')\n\n\n\n\n\n\n\n\nFig. 3. The risk score, shown in percentage on the y-axis, by the top and bottom two most at-risk ethnic groups based on condition subset.\n\n\nThe first thing we checked with these results was whether the risks were skewed based on representation, i.e. whether some ethnicities appear much more frequently or much less frequently in our overall dataset. If for example Russian appeared infrequently in our dataset, and had a lower score overall for many of the conditions, we could not be sure whether this ethnic Russians truly have a lower risk, or are just underrepresented in our dataset. After comparing the distributions of ethnic groups in our dataset, we found that Russian and Scottish ethnic groups seemed to be slightly underrepresented, but that overall no ethnic group appeared drastically less or more frequently than any other, making us confident that most of the driving forced behind our results are true patterns in diagnosis in our dataset, and not issues of skew.\nIn figure three, we see that our most at-risk ethnic groups by condition align with our most at-risk racial groups by condition, which is promising. The diabetes, lung, and heart conditions also all show increased risk overall, which matches our models predictions that those groups of conditions appear to be more common than cancer and pregnancy-related conditions. We hope that the figure above could serve a starting point for further investigation into the implications of increased/decreased risk for conditions based on having these ethnic identities. Without being intimately familiar with the demographic history and trends of Massachussetts, it’s hard to say whether environmental or genetic factors are the main factors influencing these results. Further research into community trends might find it valuable to draw on these scores as quantitative background for the effects of observed environmental, genetic, and societal factors\n\n\nResults by Town of Birth/Current Residence:\nThe last category we examined our data through was the town of birth and the the town current residence. We chose to compare both, as we weren’t certain whether the place of someone’s birth, or the place they currently reside (and therefore may have been residing for much of their adult lives) effects their health outcome more.\n\nbptown = pd.DataFrame()\n\nbptown['TownCat'] = ['Wealthy', 'Non-Wealthy']\nbptown['Lung'] = [0.4872429058942045, 0.5189382015534418]\nbptown['Heart'] = [0.10892307692307693, 0.1103076923076923]\nbptown['Cancer'] = [0.06399830132085002, 0.04238804096017698]\nbptown['Pregnancy'] = [0.038981469137448335, 0.03668844154112784]\nbptown['Diabetes'] =  [0.33305156382079454, 0.31947027331642713]\n\ncrtown = pd.DataFrame()\ncrtown['TownCat'] = ['Wealthy', 'Non-Wealthy']\ncrtown['Lung'] = [0.5039833131472166, 0.5124758651408807]\ncrtown['Heart'] = [0.08661538461538462, 0.09256410256410255]\ncrtown['Cancer'] = [0.03621368085712754, 0.05164958111475114]\ncrtown['Pregnancy'] = [0.04586055192640981, 0.03630627027507444]\ncrtown['Diabetes'] = [0.25274725274725274, 0.2827087442472057]\n\nprint(bptown)\nprint('Fig. 4. Average Risk Score by Condition Group According to Wealth Designation of Birthplace.')\n\n       TownCat      Lung     Heart    Cancer  Pregnancy  Diabetes\n0      Wealthy  0.487243  0.108923  0.063998   0.038981  0.333052\n1  Non-Wealthy  0.518938  0.110308  0.042388   0.036688  0.319470\nFig. 4. Average Risk Score by Condition Group According to Wealth Designation of Birthplace.\n\n\n\nprint(crtown)\nprint('Fig. 5. Average Risk Score by Condition Group According to Wealth Designation of Current Town of Residence.')\n\n       TownCat      Lung     Heart    Cancer  Pregnancy  Diabetes\n0      Wealthy  0.503983  0.086615  0.036214   0.045861  0.252747\n1  Non-Wealthy  0.512476  0.092564  0.051650   0.036306  0.282709\nFig. 5. Average Risk Score by Condition Group According to Wealth Designation of Current Town of Residence.\n\n\nFrom the tables above, we see that for most conditions, wealthy versus non-wealthy towns have relatively similar risk scores. This could be due to the lack of great variety in our dataset in terms of environment. Of course some towns are wealthier than others, but they still all exist within Massachusetts, a smaller state (area-wise) compared to most others, and therefore not many of the towns we include truly exist in their own bubble where direct correlations between town wealth and health of its inhabitants can easily be measured.\nThat being said, there are some differences worth noting. We found the disparity in pulmonary condition risk score between wealthy and non-wealthy towns being wider by birthplace interesting, as we knew from the literature that childhood asthma is often linked to environmental factors such as air quality and poverty [ChildhoodAsthmaPovertyLink]. We also see that patients with a non-wealthy town of current residence are slightly more at risk for developing diabetes. This is a rational result for two reasons. Firstly, the food someone eats as a baby has less of an effect on their present risk for developing diabetes than the foods they have currently have access to. Secondly, non-wealthy towns often lack access to fresh fruits and vegetables, and other more expensive, but less-processed, food items. Interestingly, both patients with a wealthy birthplace and those with a wealthy current town are at a greater risk of pregnancy complications according to our model. Speculatively, this could be due to differences in mothers age, as younger mothers are more likely to be lower income than older mothers, but older mother are at a much higher risk for complications [AgeatBirthvsIncome]. Our cancer risk is higher for those born in wealthy towns, but lower for those currently residing in a wealthy town. This second result could be explained by increased access to early-intervention/preventative healthcare and diagnostics. However, we are unsure about the significance of birthplace disparity. Furthermore, our risk for heath diseases are almost equal across birthplaces and current towns of residence."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#concluding-discussion",
    "href": "posts/finalProj/PostHomePage.html#concluding-discussion",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project was able to accomplish our goal of analyzing risk rates for various illnesses and conditions for different identities. Due to the large quantity of data we possessed, we were unable to analyze all of the data we had access to to make predictions. Ideally, we would have been able to predict medication use or various observations in addition to specific conditions. Also, if we had access to more data, we could have made more specific predictions- like for asthma instead of general lung ailments. If we had more time, computational resources, and data we would like to extend our study to include healthcare information for different conditions as well as different geographical regions outside of Massachusetts. By amplifying the range of data we include, we would be able to come to more concrete conclusions on different risk rates. However, we were able to complete our aspirations for this project by generating risk rates for race, gender, ethnicity, birthplace, and current address for five different ailments.\nOur results compare to the results of those who have studied similar problems. For example, there is a large quantity of scientific data that shows that people at lower socio-economic status are more likely to get diabetes [DiabetesSocioeconomic]. Furthermore, race has been strongly connected to material mortality and health. Specifically, black and hispanic women are at much higher risk of issues with pregnancy than their white counterparts [PregnancyRisk]. We saw both these trends and more replicated in our model’s predictions. Therefore we can conclude that our model is creating predictions that are correlated with real life trends.\n\nCritical Discussion\nThe goal of our presentation is to analyze the bias present in our healthcare system and the risk of certain groups of different illnesses and health conditions. There are many organizations that might find this type of model useful or interesting. One interested party could be a hospital that wants to allocate resources based on the communities they serve. This could be helpful as they could adapt to real community needs. A similar use case could be if a town is building or allocating healthcare resources and wants to understand the risks of their township or locality. Hopefully, this model could help allow resources to go to the places in which there is great need. However, an important note is that this dataset measures the recorded rates of a hospital setting. This could widely vary from real illness rates, as certain communities are under-treated or under-diagnosed in the US healthcare system.\nA more harmful use case would include an insurance company incorporating this model into their decisions about providing healthcare coverage. Therefore, our model has the risk, if put into the wrong hands, to have negative impacts on already marginalized communities. As insurance companies have significant resources, it is probable they would finance a project like this. Thus the question arises of whether this model should be allowed to be employed in decision-making scenarios.\nWe completed this work out of curiosity as part of an educational pursuit. If used for knowledge or understanding of the impact of different illnesses and conditions on identity groups, it can be helpful and informative. However, there is also the risk of further harming groups that have already been historically marginalized within the US healthcare system."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#the-3-definitions-of-fairness",
    "href": "posts/finalProj/PostHomePage.html#the-3-definitions-of-fairness",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "The 3 Definitions of Fairness",
    "text": "The 3 Definitions of Fairness\nIn their paper [Barocas2023], Barocas, Hardt, and Narayanan outline three relative notions of fairness: the narrow, middle, and broad views. The narrow view of fairness suggests that we should treat similar individuals in the same manner, given how currently similar they are. The broad perspective advocates for structuring society to facilitate similar outcomes for people with comparable abilities and ambitions. The middle-ground stance proposes that we treat different people equally, under the assumption that their apparent dissimilarities stem from factors just as past injustices or misfortunes that should be disregarded.\nSince our project involves comparing people of different demographic and characteristics, evaluating risk scores and examining fairness, we must look into our project and identify what is fair, as well as what we are choosing to define as fair.\nUnder the narrow view of fairness, since the comparison is between individuals and not directly concerned with the way members of specific groups might be treated, The narrow view only commands that similar people be treated similarly. In our models, similar people (eg of the same demographic factor being studied such as race, gender, ethnicity…) are being treated similarly in our models.\nUnder the middle view of fairness, since the decision makers have an obligation to avoid perpetuating injustice, our evolution of our model and data’s biases in attempts to expose the perpetuation of injustice keeps us in alignment with the middle view of fairness. However, if this model were to be used in an alternative way, say by insurance companies in deciding coverage, it could potentially violate this definition.\nThe broad view of fairness focuses on the degree to which society overall is structured to allow people of similar ability and ambition to achieve similar success. Under this definition, outcomes are solely attributed to difference in ability and ambition. However, the clear differences in risk scores for varying groups of demographics suggests this third definition is violated. An example of this is how as Black people are more likely to have pregnancy complications and men are twice as likely to get cancer. The disproportionate effects of diseases predicted by our models suggest that environmental or systemic factors may be at play. Intervention and change are needed at the basic level of societal structure to accomplish this third definition of fairness."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#group-contributions",
    "href": "posts/finalProj/PostHomePage.html#group-contributions",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Group Contributions",
    "text": "Group Contributions\nImportant Note: The additions/deletions on Github look skewed because of the creations/deletions of the csv files, not the actual code.\nLindsey: At the beginning of the project, I worked alongside Sophie to clean the data and figure out how to merge our information without the kernel dying. This included pivoted columns, filtering the data into various conditions, and merging the datasets. The next large role I took on was creating the code to train models on the dataframes we had created in our DataCleaning file. Together, the three of us worked to create code to evaluate the models based on race, gender, ethnicity, birthplace, and current address. I took a leading role in figuring out how to evaluate birthplace risk based on wealth of cities in Massachusetts. I also worked to re-organize this evaluation code so that we could reuse it for all of the conditions we studied. In terms of the blog post writing, I completed the two discussion sections, found sources to reference in our introduction that helped us develop our analysis throughout the Blog Post, and edited the writing throughout. Although our attention was divided among various aspects of the project, we collaborated effectively as a team, supporting one another whenever any member encountered a challenge.\nJulia: In the beginning of the project, I created data visualizations allowing us to better understand our data and project question goals. The large role I took on was to create code to evaluate the models based on race, gender, ethnicity, birthplace, and current address. I implemented the evaluation of cancer, heart diseases, and lung diseases. I worked alongside with Lindsey to find and import our Massachusetts wealth information, in order to evaluate birthplace and current town residence risk based on wealth.\nI additionally went through our model code and repaired seeding and randomness issues, to ensure our models were performing to the same caliber and ‘accuracy’ across our code. Throughout the project, I took on the role of keeping our data thoroughly cleaned and organized, as we frequently found ourselves with extraneous and additional code we did not need, as well as a need to organize our code for comprehensibility and readability as we worked both separably and together. In terms of the blog post writing, I completed our Values Statement, Material & Methods section, as well as the discussion on the three views of fairness. Our efforts were comprehensive and collaborative throughout this project. Pair-programming was utilized alongside our individual divide-and-conquer. We functioned cohesively as a group, completing our project with an happy divide of labor and effort.\nSophie: Lindsey and I started by working on cleaning and merging our data into usable (i.e. not large enough to crash our kernel every time) datasets for model training. Afterwards, I contributed to the exploratory data analysis document by creating graphs showing differences in condition prevalence by race, ethnicity, and birthplace (for which I had to find the populations of each town in our dataset to calculate prevalence). We worked together on writing the code to generate models and risk scores for our conditions. I started to address the problem of random seeding in our data, which was causing variable results each time we ran, which Julia took on later. Afterwards I focused on the organization aspect of our blog post, writing explanatory comments for all our documents/code lines, and creating the format for our post in terms of linking all of our various working documents together into this one, more streamlined document. I also wrote our introduction, abstract, and results section, and Julia and I worked together on our values statement, and I wrote parts of our approach. Overall, I think we worked very well as a group in terms of division of labor and coding together. We were all proactive in taking the lead on certain aspects of the project, and worked very collaboratively together when we were stuck on certain parts."
  },
  {
    "objectID": "posts/finalProj/PostHomePage.html#personal-reflection",
    "href": "posts/finalProj/PostHomePage.html#personal-reflection",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Personal Reflection",
    "text": "Personal Reflection"
  },
  {
    "objectID": "posts/post5/homework5.html",
    "href": "posts/post5/homework5.html",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nLink to source code (logistic.py)\nFor this blog post I implemented logistic regression and performed several experiments on my model. The first experiment I conducted was to see vanilla gradient descent. Vanilla gradient descent is when beta=0 and we were able to know that the model works because we saw loss decrease monotonically. The second experiment was to understand the benefits of momentum. By increasing the beta value we were able to see loss decrease faster than the vanilla gradient descent case. Then, finally, we conducted an experiment to see the potential harms of overfitting our data. By reaching 100% accuracy on training data, we could see the drawback directly by a lower accuracy rate on training data. Overall, I was able to learn more about implementing machine learning models and how to test their functionality. I was able to concretely understand the benefits of momentum and the drawbacks of overfitting.\n\n\n\nBefore doing any experiments, I had to generate data for a classification problem.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nPlot the data\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# p_dims is 2\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nCode to graph a straight line\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nThe first experiment that I performed was vanilla gradient descent: When p_dim = 2, when alpha is sufficiently small and beta=0.\nWant to see:\n\nGradient descent for logistic regression converges to a weight vector w that looks visually correct\n\nshow this by plot the decision boundary with the data\n\nLoss decreases monotonically: A monotonic function is a function which is either entirely nonincreasing or nondecreasing.\n\nshow this by plotting the loss over iterations\n\n\nFirst implement a training loop with graphs with a dividing line to visualize our progress.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec_van = []\n\nfor index in range(5000):\n    \n    # for vanilla gradient descent, alpha must be sufficiently small and beta must be 0\n    opt.step(X, y, alpha = 0.01, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec_van.append(loss)\n\n\ndef find_accuracy(X, y):\n\n    predictions = LR.predict(X)\n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 0.996666669845581\n\n\nPlot the loss over time over the 5000 iterations.\n\nimport numpy as np\n\ndef plot_loss(loss, label= \"\"):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(loss, color = \"blue\", label=label)\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\nplot_loss(loss_vec_van)\n\n\n\n\n\n\n\n\nWe can see that the loss is decreasing monotonically over time through this graph of the loss. The negative slope shows us that the loss is in fact decreasing over time. In other words, our machine learning model is learning!\nPlot the final line separating the data\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nWe can see that the dividing line almost perfectly divides the classes. In time, we could see the logistic regression training could become perfectly accurate.\n\n\n\n\nOur next experiment was to see the benefits of momentum. On the same data, gradient descent with momentum (e.g. beta=0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta=0).\nWe want to see:\n\nA model that learns at a faster rate\n\nshow loss decreasing at a faster rate than when beta was 0\n\n\nFirst implement a training loop with graphs with a dividing line to visualize our progress.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nX, y = classification_data(noise = 0.2)\n\n# initialize for main loop\nloss_vec_mom = []\n\nfor index in range(5000):\n    \n    # to see the benefits of momentum, alpha must be sufficiently small and beta must be 0.9\n    opt.step(X, y, alpha = 0.01, beta = 0.9)\n    loss = LR.loss(X, y).item()            \n    loss_vec_mom.append(loss)\n\nPlot the loss over time over the 5000 iterations.\n\nplt.plot(loss_vec_van, color = \"green\", label='Vanilla')\nplt.plot(loss_vec_mom, color = \"blue\", label='Momentum')\n\nplt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe faster declining slope of the loss shows that the larger beta value does in fact increase the learning speed of the machine learning model.\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nWe can see the benefits of increase of momentum by viewing the improved dividing line. The increase in the beta value allows our logistic regression to improve at a much faster rate then when beta=0. We know that because with the same number of iterations, the loss decreased more, or in other words, the model learned to classify at a faster rate.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nA perfect accuracy rate!\n\n\n\nOur final experiment was to show the danger of overfitting. To show this I need to generate some data where p_dim &gt; n_points and create an instance where the same logistic regression model has a 100% accuracy rate on training data.\nWant to see:\n\nPerfect accuracy for training data\nLess accurate classification for testing data with the exact same parameters\n\nFor overfitting, we need to generate data where p_dim &gt; n_points.\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nGoal to achieve 100% accuracy with the training data.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(2000):\n   \n    opt.step(X_train, y_train, alpha = 0.01, beta = 0.9)\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec.append(loss)\n\n\nprint(loss_vec)\n\n[0.633738100528717, 0.5438883304595947, 0.4618161916732788, 0.40402740240097046, 0.36823952198028564, 0.3463859558105469, 0.33207085728645325, 0.32158127427101135, 0.3129945993423462, 0.30535393953323364, 0.2981888949871063, 0.29127079248428345, 0.2844916582107544, 0.27780479192733765, 0.2711944878101349, 0.2646607458591461, 0.25821107625961304, 0.25185608863830566, 0.24560725688934326, 0.23947563767433167, 0.23347118496894836, 0.22760243713855743, 0.22187648713588715, 0.21629880368709564, 0.21087360382080078, 0.20560361444950104, 0.20049048960208893, 0.19553472101688385, 0.19073593616485596, 0.18609294295310974, 0.1816038191318512, 0.17726610600948334, 0.17307688295841217, 0.16903269290924072, 0.16512995958328247, 0.16136476397514343, 0.1577329784631729, 0.15423043072223663, 0.15085284411907196, 0.1475958526134491, 0.14445513486862183, 0.14142641425132751, 0.13850541412830353, 0.1356879472732544, 0.1329699158668518, 0.13034740090370178, 0.12781642377376556, 0.1253732591867447, 0.12301427125930786, 0.12073586136102676, 0.11853475868701935, 0.11640762537717819, 0.1143513098359108, 0.11236286163330078, 0.11043937504291534, 0.10857807099819183, 0.10677634924650192, 0.10503166168928146, 0.10334160178899765, 0.10170385986566544, 0.10011627525091171, 0.09857672452926636, 0.09708324074745178, 0.09563390910625458, 0.0942269116640091, 0.09286051988601685, 0.0915331095457077, 0.09024310111999512, 0.08898898959159851, 0.08776937425136566, 0.08658287674188614, 0.08542823046445847, 0.08430419862270355, 0.08320960402488708, 0.08214332908391953, 0.08110430091619492, 0.08009150624275208, 0.0791039913892746, 0.07814081013202667, 0.07720109820365906, 0.0762840062379837, 0.07538872957229614, 0.07451450824737549, 0.07366058975458145, 0.07282629609107971, 0.07201094925403595, 0.07121390104293823, 0.0704345628619194, 0.06967232376337051, 0.06892664730548859, 0.06819696724414825, 0.06748278439044952, 0.0667836144566536, 0.06609895825386047, 0.06542839109897614, 0.06477145105600357, 0.06412775814533234, 0.06349688023328781, 0.06287844479084015, 0.0622720830142498, 0.06167743727564812, 0.06109416484832764, 0.06052194535732269, 0.05996045097708702, 0.059409402310848236, 0.05886848270893097, 0.05833742767572403, 0.05781596153974533, 0.05730381980538368, 0.056800756603479385, 0.05630652979016304, 0.05582091212272644, 0.05534365773200989, 0.05487455800175667, 0.05441341921687126, 0.05396000295877457, 0.05351414158940315, 0.053075648844242096, 0.052644334733486176, 0.05222000926733017, 0.05180253088474274, 0.05139169096946716, 0.050987374037504196, 0.05058939382433891, 0.05019763112068176, 0.04981191083788872, 0.04943211376667023, 0.04905809089541435, 0.04868970066308975, 0.04832683503627777, 0.04796937480568886, 0.04761717468500137, 0.04727013036608696, 0.046928148716688156, 0.046591076999902725, 0.046258844435214996, 0.04593133553862572, 0.045608438551425934, 0.045290082693099976, 0.044976137578487396, 0.04466654732823372, 0.04436120018362999, 0.04406002163887024, 0.04376290738582611, 0.043469786643981934, 0.043180570006370544, 0.042895182967185974, 0.042613573372364044, 0.0423356257379055, 0.04206128418445587, 0.04179048910737038, 0.04152318462729454, 0.041259266436100006, 0.040998686105012894, 0.04074139520525932, 0.04048732668161392, 0.040236398577690125, 0.039988573640584946, 0.03974378481507301, 0.039501968771219254, 0.03926309943199158, 0.03902710601687431, 0.03879392519593239, 0.03856353089213371, 0.038335852324962616, 0.038110848516225815, 0.03788847476243973, 0.03766869008541107, 0.03745144233107567, 0.03723669424653053, 0.03702438995242119, 0.03681449592113495, 0.03660697489976883, 0.03640177845954895, 0.036198876798152924, 0.035998232662677765, 0.03579980134963989, 0.03560354933142662, 0.035409435629844666, 0.035217419266700745, 0.03502750024199486, 0.03483960032463074, 0.034653712064027786, 0.03446980565786362, 0.034287840127944946, 0.03410777449607849, 0.033929597586393356, 0.033753279596567154, 0.0335787832736969, 0.03340607509016991, 0.03323514759540558, 0.03306595981121063, 0.03289847820997238, 0.03273269534111023, 0.03256857767701149, 0.032406099140644073, 0.0322452187538147, 0.03208594769239426, 0.03192823752760887, 0.031772054731845856, 0.031617421656847, 0.031464263796806335, 0.03131258487701416, 0.031162375584244728, 0.03101358562707901, 0.030866215005517006, 0.03072025254368782, 0.03057565540075302, 0.030432408675551414, 0.030290512368083, 0.030149927362799644, 0.030010638758540154, 0.029872631654143333, 0.029735885560512543, 0.029600396752357483, 0.029466135427355766, 0.029333079233765602, 0.02920122817158699, 0.02907055802643299, 0.02894105762243271, 0.028812691569328308, 0.028685469180345535, 0.02855936996638775, 0.028434377163648605, 0.028310470283031464, 0.028187651187181473, 0.0280658770352602, 0.027945155277848244, 0.027825474739074707, 0.0277068130671978, 0.027589164674282074, 0.02747250720858574, 0.0273568257689476, 0.027242114767432213, 0.027128372341394424, 0.02701556868851185, 0.026903703808784485, 0.026792753487825394, 0.026682719588279724, 0.02657359652221203, 0.026465341448783875, 0.02635798044502735, 0.02625148370862007, 0.02614584192633629, 0.02604104019701481, 0.02593708597123623, 0.02583394944667816, 0.0257316492497921, 0.025630144402384758, 0.025529442355036736, 0.02542952261865139, 0.02533038891851902, 0.025232018902897835, 0.025134418159723282, 0.02503756247460842, 0.024941453710198402, 0.02484607696533203, 0.024751434102654457, 0.024657506495714188, 0.024564284831285477, 0.02447177842259407, 0.02437996119260788, 0.024288825690746307, 0.024198371917009354, 0.02410859800875187, 0.024019470438361168, 0.02393101155757904, 0.023843200877308846, 0.023756034672260284, 0.02366950735449791, 0.02358359843492508, 0.02349831536412239, 0.02341364696621895, 0.023329583927989006, 0.023246141150593758, 0.023163264617323875, 0.023080989718437195, 0.022999301552772522, 0.022918183356523514, 0.022837644442915916, 0.022757668048143387, 0.022678246721625328, 0.02259938232600689, 0.022521063685417175, 0.022443287074565887, 0.022366054356098175, 0.0222893375903368, 0.022213155403733253, 0.02213749848306179, 0.02206234261393547, 0.02198770083487034, 0.021913563832640648, 0.02183992601931095, 0.0217667818069458, 0.021694118157029152, 0.02162194810807705, 0.0215502567589283, 0.021479027345776558, 0.021408285945653915, 0.021338002756237984, 0.021268177777528763, 0.02119881473481655, 0.021129891276359558, 0.02106141857802868, 0.020993396639823914, 0.02092580311000347, 0.020858652889728546, 0.020791923627257347, 0.02072562277317047, 0.020659752190113068, 0.020594289526343346, 0.02052924782037735, 0.020464608445763588, 0.020400382578372955, 0.02033654972910881, 0.020273126661777496, 0.020210087299346924, 0.020147452130913734, 0.02008519321680069, 0.020023318007588387, 0.019961828365921974, 0.019900711253285408, 0.019839977845549583, 0.019779611378908157, 0.019719600677490234, 0.01965995691716671, 0.019600683823227882, 0.01954176276922226, 0.01948319748044014, 0.01942497305572033, 0.019367104396224022, 0.019309576600790024, 0.019252397119998932, 0.019195545464754105, 0.019139036536216736, 0.01908285543322563, 0.01902700588107109, 0.018971487879753113, 0.018916286528110504, 0.018861401826143265, 0.01880684122443199, 0.018752597272396088, 0.01869867369532585, 0.018645040690898895, 0.01859172433614731, 0.01853870414197445, 0.01848599687218666, 0.0184335820376873, 0.018381472676992416, 0.018329650163650513, 0.018278125673532486, 0.01822689361870289, 0.018175937235355377, 0.0181252621114254, 0.01807487942278385, 0.018024779856204987, 0.01797494851052761, 0.01792539469897747, 0.017876114696264267, 0.0178271122276783, 0.017778366804122925, 0.017729882150888443, 0.017681676894426346, 0.017633728682994843, 0.01758604124188423, 0.017538612708449364, 0.017491435632109642, 0.017444506287574768, 0.017397843301296234, 0.017351416870951653, 0.01730523258447647, 0.017259307205677032, 0.017213625833392143, 0.017168192192912102, 0.017122982069849968, 0.017078010365366936, 0.017033280804753304, 0.016988782212138176, 0.016944529488682747, 0.01690048538148403, 0.016856681555509567, 0.016813097521662712, 0.016769759356975555, 0.016726627945899963, 0.01668372191488743, 0.01664102077484131, 0.016598554328083992, 0.016556303948163986, 0.016514264047145844, 0.016472432762384415, 0.016430824995040894, 0.016389409080147743, 0.0163482204079628, 0.016307227313518524, 0.01626645401120186, 0.016225868836045265, 0.016185499727725983, 0.016145318746566772, 0.016105342656373978, 0.01606556586921215, 0.016025977209210396, 0.015986597165465355, 0.015947403386235237, 0.015908408910036087, 0.015869593247771263, 0.015830975025892258, 0.015792537480592728, 0.01575428992509842, 0.015716219320893288, 0.01567832939326763, 0.01564064435660839, 0.015603124164044857, 0.015565778128802776, 0.015528624877333641, 0.01549164392054081, 0.01545484084635973, 0.01541820727288723, 0.015381750650703907, 0.015345468185842037, 0.015309352427721024, 0.01527340803295374, 0.015237616375088692, 0.015202010050415993, 0.015166568569839, 0.015131289139389992, 0.015096166171133518, 0.0150612136349082, 0.015026427805423737, 0.014991790056228638, 0.014957324601709843, 0.014923018403351307, 0.014888865873217583, 0.01485486887395382, 0.014821025542914867, 0.014787348918616772, 0.014753805473446846, 0.014720415696501732, 0.014687180519104004, 0.014654100872576237, 0.01462116464972496, 0.014588368125259876, 0.014555740170180798, 0.014523230493068695, 0.014490890316665173, 0.014458680525422096, 0.014426631852984428, 0.014394722878932953, 0.014362944290041924, 0.01433132030069828, 0.01429982203990221, 0.014268457889556885, 0.014237234368920326, 0.014206149615347385, 0.014175190590322018, 0.014144386164844036, 0.014113698154687881, 0.014083152636885643, 0.014052736572921276, 0.014022455550730228, 0.013992296531796455, 0.013962272554636002, 0.013932373374700546, 0.013902622275054455, 0.013872980140149593, 0.013843467459082603, 0.013814077712595463, 0.01378481462597847, 0.013755671679973602, 0.013726655393838882, 0.013697751797735691, 0.01366897951811552, 0.013640315271914005, 0.01361178606748581, 0.013583367690443993, 0.013555066660046577, 0.013526885770261288, 0.01349882036447525, 0.013470884412527084, 0.013443049974739552, 0.013415336608886719, 0.013387740589678288, 0.013360254466533661, 0.013332877308130264, 0.013305635191500187, 0.013278481550514698, 0.013251448050141335, 0.013224517926573753, 0.013197695836424828, 0.013170987367630005, 0.013144386000931263, 0.013117888942360878, 0.013091490603983402, 0.013065210543572903, 0.01303903479129076, 0.013012951239943504, 0.012986983172595501, 0.012961126863956451, 0.012935347855091095, 0.012909691780805588, 0.012884140014648438, 0.012858685106039047, 0.012833324261009693, 0.012808071449398994, 0.012782922945916653, 0.012757867574691772, 0.012732910923659801, 0.012708064168691635, 0.012683290988206863, 0.012658625841140747, 0.012634054757654667, 0.012609578669071198, 0.012585198506712914, 0.012560906819999218, 0.012536711990833282, 0.012512611225247383, 0.012488599866628647, 0.012464679777622223, 0.012440850958228111, 0.012417115271091461, 0.012393467128276825, 0.01236991211771965, 0.012346451170742512, 0.012323069386184216, 0.012299779802560806, 0.012276582419872284, 0.012253474444150925, 0.012230447493493557, 0.01220751740038395, 0.012184670194983482, 0.012161901220679283, 0.01213922630995512, 0.012116632424294949, 0.012094126082956791, 0.012071695178747177, 0.012049359269440174, 0.012027101591229439, 0.012004919350147247, 0.011982819996774197, 0.011960810050368309, 0.011938877403736115, 0.011917012743651867, 0.01189524307847023, 0.011873540468513966, 0.011851922608911991, 0.011830383911728859, 0.01180893275886774, 0.011787539348006248, 0.011766238138079643, 0.011745008639991283, 0.011723853647708893, 0.01170278713107109, 0.011681786738336086, 0.011660866439342499, 0.011640023440122604, 0.011619243770837784, 0.011598551645874977, 0.01157793402671814, 0.011557374149560928, 0.011536904610693455, 0.011516503058373928, 0.011496172286570072, 0.011475921608507633, 0.011455731466412544, 0.011435611173510551, 0.011415582150220871, 0.011395602487027645, 0.011375694535672665, 0.01135585643351078, 0.011336083523929119, 0.011316383257508278, 0.011296744458377361, 0.011277178302407265, 0.011257678270339966, 0.011238238774240017, 0.011218883097171783, 0.011199580505490303, 0.01118034590035677, 0.011161179281771183, 0.011142078787088394, 0.011123034171760082, 0.011104064993560314, 0.01108515728265047, 0.011066317558288574, 0.01104754488915205, 0.011028826236724854, 0.0110101830214262, 0.01099159475415945, 0.010973077267408371, 0.010954611003398895, 0.01093621738255024, 0.010917886160314083, 0.010899608954787254, 0.010881399735808372, 0.010863247327506542, 0.010845160111784935, 0.010827148333191872, 0.010809175670146942, 0.010791275650262833, 0.010773427784442902, 0.010755636729300022, 0.010737907141447067, 0.010720236226916313, 0.010702623054385185, 0.010685062035918236, 0.010667573660612106, 0.010650125332176685, 0.010632737539708614, 0.010615409351885319, 0.010598130524158478, 0.010580918751657009, 0.010563752613961697, 0.010546651668846607, 0.010529591701924801, 0.010512597858905792, 0.010495648719370365, 0.010478755459189415, 0.01046192366629839, 0.010445140302181244, 0.010428410954773426, 0.010411742143332958, 0.010395115241408348, 0.010378550738096237, 0.010362026281654835, 0.01034557819366455, 0.010329164564609528, 0.010312809608876705, 0.010296506807208061, 0.010280255228281021, 0.01026405580341816, 0.010247906669974327, 0.010231805965304375, 0.010215753689408302, 0.01019976008683443, 0.010183824226260185, 0.010167925618588924, 0.010152066126465797, 0.010136292316019535, 0.010120546445250511, 0.010104848071932793, 0.01008920930325985, 0.010073610581457615, 0.010058061219751835, 0.010042560286819935, 0.010027103126049042, 0.010011697188019753, 0.009996342472732067, 0.009981030598282814, 0.009965762495994568, 0.009950540028512478, 0.009935375303030014, 0.009920244105160236, 0.009905153885483742, 0.00989012885838747, 0.009875130839645863, 0.009860186837613583, 0.00984528660774231, 0.009830429218709469, 0.009815610945224762, 0.009800847619771957, 0.009786119684576988, 0.00977144856005907, 0.00975680910050869, 0.00974221620708704, 0.009727670811116695, 0.009713170118629932, 0.009698702953755856, 0.009684288874268532, 0.009669915772974491, 0.00965557899326086, 0.00964128877967596, 0.009627040475606918, 0.00961282942444086, 0.009598658420145512, 0.009584545157849789, 0.009570457972586155, 0.009556428529322147, 0.009542430751025677, 0.009528476744890213, 0.00951455906033516, 0.009500687010586262, 0.009486845694482327, 0.009473067708313465, 0.009459313005208969, 0.009445608593523502, 0.009431927464902401, 0.00941829476505518, 0.009404707700014114, 0.009391160681843758, 0.009377644397318363, 0.009364182129502296, 0.009350746870040894, 0.00933736003935337, 0.009324000217020512, 0.009310691617429256, 0.009297401644289494, 0.00928416382521391, 0.009270966984331608, 0.009257792495191097, 0.009244668297469616, 0.009231572970747948, 0.009218519553542137, 0.009205501526594162, 0.009192511439323425, 0.009179568849503994, 0.009166655130684376, 0.009153790771961212, 0.00914094503968954, 0.0091281458735466, 0.009115377441048622, 0.009102646261453629, 0.009089949540793896, 0.009077288210391998, 0.009064658544957638, 0.009052070789039135, 0.009039511904120445, 0.009026985615491867, 0.00901450589299202, 0.009002056904137135, 0.008989628404378891, 0.008977248333394527, 0.008964898064732552, 0.008952583186328411, 0.00894029438495636, 0.008928051218390465, 0.008915829472243786, 0.008903641253709793, 0.008891492150723934, 0.008879383094608784, 0.008867292664945126, 0.0088552450761199, 0.008843227289617062, 0.008831238374114037, 0.0088192792609334, 0.00880736205726862, 0.008795462548732758, 0.008783616125583649, 0.008771789260208607, 0.00875999964773655, 0.008748234249651432, 0.008736498653888702, 0.008724815212190151, 0.008713137358427048, 0.00870150700211525, 0.008689898066222668, 0.008678330108523369, 0.008666795678436756, 0.008655278943479061, 0.008643806912004948, 0.008632354438304901, 0.008620933629572392, 0.008609545417129993, 0.008598193526268005, 0.008586866781115532, 0.00857557449489832, 0.00856431107968092, 0.008553066290915012, 0.00854188296943903, 0.008530696853995323, 0.0085195517167449, 0.008508438244462013, 0.00849735178053379, 0.008486300706863403, 0.008475268259644508, 0.008464280515909195, 0.008453297428786755, 0.008442363701760769, 0.008431445807218552, 0.008420557714998722, 0.008409702219069004, 0.008398869074881077, 0.008388069458305836, 0.008377295918762684, 0.008366545662283897, 0.008355831727385521, 0.008345132693648338, 0.008334473706781864, 0.008323834277689457, 0.008313219994306564, 0.008302636444568634, 0.008292073383927345, 0.008281546644866467, 0.008271039463579655, 0.00826056208461523, 0.008250114507973194, 0.00823968555778265, 0.008229285478591919, 0.00821891613304615, 0.008208568207919598, 0.008198249153792858, 0.00818795058876276, 0.008177685551345348, 0.008167429827153683, 0.008157218806445599, 0.008147021755576134, 0.008136851713061333, 0.00812671147286892, 0.008116597309708595, 0.008106500841677189, 0.00809643417596817, 0.00808639731258154, 0.008076366037130356, 0.008066377602517605, 0.008056405000388622, 0.008046472445130348, 0.008036535233259201, 0.008026642724871635, 0.00801677256822586, 0.008006924763321877, 0.007997103035449982, 0.00798730831593275, 0.007977524772286415, 0.007967781275510788, 0.007958047091960907, 0.007948347367346287, 0.007938667200505733, 0.00792901124805212, 0.007919371128082275, 0.007909778505563736, 0.007900181226432323, 0.007890617474913597, 0.007881076075136662, 0.007871560752391815, 0.007862064987421036, 0.007852593436837196, 0.007843147031962872, 0.007833724841475487, 0.007824317552149296, 0.007814938202500343, 0.007805581670254469, 0.0077962446957826614, 0.007786931935697794, 0.007777645718306303, 0.007768371608108282, 0.007759136147797108, 0.007749906741082668, 0.007740712724626064, 0.00773153081536293, 0.007722381502389908, 0.007713246159255505, 0.007704134564846754, 0.007695045322179794, 0.007685979828238487, 0.007676927372813225, 0.007667903788387775, 0.00765890022739768, 0.007649928797036409, 0.007640963885933161, 0.007632033433765173, 0.007623111829161644, 0.0076142167672514915, 0.007605339400470257, 0.007596494629979134, 0.007587661035358906, 0.007578848395496607, 0.007570060435682535, 0.007561282720416784, 0.007552536204457283, 0.007543810643255711, 0.007535101845860481, 0.007526407018303871, 0.007517742458730936, 0.007509091403335333, 0.00750045757740736, 0.00749184750020504, 0.007483258377760649, 0.0074746813625097275, 0.007466130889952183, 0.007457602769136429, 0.0074490890838205814, 0.007440594024956226, 0.007432121783494949, 0.007423659786581993, 0.007415228057652712, 0.007406814023852348, 0.007398413494229317, 0.007390039041638374, 0.0073816776275634766, 0.00737333670258522, 0.007365015335381031, 0.007356700953096151, 0.007348412182182074, 0.007340153679251671, 0.0073319124057888985, 0.007323676720261574, 0.007315460592508316, 0.007307269610464573, 0.007299093063920736, 0.007290935609489679, 0.007282801903784275, 0.007274670526385307, 0.007266576401889324, 0.0072584873996675014, 0.007250417955219746, 0.007242375984787941, 0.00723434379324317, 0.0072263265028595924, 0.007218333892524242, 0.007210352923721075, 0.0072023929096758366, 0.007194451056420803, 0.007186520844697952, 0.007178615313023329, 0.007170721888542175, 0.007162848487496376, 0.007154995109885931, 0.007147153839468956, 0.007139327470213175, 0.007131523452699184, 0.007123734336346388, 0.007115961983799934, 0.007108209189027548, 0.00710047222673893, 0.00709275109693408, 0.007085044402629137, 0.007077352609485388, 0.007069683168083429, 0.007062026299536228, 0.007054388057440519, 0.007046771701425314, 0.007039153948426247, 0.007031569257378578, 0.007024005055427551, 0.007016440853476524, 0.007008907850831747, 0.007001371588557959, 0.006993868853896856, 0.006986378226429224, 0.006978901568800211, 0.006971440277993679, 0.006963989697396755, 0.006956565193831921, 0.006949152797460556, 0.006941754836589098, 0.006934370379894972, 0.006927005480974913, 0.006919649429619312, 0.00691232131794095, 0.006904997862875462, 0.006897692568600178, 0.006890406832098961, 0.0068831308744847775, 0.006875867955386639, 0.006868631113320589, 0.006861405447125435, 0.006854191422462463, 0.006846996955573559, 0.006839805748313665, 0.006832651328295469, 0.006825484801083803, 0.00681835412979126, 0.006811228580772877, 0.006804130505770445, 0.006797031965106726, 0.006789958570152521, 0.00678289495408535, 0.006775849498808384, 0.0067688170820474625, 0.0067617930471897125, 0.006754789501428604, 0.006747799459844828, 0.006740829441696405, 0.0067338645458221436, 0.0067269206047058105, 0.00671999529004097, 0.006713078822940588, 0.006706177722662687, 0.006699288263916969, 0.006692416034638882, 0.006685548461973667, 0.00667871069163084, 0.006671878509223461, 0.006665067747235298, 0.006658259779214859, 0.006651477422565222, 0.0066447025164961815, 0.006637943908572197, 0.006631198339164257, 0.006624465808272362, 0.006617764011025429, 0.006611051503568888, 0.006604357156902552, 0.00659768283367157, 0.006591021548956633, 0.006584377959370613, 0.006577745079994202, 0.0065711187198758125, 0.006564512383192778, 0.006557926069945097, 0.006551347207278013, 0.0065447743982076645, 0.006538227666169405, 0.006531683728098869, 0.006525161676108837, 0.006518653593957424, 0.006512152496725321, 0.006505663972347975, 0.006499183364212513, 0.006492726970463991, 0.006486285477876663, 0.006479848176240921, 0.006473428104072809, 0.006467009894549847, 0.006460616365075111, 0.006454231683164835, 0.006447855848819017, 0.006441492587327957, 0.006435150280594826, 0.006428818218410015, 0.006422500591725111, 0.0064161852933466434, 0.006409894675016403, 0.00640359940007329, 0.006397326476871967, 0.006391067057847977, 0.006384817883372307, 0.006378581747412682, 0.006372362375259399, 0.006366144400089979, 0.006359945051372051, 0.006353757344186306, 0.00634758872911334, 0.006341420579701662, 0.006335269659757614, 0.006329124793410301, 0.006323000881820917, 0.006316883955150843, 0.006310780066996813, 0.006304674781858921, 0.006298597902059555, 0.006292534060776234, 0.006286469753831625, 0.006280425935983658, 0.006274391897022724, 0.0062683699652552605, 0.006262357346713543, 0.00625635264441371, 0.006250367034226656, 0.006244397722184658, 0.006238419562578201, 0.00623246468603611, 0.006226527038961649, 0.006220594514161348, 0.006214665714651346, 0.006208760663866997, 0.006202866323292255, 0.006196971982717514, 0.006191100459545851, 0.0061852335929870605, 0.006179377902299166, 0.006173540838062763, 0.0061677005141973495, 0.006161878816783428, 0.006156077608466148, 0.006150275003165007, 0.006144484970718622, 0.006138714030385017, 0.006132945418357849, 0.0061271898448467255, 0.006121443118900061, 0.006115708965808153, 0.006109983194619417, 0.006104275118559599, 0.006098574493080378, 0.006092878058552742, 0.006087201647460461, 0.006081533618271351, 0.006075863726437092, 0.006070215255022049, 0.006064583547413349, 0.006058948114514351, 0.006053332705050707, 0.006047731731086969, 0.006042126566171646, 0.006036539562046528, 0.006030960474163294, 0.006025394890457392, 0.006019837688654661, 0.006014289800077677, 0.0060087586753070354, 0.0060032266192138195, 0.005997713189572096, 0.0059922123327851295, 0.0059867138043046, 0.005981224589049816, 0.005975754931569099, 0.005970277823507786, 0.005964819807559252, 0.005959382280707359, 0.0059539503417909145, 0.00594851840287447, 0.005943107418715954, 0.005937708541750908, 0.005932298488914967, 0.00592691358178854, 0.005921542644500732, 0.005916171707212925, 0.005910817999392748, 0.005905468948185444, 0.005900128278881311, 0.00589480297639966, 0.005889481399208307, 0.005884159822016954, 0.0058788699097931385, 0.005873576272279024, 0.005868298467248678, 0.005863024387508631, 0.0058577642776072025, 0.005852504167705774, 0.005847269669175148, 0.005842023994773626, 0.005836799740791321, 0.005831591319292784, 0.005826386157423258, 0.005821180995553732, 0.005816005636006594, 0.005810820963233709, 0.005805652588605881, 0.005800487939268351, 0.005795335862785578, 0.005790191702544689, 0.005785058252513409, 0.005779941566288471, 0.0057748244144022465, 0.0057697175070643425, 0.005764626432210207, 0.005759526509791613, 0.005754453130066395, 0.005749385338276625, 0.0057443189434707165, 0.005739267915487289, 0.005734228994697332, 0.005729195661842823, 0.005724175367504358, 0.005719155538827181, 0.0057141417637467384, 0.005709141492843628, 0.005704149603843689, 0.005699170753359795, 0.005694202147424221, 0.005689234007149935, 0.005684277042746544, 0.005679322872310877, 0.00567438080906868, 0.005669455975294113, 0.005664539989084005, 0.005659622605890036, 0.005654714535921812, 0.0056498064659535885, 0.005644924007356167, 0.005640046671032906, 0.005635174456983805, 0.005630312487483025, 0.005625460296869278, 0.0056206099689006805, 0.005615769419819117, 0.005610944237560034, 0.005606110207736492, 0.0056013073772192, 0.005596510600298643, 0.005591703578829765, 0.005586917977780104, 0.005582140292972326, 0.005577368661761284, 0.005572600290179253, 0.005567850545048714, 0.0055630989372730255, 0.005558360368013382, 0.005553624592721462, 0.005548907443881035, 0.005544192157685757, 0.0055394768714904785, 0.005534785334020853, 0.005530092865228653, 0.005525416228920221, 0.005520735401660204, 0.0055160666815936565, 0.005511409603059292, 0.005506751127541065, 0.005502108950167894, 0.005497486796230078, 0.005492856726050377, 0.005488222930580378, 0.005483622662723064, 0.005479016341269016, 0.005474422127008438, 0.005469826515763998, 0.005465254653245211, 0.005460682325065136, 0.0054561118595302105, 0.005451558157801628, 0.005447012837976217, 0.0054424721747636795, 0.005437937565147877, 0.0054334234446287155, 0.005428884644061327, 0.005424382630735636, 0.00541987968608737, 0.005415388382971287, 0.005410896148532629, 0.005406424403190613, 0.005401948466897011, 0.00539748091250658, 0.005393031053245068, 0.005388566758483648, 0.005384128075093031, 0.005379693582653999, 0.00537527073174715, 0.005370853003114462, 0.005366436671465635, 0.005362031050026417, 0.005357644986361265, 0.005353246815502644, 0.005348869599401951, 0.005344492848962545, 0.00534012308344245, 0.005335765890777111, 0.005331411492079496, 0.00532706780359149, 0.005322725046426058, 0.005318397656083107, 0.005314073525369167, 0.005309766624122858, 0.0053054471500217915, 0.005301150493323803, 0.005296857561916113, 0.00529256509616971, 0.005288287997245789, 0.005284017883241177, 0.005279744043946266, 0.0052754865027964115, 0.005271235015243292, 0.005266985855996609, 0.0052627455443143845, 0.005258512683212757, 0.005254288204014301, 0.005250072572380304, 0.0052458541467785835, 0.005241646897047758, 0.005237448029220104, 0.005233259405940771, 0.0052290731109678745, 0.005224897991865873, 0.0052207279950380325, 0.005216555204242468, 0.005212402902543545, 0.005208251066505909, 0.0052040982991456985, 0.005199959967285395, 0.005195829086005688, 0.005191700533032417, 0.005187581293284893, 0.0051834662444889545, 0.005179364234209061, 0.005175260826945305, 0.005171166732907295, 0.005167079158127308, 0.00516300555318594, 0.005158929619938135, 0.005154858343303204, 0.005150794517248869, 0.0051467507146298885, 0.005142691545188427, 0.005138645879924297, 0.005134621635079384, 0.005130586680024862, 0.005126559641212225, 0.005122544709593058, 0.005118531174957752, 0.0051145292818546295, 0.005110536236315966, 0.005106537137180567, 0.005102549679577351, 0.005098577588796616, 0.005094596184790134, 0.005090631078928709, 0.005086671095341444, 0.005082720424979925, 0.00507876044139266, 0.005074827466160059, 0.005070884246379137, 0.005066949874162674, 0.005063030868768692, 0.0050591095350682735, 0.0050552012398839, 0.005051286891102791, 0.005047392100095749, 0.005043498706072569, 0.005039608106017113, 0.005035726353526115, 0.00503184599801898, 0.0050279865972697735, 0.005024113692343235, 0.005020256619900465, 0.005016403738409281, 0.0050125508569180965, 0.005008709616959095, 0.0050048744305968285, 0.005001053214073181, 0.004997221753001213, 0.0049933986738324165, 0.004989601206034422, 0.004985798615962267, 0.00498198764398694, 0.00497819110751152, 0.004974403418600559, 0.0049706194549798965, 0.00496684480458498, 0.004963070619851351, 0.0049593085423111916, 0.00495555205270648, 0.004951791372150183, 0.004948044195771217, 0.004944306798279285, 0.00494055962190032, 0.0049368287436664104, 0.004933102056384087, 0.004929375369101763, 0.004925672430545092, 0.0049219559878110886, 0.004918253049254417, 0.004914553835988045, 0.00491086533293128, 0.004907181952148676, 0.0049034953117370605, 0.004899811930954456, 0.004896145313978195, 0.004892486147582531, 0.004888809751719236, 0.004885162226855755, 0.004881523549556732, 0.004877867177128792, 0.004874227102845907, 0.004870608914643526, 0.004866969771683216, 0.004863359499722719, 0.0048597343266010284, 0.004856116138398647, 0.004852514714002609, 0.004848916549235582, 0.004845325835049152, 0.004841725341975689, 0.004838145803660154, 0.004834564868360758, 0.004830995108932257, 0.004827428609132767, 0.004823857918381691, 0.004820303991436958, 0.004816750064492226, 0.004813203122466803, 0.0048096622340381145, 0.004806118551641703, 0.004802593495696783, 0.004799061454832554, 0.00479554571211338, 0.0047920201905071735, 0.004788513295352459, 0.00478500546887517, 0.004781506489962339, 0.004778003320097923, 0.004774508997797966, 0.004771034233272076, 0.004767553880810738, 0.004764073062688112, 0.004760598763823509, 0.0047571416944265366, 0.004753674380481243, 0.004750218242406845, 0.004746775142848492, 0.004743324592709541, 0.004739878699183464, 0.004736447241157293, 0.0047330111265182495, 0.004729589447379112, 0.0047261668369174, 0.0047227549366652966, 0.004719341639429331, 0.004715931136161089, 0.00471253227442503, 0.004709131550043821, 0.004705744795501232, 0.004702357575297356, 0.004698978271335363, 0.004695598967373371, 0.004692218732088804, 0.004688865039497614, 0.004685496911406517, 0.004682138096541166, 0.004678794182837009, 0.0046754442155361176, 0.0046720951795578, 0.004668761044740677, 0.00466542411595583, 0.00466209277510643, 0.004658769816160202, 0.00465544406324625, 0.00465213181450963, 0.004648829344660044, 0.004645520355552435, 0.004642218817025423, 0.004638929385691881, 0.004635631572455168, 0.004632351454347372, 0.004629063419997692, 0.004625784698873758, 0.004622514359652996, 0.004619238432496786, 0.004615978337824345, 0.004612721502780914, 0.0046094683930277824, 0.004606213420629501, 0.004602967295795679, 0.004599733278155327, 0.004596492275595665, 0.0045932685025036335, 0.004590036813169718, 0.004586820490658283, 0.00458359532058239, 0.004580385517328978, 0.004577173851430416, 0.004573964513838291, 0.004570768214762211, 0.004567568190395832, 0.004564371891319752, 0.004561194684356451, 0.004558010026812553, 0.004554837476462126, 0.004551658406853676, 0.004548491910099983, 0.004545321688055992, 0.00454216543585062, 0.004539003595709801, 0.004535853397101164, 0.004532709252089262, 0.004529571160674095, 0.004526419099420309, 0.004523288458585739, 0.00452016107738018, 0.004517039749771357, 0.004513923544436693, 0.004510801285505295, 0.004507682751864195, 0.004504579119384289, 0.00450146896764636, 0.004498376976698637, 0.004495282657444477, 0.004492189269512892, 0.004489099606871605, 0.004486015532165766, 0.004482931923121214, 0.004479861818253994, 0.004476784262806177, 0.004473729524761438, 0.004470666870474815, 0.004467600490897894, 0.0044645508751273155, 0.004461502656340599, 0.004458462353795767, 0.004455415531992912, 0.004452375695109367, 0.004449343308806419, 0.004446316976100206, 0.0044432892464101315, 0.00444027129560709, 0.004437252879142761, 0.004434248432517052, 0.004431235138326883, 0.004428233951330185, 0.004425229504704475, 0.004422229714691639, 0.004419239237904549, 0.004416257608681917, 0.004413270857185125, 0.004410282243043184, 0.004407315980643034, 0.004404345061630011, 0.004401376936584711, 0.004398414399474859, 0.004395452328026295, 0.004392491187900305, 0.004389539826661348, 0.004386590328067541, 0.00438365014269948, 0.004380711819976568, 0.004377778619527817, 0.004374845419079065, 0.004371915943920612, 0.004368987400084734, 0.004366070032119751, 0.004363157320767641, 0.004360240884125233, 0.004357333295047283, 0.004354429896920919, 0.004351525567471981, 0.0043486375361680984, 0.0043457369320094585, 0.004342850763350725, 0.004339957144111395, 0.0043370844796299934, 0.004334202501922846, 0.004331324715167284, 0.0043284534476697445, 0.00432558823376894, 0.004322721157222986, 0.004319870378822088, 0.004317014943808317, 0.00431416928768158, 0.00431131012737751, 0.0043084691278636456, 0.004305627662688494, 0.004302798304706812, 0.004299953579902649, 0.004297131672501564, 0.004294300451874733, 0.004291480407118797, 0.004288668744266033, 0.004285851493477821, 0.004283036570996046, 0.0042802379466593266, 0.0042774309404194355, 0.004274636507034302, 0.004271843936294317, 0.004269039258360863, 0.0042662532068789005, 0.004263469483703375, 0.004260695539414883, 0.004257910419255495, 0.004255139734596014, 0.004252362996339798, 0.0042496006935834885, 0.004246838856488466, 0.004244076553732157, 0.004241325426846743, 0.0042385742999613285, 0.004235823173075914, 0.004233075305819511, 0.004230329301208258, 0.004227601457387209, 0.004224870353937149, 0.0042221322655677795, 0.0042194039560854435, 0.004216685425490141, 0.0042139580473303795, 0.004211234860122204, 0.004208529368042946, 0.004205819219350815, 0.004203109070658684, 0.004200404975563288, 0.004197710659354925, 0.004195008892565966, 0.004192324820905924, 0.004189627710729837, 0.004186946898698807, 0.004184267949312925, 0.004181584808975458, 0.00417891051620245, 0.004176233895123005, 0.004173563327640295, 0.004170895088464022, 0.004168242681771517, 0.004165584221482277, 0.0041629234328866005, 0.004160268232226372, 0.004157631192356348, 0.00415497412905097, 0.004152338951826096, 0.00414970563724637, 0.00414706626906991, 0.0041444264352321625, 0.004141802899539471, 0.004139179363846779, 0.0041365595534443855, 0.00413393834605813, 0.004131323657929897, 0.004128712695091963, 0.004126102663576603, 0.004123499616980553, 0.004120893310755491, 0.004118297714740038, 0.004115699790418148, 0.004113112576305866, 0.004110523033887148, 0.004107933957129717, 0.004105350933969021, 0.004102767910808325, 0.00410020025447011, 0.004097626078873873, 0.004095055628567934, 0.004092490766197443, 0.004089930560439825, 0.004087371286004782, 0.004084801767021418, 0.00408225879073143, 0.004079708829522133, 0.004077162593603134, 0.004074613098055124, 0.004072075709700584, 0.004069541115313768, 0.004066993948072195, 0.004064470995217562, 0.0040619466453790665, 0.004059417173266411, 0.004056899342685938, 0.004054379649460316, 0.004051855765283108, 0.004049350041896105, 0.004046835005283356, 0.00404432462528348, 0.00404182355850935, 0.004039324354380369, 0.004036827478557825, 0.004034343641251326, 0.004031848162412643, 0.004029355943202972, 0.004026869311928749, 0.004024391993880272, 0.004021911416202784, 0.004019427113234997, 0.00401696003973484, 0.004014498088508844, 0.0040120212361216545, 0.004009561147540808, 0.004007095005363226, 0.0040046414360404015, 0.004002185072749853, 0.003999735694378614, 0.003997281659394503, 0.003994845785200596, 0.0039923954755067825, 0.003989954479038715, 0.003987519536167383, 0.003985094837844372, 0.003982651513069868, 0.003980229143053293, 0.003977808635681868, 0.003975386265665293, 0.003972968552261591, 0.003970547113567591, 0.003968134522438049, 0.003965719137340784, 0.003963314462453127, 0.003960914444178343, 0.003958515357226133, 0.003956112079322338, 0.003953714855015278, 0.003951329737901688, 0.003948931582272053, 0.003946543671190739, 0.003944166470319033, 0.003941773436963558, 0.003939406480640173, 0.003937033005058765, 0.0039346604607999325, 0.0039322893135249615, 0.0039299167692661285, 0.0039275577291846275, 0.003925189841538668, 0.0039228410460054874, 0.003920493647456169, 0.003918130416423082, 0.00391578720882535, 0.003913436084985733, 0.003911093343049288, 0.003908755257725716, 0.003906415309756994, 0.003904081182554364, 0.0039017510134726763, 0.0038994168862700462, 0.0038970871828496456, 0.0038947605062276125, 0.003892447566613555, 0.0038901311345398426, 0.0038878126069903374, 0.003885496873408556, 0.003883186262100935, 0.003880887059494853, 0.0038785787764936686, 0.0038762763142585754, 0.0038739782758057117, 0.003871689550578594, 0.0038693943060934544, 0.003867097431793809, 0.0038648112677037716, 0.003862527897581458, 0.003860239638015628, 0.0038579662796109915, 0.0038556938525289297, 0.0038534151390194893, 0.003851146437227726, 0.0038488840218633413, 0.0038466041442006826, 0.0038443508092314005, 0.0038420867640525103, 0.00383983226493001, 0.003837574040517211, 0.0038353290874511003, 0.003833071794360876, 0.003830827074125409, 0.003828583285212517, 0.0038263422902673483, 0.003824104554951191, 0.003821869380772114, 0.0038196356035768986, 0.003817410673946142, 0.0038151752669364214, 0.003812953596934676, 0.0038107316941022873, 0.003808504668995738, 0.0038062878884375095, 0.0038040759973227978, 0.0038018650375306606, 0.0037996619939804077, 0.0037974556908011436, 0.0037952458951622248, 0.0037930470425635576, 0.003790851216763258, 0.0037886514328420162, 0.0037864618934690952, 0.003784267930313945, 0.003782084211707115, 0.003779900958761573, 0.0037777144461870193, 0.003775539342314005, 0.003773351199924946, 0.0037711793556809425, 0.0037690079770982265, 0.0037668340373784304, 0.0037646668497473, 0.003762501524761319, 0.0037603434175252914, 0.003758183214813471, 0.0037560253404080868, 0.0037538735195994377, 0.0037517284508794546, 0.003749563591554761, 0.0037474273703992367, 0.003745279274880886, 0.003743140958249569, 0.003741000546142459, 0.0037388682831078768, 0.0037367427721619606, 0.003734600730240345, 0.0037324661388993263, 0.0037303445860743523, 0.003728220472112298, 0.0037261012475937605, 0.003723978064954281, 0.003721862332895398, 0.003719756845384836, 0.0037176429759711027, 0.003715537255629897, 0.003713421756401658, 0.0037113225553184748, 0.003709223819896579, 0.0037071171682327986, 0.0037050226237624884, 0.0037029392551630735, 0.003700842848047614, 0.003698752261698246, 0.0036966705229133368, 0.0036945862229913473, 0.0036924993619322777, 0.0036904250737279654, 0.0036883431021124125, 0.0036862650886178017, 0.003684196388348937, 0.003682132577523589, 0.0036800571251660585, 0.003677995875477791, 0.0036759248469024897, 0.0036738549824804068, 0.0036718049086630344, 0.0036697506438940763, 0.003667698707431555, 0.0036656439770013094, 0.0036636008881032467, 0.0036615515127778053, 0.003659503534436226, 0.0036574676632881165, 0.0036554252728819847, 0.0036533779930323362, 0.0036513442173600197, 0.0036493095103651285, 0.003647285746410489, 0.0036452533677220345, 0.0036432272754609585, 0.003641207702457905, 0.0036391806788742542, 0.0036371545866131783, 0.003635138040408492, 0.0036331156734377146, 0.003631112864241004, 0.003629101440310478, 0.0036271053832024336, 0.0036250886041671038, 0.003623075783252716, 0.003621078794822097, 0.003619077615439892, 0.0036170827224850655, 0.003615077119320631, 0.0036130857188254595, 0.003611101768910885, 0.0036091154906898737, 0.0036071205977350473, 0.0036051380448043346, 0.0036031478084623814, 0.003601175732910633, 0.003599202958866954, 0.0035972262267023325, 0.0035952541511505842, 0.00359327532351017, 0.003591298358514905, 0.003589337458834052, 0.003587366547435522, 0.0035854089073836803, 0.0035834566224366426, 0.003581494325771928, 0.003579525277018547, 0.0035775804426521063, 0.003575625829398632, 0.0035736828576773405, 0.003571727778762579, 0.0035697827115654945, 0.0035678374115377665, 0.003565895603969693, 0.0035639500711113214, 0.0035620250273495913, 0.003560083219781518, 0.003558150492608547, 0.0035562149714678526, 0.0035542920231819153, 0.0035523567348718643, 0.0035504368133842945, 0.0035485122352838516, 0.003546595573425293, 0.0035446779802441597, 0.0035427603870630264, 0.0035408451221883297, 0.0035389303229749203, 0.003537018783390522, 0.003535109106451273, 0.003533201292157173, 0.0035313009284436703, 0.003529398934915662, 0.003527495777234435, 0.0035255926195532084, 0.003523707389831543, 0.0035218046978116035, 0.0035199164412915707, 0.0035180291160941124, 0.003516140393912792, 0.003514248179271817, 0.003512369003146887, 0.0035104902926832438, 0.0035086097195744514, 0.00350672984495759, 0.0035048555582761765, 0.003502980573102832, 0.0035011086147278547, 0.003499237820506096, 0.0034973653964698315, 0.0034954994916915894, 0.003493637777864933, 0.0034917721059173346, 0.003489923197776079, 0.0034880610182881355, 0.003486201399937272, 0.0034843480680137873, 0.003482497064396739, 0.0034806516487151384, 0.0034788029734045267, 0.0034769601188600063, 0.003475105157122016, 0.0034732650965452194, 0.003471420146524906, 0.0034695849753916264, 0.003467750269919634, 0.0034659230150282383, 0.003464088076725602, 0.0034622522071003914, 0.003460422856733203, 0.0034585981629788876, 0.0034567713737487793, 0.0034549504052847624, 0.003453125711530447, 0.0034513070713728666, 0.0034494956489652395, 0.0034476756118237972, 0.0034458618611097336, 0.003444056725129485, 0.0034422334283590317, 0.003440435277298093, 0.0034386313054710627, 0.003436820115894079, 0.003435020800679922, 0.003433224745094776, 0.003431426826864481, 0.003429633332416415, 0.0034278377424925566, 0.0034260412212461233, 0.0034242537803947926, 0.0034224584233015776, 0.0034206712152808905, 0.0034188912250101566, 0.003417106345295906, 0.0034153240267187357, 0.00341355474665761, 0.0034117656759917736, 0.0034099863842129707, 0.0034082222264260054, 0.0034064408391714096, 0.0034046685323119164, 0.0034028973896056414, 0.003401136491447687, 0.0033993732649832964, 0.003397610504180193, 0.003395848209038377, 0.0033940915018320084, 0.0033923315349966288, 0.003390578320249915, 0.003388826036825776, 0.0033870702609419823, 0.00338532030582428, 0.003383568488061428, 0.0033818252850323915, 0.0033800797536969185, 0.003378335852175951, 0.003376592416316271, 0.003374855499714613, 0.003373126033693552, 0.0033713849261403084, 0.003369644982740283, 0.003367908298969269, 0.0033661862835288048, 0.0033644542563706636, 0.003362729912623763, 0.0033610048703849316, 0.0033592835534363985, 0.0033575627021491528, 0.003355837194249034, 0.003354121930897236, 0.003352406434714794, 0.003350687911733985, 0.003348973346874118, 0.003347266698256135, 0.003345555393025279, 0.003343846881762147, 0.0033421427942812443, 0.003340437775477767, 0.0033387348521500826, 0.003337039379402995, 0.0033353427425026894, 0.00333363632671535, 0.0033319422509521246, 0.0033302498050034046, 0.0033285534009337425, 0.0033268711995333433, 0.0033251759596168995, 0.003323489800095558, 0.0033217985183000565, 0.0033201181795448065, 0.0033184350468218327, 0.003316758666187525, 0.0033150864765048027, 0.0033133989199995995, 0.003311724402010441, 0.0033100515138357878, 0.003308382350951433, 0.0033067038748413324, 0.003305044723674655, 0.00330337998457253, 0.0033017126843333244, 0.0033000472467392683, 0.003298384603112936, 0.0032967214938253164, 0.0032950721215456724, 0.0032934173941612244, 0.0032917545177042484, 0.003290100023150444, 0.003288449952378869, 0.003286802675575018, 0.0032851500436663628, 0.003283509984612465, 0.003281861310824752, 0.0032802210189402103, 0.0032785777002573013, 0.003276936709880829, 0.0032753010746091604, 0.0032736624125391245, 0.0032720190938562155, 0.0032703836914151907, 0.0032687594648450613, 0.0032671287190169096, 0.0032655009999871254, 0.003263872815296054, 0.003262249520048499, 0.0032606327440589666, 0.003259007353335619, 0.003257378237321973, 0.003255759831517935, 0.003254141891375184]\n\n\nThe logistic regression model has been fit perfectly to the training data.\n\nfind_accuracy(X_train, y_train)\n\nAccuracy: 1.0\n\n\nSo we achieved 100% accuracy.\nThen we must initialize the test data with the same parameters as the training data.\n\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nfind_accuracy(X_test, y_test)\n\nAccuracy: 1.0\n\n\nWe can see that the logistic regression model has overfit to the training data and cannot classify the test data with the same accuracy. This it the danger of fitting a model too well to training data, as it is now not generalizable to other data.\n\n\n\nIn this blog post, I was able to investigate more about gradient descent and its application in solving the empirical risk minimization problem, specifically focusing on logistic regression. By implementing gradient descent in the logistic regression model, I gained a deeper understanding of how the algorithm works and how it can be customized to suit different classification needs.\nFurthermore, I explored a key variant of gradient descent called momentum, which allows logistic regression to achieve faster convergence. Through experiments and analysis, we observed the impact of momentum on the convergence speed on the logistic regression model.\nOverall, these experiments taught me valuable lessons in optimization techniques for machine learning models. Through this experience I learned the importance of different parameters though changing the value of beta, and the impact of overfitting on our training data. By combining theory with practical implementation and experimentation, I gained a comprehensive understanding of gradient descent and its variants in the context of logistic regression.\nAs we continue to learn various machine learning algorithms and optimization techniques, the knowledge and insights gained from this blog post will help me develop practices to build more complex and efficient models in the future."
  },
  {
    "objectID": "posts/post5/homework5.html#abstract",
    "href": "posts/post5/homework5.html#abstract",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Link to source code (logistic.py)\nFor this blog post I implemented logistic regression and performed several experiments on my model. The first experiment I conducted was to see vanilla gradient descent. Vanilla gradient descent is when beta=0 and we were able to know that the model works because we saw loss decrease monotonically. The second experiment was to understand the benefits of momentum. By increasing the beta value we were able to see loss decrease faster than the vanilla gradient descent case. Then, finally, we conducted an experiment to see the potential harms of overfitting our data. By reaching 100% accuracy on training data, we could see the drawback directly by a lower accuracy rate on training data. Overall, I was able to learn more about implementing machine learning models and how to test their functionality. I was able to concretely understand the benefits of momentum and the drawbacks of overfitting."
  },
  {
    "objectID": "posts/post5/homework5.html#experiments",
    "href": "posts/post5/homework5.html#experiments",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Before doing any experiments, I had to generate data for a classification problem.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nPlot the data\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# p_dims is 2\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nCode to graph a straight line\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nThe first experiment that I performed was vanilla gradient descent: When p_dim = 2, when alpha is sufficiently small and beta=0.\nWant to see:\n\nGradient descent for logistic regression converges to a weight vector w that looks visually correct\n\nshow this by plot the decision boundary with the data\n\nLoss decreases monotonically: A monotonic function is a function which is either entirely nonincreasing or nondecreasing.\n\nshow this by plotting the loss over iterations\n\n\nFirst implement a training loop with graphs with a dividing line to visualize our progress.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec_van = []\n\nfor index in range(5000):\n    \n    # for vanilla gradient descent, alpha must be sufficiently small and beta must be 0\n    opt.step(X, y, alpha = 0.01, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec_van.append(loss)\n\n\ndef find_accuracy(X, y):\n\n    predictions = LR.predict(X)\n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 0.996666669845581\n\n\nPlot the loss over time over the 5000 iterations.\n\nimport numpy as np\n\ndef plot_loss(loss, label= \"\"):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(loss, color = \"blue\", label=label)\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\nplot_loss(loss_vec_van)\n\n\n\n\n\n\n\n\nWe can see that the loss is decreasing monotonically over time through this graph of the loss. The negative slope shows us that the loss is in fact decreasing over time. In other words, our machine learning model is learning!\nPlot the final line separating the data\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nWe can see that the dividing line almost perfectly divides the classes. In time, we could see the logistic regression training could become perfectly accurate."
  },
  {
    "objectID": "posts/post5/homework5.html#benefits-of-momentum",
    "href": "posts/post5/homework5.html#benefits-of-momentum",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Our next experiment was to see the benefits of momentum. On the same data, gradient descent with momentum (e.g. beta=0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta=0).\nWe want to see:\n\nA model that learns at a faster rate\n\nshow loss decreasing at a faster rate than when beta was 0\n\n\nFirst implement a training loop with graphs with a dividing line to visualize our progress.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nX, y = classification_data(noise = 0.2)\n\n# initialize for main loop\nloss_vec_mom = []\n\nfor index in range(5000):\n    \n    # to see the benefits of momentum, alpha must be sufficiently small and beta must be 0.9\n    opt.step(X, y, alpha = 0.01, beta = 0.9)\n    loss = LR.loss(X, y).item()            \n    loss_vec_mom.append(loss)\n\nPlot the loss over time over the 5000 iterations.\n\nplt.plot(loss_vec_van, color = \"green\", label='Vanilla')\nplt.plot(loss_vec_mom, color = \"blue\", label='Momentum')\n\nplt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe faster declining slope of the loss shows that the larger beta value does in fact increase the learning speed of the machine learning model.\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nWe can see the benefits of increase of momentum by viewing the improved dividing line. The increase in the beta value allows our logistic regression to improve at a much faster rate then when beta=0. We know that because with the same number of iterations, the loss decreased more, or in other words, the model learned to classify at a faster rate.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nA perfect accuracy rate!"
  },
  {
    "objectID": "posts/post5/homework5.html#overfitting",
    "href": "posts/post5/homework5.html#overfitting",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Our final experiment was to show the danger of overfitting. To show this I need to generate some data where p_dim &gt; n_points and create an instance where the same logistic regression model has a 100% accuracy rate on training data.\nWant to see:\n\nPerfect accuracy for training data\nLess accurate classification for testing data with the exact same parameters\n\nFor overfitting, we need to generate data where p_dim &gt; n_points.\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nGoal to achieve 100% accuracy with the training data.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(2000):\n   \n    opt.step(X_train, y_train, alpha = 0.01, beta = 0.9)\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec.append(loss)\n\n\nprint(loss_vec)\n\n[0.633738100528717, 0.5438883304595947, 0.4618161916732788, 0.40402740240097046, 0.36823952198028564, 0.3463859558105469, 0.33207085728645325, 0.32158127427101135, 0.3129945993423462, 0.30535393953323364, 0.2981888949871063, 0.29127079248428345, 0.2844916582107544, 0.27780479192733765, 0.2711944878101349, 0.2646607458591461, 0.25821107625961304, 0.25185608863830566, 0.24560725688934326, 0.23947563767433167, 0.23347118496894836, 0.22760243713855743, 0.22187648713588715, 0.21629880368709564, 0.21087360382080078, 0.20560361444950104, 0.20049048960208893, 0.19553472101688385, 0.19073593616485596, 0.18609294295310974, 0.1816038191318512, 0.17726610600948334, 0.17307688295841217, 0.16903269290924072, 0.16512995958328247, 0.16136476397514343, 0.1577329784631729, 0.15423043072223663, 0.15085284411907196, 0.1475958526134491, 0.14445513486862183, 0.14142641425132751, 0.13850541412830353, 0.1356879472732544, 0.1329699158668518, 0.13034740090370178, 0.12781642377376556, 0.1253732591867447, 0.12301427125930786, 0.12073586136102676, 0.11853475868701935, 0.11640762537717819, 0.1143513098359108, 0.11236286163330078, 0.11043937504291534, 0.10857807099819183, 0.10677634924650192, 0.10503166168928146, 0.10334160178899765, 0.10170385986566544, 0.10011627525091171, 0.09857672452926636, 0.09708324074745178, 0.09563390910625458, 0.0942269116640091, 0.09286051988601685, 0.0915331095457077, 0.09024310111999512, 0.08898898959159851, 0.08776937425136566, 0.08658287674188614, 0.08542823046445847, 0.08430419862270355, 0.08320960402488708, 0.08214332908391953, 0.08110430091619492, 0.08009150624275208, 0.0791039913892746, 0.07814081013202667, 0.07720109820365906, 0.0762840062379837, 0.07538872957229614, 0.07451450824737549, 0.07366058975458145, 0.07282629609107971, 0.07201094925403595, 0.07121390104293823, 0.0704345628619194, 0.06967232376337051, 0.06892664730548859, 0.06819696724414825, 0.06748278439044952, 0.0667836144566536, 0.06609895825386047, 0.06542839109897614, 0.06477145105600357, 0.06412775814533234, 0.06349688023328781, 0.06287844479084015, 0.0622720830142498, 0.06167743727564812, 0.06109416484832764, 0.06052194535732269, 0.05996045097708702, 0.059409402310848236, 0.05886848270893097, 0.05833742767572403, 0.05781596153974533, 0.05730381980538368, 0.056800756603479385, 0.05630652979016304, 0.05582091212272644, 0.05534365773200989, 0.05487455800175667, 0.05441341921687126, 0.05396000295877457, 0.05351414158940315, 0.053075648844242096, 0.052644334733486176, 0.05222000926733017, 0.05180253088474274, 0.05139169096946716, 0.050987374037504196, 0.05058939382433891, 0.05019763112068176, 0.04981191083788872, 0.04943211376667023, 0.04905809089541435, 0.04868970066308975, 0.04832683503627777, 0.04796937480568886, 0.04761717468500137, 0.04727013036608696, 0.046928148716688156, 0.046591076999902725, 0.046258844435214996, 0.04593133553862572, 0.045608438551425934, 0.045290082693099976, 0.044976137578487396, 0.04466654732823372, 0.04436120018362999, 0.04406002163887024, 0.04376290738582611, 0.043469786643981934, 0.043180570006370544, 0.042895182967185974, 0.042613573372364044, 0.0423356257379055, 0.04206128418445587, 0.04179048910737038, 0.04152318462729454, 0.041259266436100006, 0.040998686105012894, 0.04074139520525932, 0.04048732668161392, 0.040236398577690125, 0.039988573640584946, 0.03974378481507301, 0.039501968771219254, 0.03926309943199158, 0.03902710601687431, 0.03879392519593239, 0.03856353089213371, 0.038335852324962616, 0.038110848516225815, 0.03788847476243973, 0.03766869008541107, 0.03745144233107567, 0.03723669424653053, 0.03702438995242119, 0.03681449592113495, 0.03660697489976883, 0.03640177845954895, 0.036198876798152924, 0.035998232662677765, 0.03579980134963989, 0.03560354933142662, 0.035409435629844666, 0.035217419266700745, 0.03502750024199486, 0.03483960032463074, 0.034653712064027786, 0.03446980565786362, 0.034287840127944946, 0.03410777449607849, 0.033929597586393356, 0.033753279596567154, 0.0335787832736969, 0.03340607509016991, 0.03323514759540558, 0.03306595981121063, 0.03289847820997238, 0.03273269534111023, 0.03256857767701149, 0.032406099140644073, 0.0322452187538147, 0.03208594769239426, 0.03192823752760887, 0.031772054731845856, 0.031617421656847, 0.031464263796806335, 0.03131258487701416, 0.031162375584244728, 0.03101358562707901, 0.030866215005517006, 0.03072025254368782, 0.03057565540075302, 0.030432408675551414, 0.030290512368083, 0.030149927362799644, 0.030010638758540154, 0.029872631654143333, 0.029735885560512543, 0.029600396752357483, 0.029466135427355766, 0.029333079233765602, 0.02920122817158699, 0.02907055802643299, 0.02894105762243271, 0.028812691569328308, 0.028685469180345535, 0.02855936996638775, 0.028434377163648605, 0.028310470283031464, 0.028187651187181473, 0.0280658770352602, 0.027945155277848244, 0.027825474739074707, 0.0277068130671978, 0.027589164674282074, 0.02747250720858574, 0.0273568257689476, 0.027242114767432213, 0.027128372341394424, 0.02701556868851185, 0.026903703808784485, 0.026792753487825394, 0.026682719588279724, 0.02657359652221203, 0.026465341448783875, 0.02635798044502735, 0.02625148370862007, 0.02614584192633629, 0.02604104019701481, 0.02593708597123623, 0.02583394944667816, 0.0257316492497921, 0.025630144402384758, 0.025529442355036736, 0.02542952261865139, 0.02533038891851902, 0.025232018902897835, 0.025134418159723282, 0.02503756247460842, 0.024941453710198402, 0.02484607696533203, 0.024751434102654457, 0.024657506495714188, 0.024564284831285477, 0.02447177842259407, 0.02437996119260788, 0.024288825690746307, 0.024198371917009354, 0.02410859800875187, 0.024019470438361168, 0.02393101155757904, 0.023843200877308846, 0.023756034672260284, 0.02366950735449791, 0.02358359843492508, 0.02349831536412239, 0.02341364696621895, 0.023329583927989006, 0.023246141150593758, 0.023163264617323875, 0.023080989718437195, 0.022999301552772522, 0.022918183356523514, 0.022837644442915916, 0.022757668048143387, 0.022678246721625328, 0.02259938232600689, 0.022521063685417175, 0.022443287074565887, 0.022366054356098175, 0.0222893375903368, 0.022213155403733253, 0.02213749848306179, 0.02206234261393547, 0.02198770083487034, 0.021913563832640648, 0.02183992601931095, 0.0217667818069458, 0.021694118157029152, 0.02162194810807705, 0.0215502567589283, 0.021479027345776558, 0.021408285945653915, 0.021338002756237984, 0.021268177777528763, 0.02119881473481655, 0.021129891276359558, 0.02106141857802868, 0.020993396639823914, 0.02092580311000347, 0.020858652889728546, 0.020791923627257347, 0.02072562277317047, 0.020659752190113068, 0.020594289526343346, 0.02052924782037735, 0.020464608445763588, 0.020400382578372955, 0.02033654972910881, 0.020273126661777496, 0.020210087299346924, 0.020147452130913734, 0.02008519321680069, 0.020023318007588387, 0.019961828365921974, 0.019900711253285408, 0.019839977845549583, 0.019779611378908157, 0.019719600677490234, 0.01965995691716671, 0.019600683823227882, 0.01954176276922226, 0.01948319748044014, 0.01942497305572033, 0.019367104396224022, 0.019309576600790024, 0.019252397119998932, 0.019195545464754105, 0.019139036536216736, 0.01908285543322563, 0.01902700588107109, 0.018971487879753113, 0.018916286528110504, 0.018861401826143265, 0.01880684122443199, 0.018752597272396088, 0.01869867369532585, 0.018645040690898895, 0.01859172433614731, 0.01853870414197445, 0.01848599687218666, 0.0184335820376873, 0.018381472676992416, 0.018329650163650513, 0.018278125673532486, 0.01822689361870289, 0.018175937235355377, 0.0181252621114254, 0.01807487942278385, 0.018024779856204987, 0.01797494851052761, 0.01792539469897747, 0.017876114696264267, 0.0178271122276783, 0.017778366804122925, 0.017729882150888443, 0.017681676894426346, 0.017633728682994843, 0.01758604124188423, 0.017538612708449364, 0.017491435632109642, 0.017444506287574768, 0.017397843301296234, 0.017351416870951653, 0.01730523258447647, 0.017259307205677032, 0.017213625833392143, 0.017168192192912102, 0.017122982069849968, 0.017078010365366936, 0.017033280804753304, 0.016988782212138176, 0.016944529488682747, 0.01690048538148403, 0.016856681555509567, 0.016813097521662712, 0.016769759356975555, 0.016726627945899963, 0.01668372191488743, 0.01664102077484131, 0.016598554328083992, 0.016556303948163986, 0.016514264047145844, 0.016472432762384415, 0.016430824995040894, 0.016389409080147743, 0.0163482204079628, 0.016307227313518524, 0.01626645401120186, 0.016225868836045265, 0.016185499727725983, 0.016145318746566772, 0.016105342656373978, 0.01606556586921215, 0.016025977209210396, 0.015986597165465355, 0.015947403386235237, 0.015908408910036087, 0.015869593247771263, 0.015830975025892258, 0.015792537480592728, 0.01575428992509842, 0.015716219320893288, 0.01567832939326763, 0.01564064435660839, 0.015603124164044857, 0.015565778128802776, 0.015528624877333641, 0.01549164392054081, 0.01545484084635973, 0.01541820727288723, 0.015381750650703907, 0.015345468185842037, 0.015309352427721024, 0.01527340803295374, 0.015237616375088692, 0.015202010050415993, 0.015166568569839, 0.015131289139389992, 0.015096166171133518, 0.0150612136349082, 0.015026427805423737, 0.014991790056228638, 0.014957324601709843, 0.014923018403351307, 0.014888865873217583, 0.01485486887395382, 0.014821025542914867, 0.014787348918616772, 0.014753805473446846, 0.014720415696501732, 0.014687180519104004, 0.014654100872576237, 0.01462116464972496, 0.014588368125259876, 0.014555740170180798, 0.014523230493068695, 0.014490890316665173, 0.014458680525422096, 0.014426631852984428, 0.014394722878932953, 0.014362944290041924, 0.01433132030069828, 0.01429982203990221, 0.014268457889556885, 0.014237234368920326, 0.014206149615347385, 0.014175190590322018, 0.014144386164844036, 0.014113698154687881, 0.014083152636885643, 0.014052736572921276, 0.014022455550730228, 0.013992296531796455, 0.013962272554636002, 0.013932373374700546, 0.013902622275054455, 0.013872980140149593, 0.013843467459082603, 0.013814077712595463, 0.01378481462597847, 0.013755671679973602, 0.013726655393838882, 0.013697751797735691, 0.01366897951811552, 0.013640315271914005, 0.01361178606748581, 0.013583367690443993, 0.013555066660046577, 0.013526885770261288, 0.01349882036447525, 0.013470884412527084, 0.013443049974739552, 0.013415336608886719, 0.013387740589678288, 0.013360254466533661, 0.013332877308130264, 0.013305635191500187, 0.013278481550514698, 0.013251448050141335, 0.013224517926573753, 0.013197695836424828, 0.013170987367630005, 0.013144386000931263, 0.013117888942360878, 0.013091490603983402, 0.013065210543572903, 0.01303903479129076, 0.013012951239943504, 0.012986983172595501, 0.012961126863956451, 0.012935347855091095, 0.012909691780805588, 0.012884140014648438, 0.012858685106039047, 0.012833324261009693, 0.012808071449398994, 0.012782922945916653, 0.012757867574691772, 0.012732910923659801, 0.012708064168691635, 0.012683290988206863, 0.012658625841140747, 0.012634054757654667, 0.012609578669071198, 0.012585198506712914, 0.012560906819999218, 0.012536711990833282, 0.012512611225247383, 0.012488599866628647, 0.012464679777622223, 0.012440850958228111, 0.012417115271091461, 0.012393467128276825, 0.01236991211771965, 0.012346451170742512, 0.012323069386184216, 0.012299779802560806, 0.012276582419872284, 0.012253474444150925, 0.012230447493493557, 0.01220751740038395, 0.012184670194983482, 0.012161901220679283, 0.01213922630995512, 0.012116632424294949, 0.012094126082956791, 0.012071695178747177, 0.012049359269440174, 0.012027101591229439, 0.012004919350147247, 0.011982819996774197, 0.011960810050368309, 0.011938877403736115, 0.011917012743651867, 0.01189524307847023, 0.011873540468513966, 0.011851922608911991, 0.011830383911728859, 0.01180893275886774, 0.011787539348006248, 0.011766238138079643, 0.011745008639991283, 0.011723853647708893, 0.01170278713107109, 0.011681786738336086, 0.011660866439342499, 0.011640023440122604, 0.011619243770837784, 0.011598551645874977, 0.01157793402671814, 0.011557374149560928, 0.011536904610693455, 0.011516503058373928, 0.011496172286570072, 0.011475921608507633, 0.011455731466412544, 0.011435611173510551, 0.011415582150220871, 0.011395602487027645, 0.011375694535672665, 0.01135585643351078, 0.011336083523929119, 0.011316383257508278, 0.011296744458377361, 0.011277178302407265, 0.011257678270339966, 0.011238238774240017, 0.011218883097171783, 0.011199580505490303, 0.01118034590035677, 0.011161179281771183, 0.011142078787088394, 0.011123034171760082, 0.011104064993560314, 0.01108515728265047, 0.011066317558288574, 0.01104754488915205, 0.011028826236724854, 0.0110101830214262, 0.01099159475415945, 0.010973077267408371, 0.010954611003398895, 0.01093621738255024, 0.010917886160314083, 0.010899608954787254, 0.010881399735808372, 0.010863247327506542, 0.010845160111784935, 0.010827148333191872, 0.010809175670146942, 0.010791275650262833, 0.010773427784442902, 0.010755636729300022, 0.010737907141447067, 0.010720236226916313, 0.010702623054385185, 0.010685062035918236, 0.010667573660612106, 0.010650125332176685, 0.010632737539708614, 0.010615409351885319, 0.010598130524158478, 0.010580918751657009, 0.010563752613961697, 0.010546651668846607, 0.010529591701924801, 0.010512597858905792, 0.010495648719370365, 0.010478755459189415, 0.01046192366629839, 0.010445140302181244, 0.010428410954773426, 0.010411742143332958, 0.010395115241408348, 0.010378550738096237, 0.010362026281654835, 0.01034557819366455, 0.010329164564609528, 0.010312809608876705, 0.010296506807208061, 0.010280255228281021, 0.01026405580341816, 0.010247906669974327, 0.010231805965304375, 0.010215753689408302, 0.01019976008683443, 0.010183824226260185, 0.010167925618588924, 0.010152066126465797, 0.010136292316019535, 0.010120546445250511, 0.010104848071932793, 0.01008920930325985, 0.010073610581457615, 0.010058061219751835, 0.010042560286819935, 0.010027103126049042, 0.010011697188019753, 0.009996342472732067, 0.009981030598282814, 0.009965762495994568, 0.009950540028512478, 0.009935375303030014, 0.009920244105160236, 0.009905153885483742, 0.00989012885838747, 0.009875130839645863, 0.009860186837613583, 0.00984528660774231, 0.009830429218709469, 0.009815610945224762, 0.009800847619771957, 0.009786119684576988, 0.00977144856005907, 0.00975680910050869, 0.00974221620708704, 0.009727670811116695, 0.009713170118629932, 0.009698702953755856, 0.009684288874268532, 0.009669915772974491, 0.00965557899326086, 0.00964128877967596, 0.009627040475606918, 0.00961282942444086, 0.009598658420145512, 0.009584545157849789, 0.009570457972586155, 0.009556428529322147, 0.009542430751025677, 0.009528476744890213, 0.00951455906033516, 0.009500687010586262, 0.009486845694482327, 0.009473067708313465, 0.009459313005208969, 0.009445608593523502, 0.009431927464902401, 0.00941829476505518, 0.009404707700014114, 0.009391160681843758, 0.009377644397318363, 0.009364182129502296, 0.009350746870040894, 0.00933736003935337, 0.009324000217020512, 0.009310691617429256, 0.009297401644289494, 0.00928416382521391, 0.009270966984331608, 0.009257792495191097, 0.009244668297469616, 0.009231572970747948, 0.009218519553542137, 0.009205501526594162, 0.009192511439323425, 0.009179568849503994, 0.009166655130684376, 0.009153790771961212, 0.00914094503968954, 0.0091281458735466, 0.009115377441048622, 0.009102646261453629, 0.009089949540793896, 0.009077288210391998, 0.009064658544957638, 0.009052070789039135, 0.009039511904120445, 0.009026985615491867, 0.00901450589299202, 0.009002056904137135, 0.008989628404378891, 0.008977248333394527, 0.008964898064732552, 0.008952583186328411, 0.00894029438495636, 0.008928051218390465, 0.008915829472243786, 0.008903641253709793, 0.008891492150723934, 0.008879383094608784, 0.008867292664945126, 0.0088552450761199, 0.008843227289617062, 0.008831238374114037, 0.0088192792609334, 0.00880736205726862, 0.008795462548732758, 0.008783616125583649, 0.008771789260208607, 0.00875999964773655, 0.008748234249651432, 0.008736498653888702, 0.008724815212190151, 0.008713137358427048, 0.00870150700211525, 0.008689898066222668, 0.008678330108523369, 0.008666795678436756, 0.008655278943479061, 0.008643806912004948, 0.008632354438304901, 0.008620933629572392, 0.008609545417129993, 0.008598193526268005, 0.008586866781115532, 0.00857557449489832, 0.00856431107968092, 0.008553066290915012, 0.00854188296943903, 0.008530696853995323, 0.0085195517167449, 0.008508438244462013, 0.00849735178053379, 0.008486300706863403, 0.008475268259644508, 0.008464280515909195, 0.008453297428786755, 0.008442363701760769, 0.008431445807218552, 0.008420557714998722, 0.008409702219069004, 0.008398869074881077, 0.008388069458305836, 0.008377295918762684, 0.008366545662283897, 0.008355831727385521, 0.008345132693648338, 0.008334473706781864, 0.008323834277689457, 0.008313219994306564, 0.008302636444568634, 0.008292073383927345, 0.008281546644866467, 0.008271039463579655, 0.00826056208461523, 0.008250114507973194, 0.00823968555778265, 0.008229285478591919, 0.00821891613304615, 0.008208568207919598, 0.008198249153792858, 0.00818795058876276, 0.008177685551345348, 0.008167429827153683, 0.008157218806445599, 0.008147021755576134, 0.008136851713061333, 0.00812671147286892, 0.008116597309708595, 0.008106500841677189, 0.00809643417596817, 0.00808639731258154, 0.008076366037130356, 0.008066377602517605, 0.008056405000388622, 0.008046472445130348, 0.008036535233259201, 0.008026642724871635, 0.00801677256822586, 0.008006924763321877, 0.007997103035449982, 0.00798730831593275, 0.007977524772286415, 0.007967781275510788, 0.007958047091960907, 0.007948347367346287, 0.007938667200505733, 0.00792901124805212, 0.007919371128082275, 0.007909778505563736, 0.007900181226432323, 0.007890617474913597, 0.007881076075136662, 0.007871560752391815, 0.007862064987421036, 0.007852593436837196, 0.007843147031962872, 0.007833724841475487, 0.007824317552149296, 0.007814938202500343, 0.007805581670254469, 0.0077962446957826614, 0.007786931935697794, 0.007777645718306303, 0.007768371608108282, 0.007759136147797108, 0.007749906741082668, 0.007740712724626064, 0.00773153081536293, 0.007722381502389908, 0.007713246159255505, 0.007704134564846754, 0.007695045322179794, 0.007685979828238487, 0.007676927372813225, 0.007667903788387775, 0.00765890022739768, 0.007649928797036409, 0.007640963885933161, 0.007632033433765173, 0.007623111829161644, 0.0076142167672514915, 0.007605339400470257, 0.007596494629979134, 0.007587661035358906, 0.007578848395496607, 0.007570060435682535, 0.007561282720416784, 0.007552536204457283, 0.007543810643255711, 0.007535101845860481, 0.007526407018303871, 0.007517742458730936, 0.007509091403335333, 0.00750045757740736, 0.00749184750020504, 0.007483258377760649, 0.0074746813625097275, 0.007466130889952183, 0.007457602769136429, 0.0074490890838205814, 0.007440594024956226, 0.007432121783494949, 0.007423659786581993, 0.007415228057652712, 0.007406814023852348, 0.007398413494229317, 0.007390039041638374, 0.0073816776275634766, 0.00737333670258522, 0.007365015335381031, 0.007356700953096151, 0.007348412182182074, 0.007340153679251671, 0.0073319124057888985, 0.007323676720261574, 0.007315460592508316, 0.007307269610464573, 0.007299093063920736, 0.007290935609489679, 0.007282801903784275, 0.007274670526385307, 0.007266576401889324, 0.0072584873996675014, 0.007250417955219746, 0.007242375984787941, 0.00723434379324317, 0.0072263265028595924, 0.007218333892524242, 0.007210352923721075, 0.0072023929096758366, 0.007194451056420803, 0.007186520844697952, 0.007178615313023329, 0.007170721888542175, 0.007162848487496376, 0.007154995109885931, 0.007147153839468956, 0.007139327470213175, 0.007131523452699184, 0.007123734336346388, 0.007115961983799934, 0.007108209189027548, 0.00710047222673893, 0.00709275109693408, 0.007085044402629137, 0.007077352609485388, 0.007069683168083429, 0.007062026299536228, 0.007054388057440519, 0.007046771701425314, 0.007039153948426247, 0.007031569257378578, 0.007024005055427551, 0.007016440853476524, 0.007008907850831747, 0.007001371588557959, 0.006993868853896856, 0.006986378226429224, 0.006978901568800211, 0.006971440277993679, 0.006963989697396755, 0.006956565193831921, 0.006949152797460556, 0.006941754836589098, 0.006934370379894972, 0.006927005480974913, 0.006919649429619312, 0.00691232131794095, 0.006904997862875462, 0.006897692568600178, 0.006890406832098961, 0.0068831308744847775, 0.006875867955386639, 0.006868631113320589, 0.006861405447125435, 0.006854191422462463, 0.006846996955573559, 0.006839805748313665, 0.006832651328295469, 0.006825484801083803, 0.00681835412979126, 0.006811228580772877, 0.006804130505770445, 0.006797031965106726, 0.006789958570152521, 0.00678289495408535, 0.006775849498808384, 0.0067688170820474625, 0.0067617930471897125, 0.006754789501428604, 0.006747799459844828, 0.006740829441696405, 0.0067338645458221436, 0.0067269206047058105, 0.00671999529004097, 0.006713078822940588, 0.006706177722662687, 0.006699288263916969, 0.006692416034638882, 0.006685548461973667, 0.00667871069163084, 0.006671878509223461, 0.006665067747235298, 0.006658259779214859, 0.006651477422565222, 0.0066447025164961815, 0.006637943908572197, 0.006631198339164257, 0.006624465808272362, 0.006617764011025429, 0.006611051503568888, 0.006604357156902552, 0.00659768283367157, 0.006591021548956633, 0.006584377959370613, 0.006577745079994202, 0.0065711187198758125, 0.006564512383192778, 0.006557926069945097, 0.006551347207278013, 0.0065447743982076645, 0.006538227666169405, 0.006531683728098869, 0.006525161676108837, 0.006518653593957424, 0.006512152496725321, 0.006505663972347975, 0.006499183364212513, 0.006492726970463991, 0.006486285477876663, 0.006479848176240921, 0.006473428104072809, 0.006467009894549847, 0.006460616365075111, 0.006454231683164835, 0.006447855848819017, 0.006441492587327957, 0.006435150280594826, 0.006428818218410015, 0.006422500591725111, 0.0064161852933466434, 0.006409894675016403, 0.00640359940007329, 0.006397326476871967, 0.006391067057847977, 0.006384817883372307, 0.006378581747412682, 0.006372362375259399, 0.006366144400089979, 0.006359945051372051, 0.006353757344186306, 0.00634758872911334, 0.006341420579701662, 0.006335269659757614, 0.006329124793410301, 0.006323000881820917, 0.006316883955150843, 0.006310780066996813, 0.006304674781858921, 0.006298597902059555, 0.006292534060776234, 0.006286469753831625, 0.006280425935983658, 0.006274391897022724, 0.0062683699652552605, 0.006262357346713543, 0.00625635264441371, 0.006250367034226656, 0.006244397722184658, 0.006238419562578201, 0.00623246468603611, 0.006226527038961649, 0.006220594514161348, 0.006214665714651346, 0.006208760663866997, 0.006202866323292255, 0.006196971982717514, 0.006191100459545851, 0.0061852335929870605, 0.006179377902299166, 0.006173540838062763, 0.0061677005141973495, 0.006161878816783428, 0.006156077608466148, 0.006150275003165007, 0.006144484970718622, 0.006138714030385017, 0.006132945418357849, 0.0061271898448467255, 0.006121443118900061, 0.006115708965808153, 0.006109983194619417, 0.006104275118559599, 0.006098574493080378, 0.006092878058552742, 0.006087201647460461, 0.006081533618271351, 0.006075863726437092, 0.006070215255022049, 0.006064583547413349, 0.006058948114514351, 0.006053332705050707, 0.006047731731086969, 0.006042126566171646, 0.006036539562046528, 0.006030960474163294, 0.006025394890457392, 0.006019837688654661, 0.006014289800077677, 0.0060087586753070354, 0.0060032266192138195, 0.005997713189572096, 0.0059922123327851295, 0.0059867138043046, 0.005981224589049816, 0.005975754931569099, 0.005970277823507786, 0.005964819807559252, 0.005959382280707359, 0.0059539503417909145, 0.00594851840287447, 0.005943107418715954, 0.005937708541750908, 0.005932298488914967, 0.00592691358178854, 0.005921542644500732, 0.005916171707212925, 0.005910817999392748, 0.005905468948185444, 0.005900128278881311, 0.00589480297639966, 0.005889481399208307, 0.005884159822016954, 0.0058788699097931385, 0.005873576272279024, 0.005868298467248678, 0.005863024387508631, 0.0058577642776072025, 0.005852504167705774, 0.005847269669175148, 0.005842023994773626, 0.005836799740791321, 0.005831591319292784, 0.005826386157423258, 0.005821180995553732, 0.005816005636006594, 0.005810820963233709, 0.005805652588605881, 0.005800487939268351, 0.005795335862785578, 0.005790191702544689, 0.005785058252513409, 0.005779941566288471, 0.0057748244144022465, 0.0057697175070643425, 0.005764626432210207, 0.005759526509791613, 0.005754453130066395, 0.005749385338276625, 0.0057443189434707165, 0.005739267915487289, 0.005734228994697332, 0.005729195661842823, 0.005724175367504358, 0.005719155538827181, 0.0057141417637467384, 0.005709141492843628, 0.005704149603843689, 0.005699170753359795, 0.005694202147424221, 0.005689234007149935, 0.005684277042746544, 0.005679322872310877, 0.00567438080906868, 0.005669455975294113, 0.005664539989084005, 0.005659622605890036, 0.005654714535921812, 0.0056498064659535885, 0.005644924007356167, 0.005640046671032906, 0.005635174456983805, 0.005630312487483025, 0.005625460296869278, 0.0056206099689006805, 0.005615769419819117, 0.005610944237560034, 0.005606110207736492, 0.0056013073772192, 0.005596510600298643, 0.005591703578829765, 0.005586917977780104, 0.005582140292972326, 0.005577368661761284, 0.005572600290179253, 0.005567850545048714, 0.0055630989372730255, 0.005558360368013382, 0.005553624592721462, 0.005548907443881035, 0.005544192157685757, 0.0055394768714904785, 0.005534785334020853, 0.005530092865228653, 0.005525416228920221, 0.005520735401660204, 0.0055160666815936565, 0.005511409603059292, 0.005506751127541065, 0.005502108950167894, 0.005497486796230078, 0.005492856726050377, 0.005488222930580378, 0.005483622662723064, 0.005479016341269016, 0.005474422127008438, 0.005469826515763998, 0.005465254653245211, 0.005460682325065136, 0.0054561118595302105, 0.005451558157801628, 0.005447012837976217, 0.0054424721747636795, 0.005437937565147877, 0.0054334234446287155, 0.005428884644061327, 0.005424382630735636, 0.00541987968608737, 0.005415388382971287, 0.005410896148532629, 0.005406424403190613, 0.005401948466897011, 0.00539748091250658, 0.005393031053245068, 0.005388566758483648, 0.005384128075093031, 0.005379693582653999, 0.00537527073174715, 0.005370853003114462, 0.005366436671465635, 0.005362031050026417, 0.005357644986361265, 0.005353246815502644, 0.005348869599401951, 0.005344492848962545, 0.00534012308344245, 0.005335765890777111, 0.005331411492079496, 0.00532706780359149, 0.005322725046426058, 0.005318397656083107, 0.005314073525369167, 0.005309766624122858, 0.0053054471500217915, 0.005301150493323803, 0.005296857561916113, 0.00529256509616971, 0.005288287997245789, 0.005284017883241177, 0.005279744043946266, 0.0052754865027964115, 0.005271235015243292, 0.005266985855996609, 0.0052627455443143845, 0.005258512683212757, 0.005254288204014301, 0.005250072572380304, 0.0052458541467785835, 0.005241646897047758, 0.005237448029220104, 0.005233259405940771, 0.0052290731109678745, 0.005224897991865873, 0.0052207279950380325, 0.005216555204242468, 0.005212402902543545, 0.005208251066505909, 0.0052040982991456985, 0.005199959967285395, 0.005195829086005688, 0.005191700533032417, 0.005187581293284893, 0.0051834662444889545, 0.005179364234209061, 0.005175260826945305, 0.005171166732907295, 0.005167079158127308, 0.00516300555318594, 0.005158929619938135, 0.005154858343303204, 0.005150794517248869, 0.0051467507146298885, 0.005142691545188427, 0.005138645879924297, 0.005134621635079384, 0.005130586680024862, 0.005126559641212225, 0.005122544709593058, 0.005118531174957752, 0.0051145292818546295, 0.005110536236315966, 0.005106537137180567, 0.005102549679577351, 0.005098577588796616, 0.005094596184790134, 0.005090631078928709, 0.005086671095341444, 0.005082720424979925, 0.00507876044139266, 0.005074827466160059, 0.005070884246379137, 0.005066949874162674, 0.005063030868768692, 0.0050591095350682735, 0.0050552012398839, 0.005051286891102791, 0.005047392100095749, 0.005043498706072569, 0.005039608106017113, 0.005035726353526115, 0.00503184599801898, 0.0050279865972697735, 0.005024113692343235, 0.005020256619900465, 0.005016403738409281, 0.0050125508569180965, 0.005008709616959095, 0.0050048744305968285, 0.005001053214073181, 0.004997221753001213, 0.0049933986738324165, 0.004989601206034422, 0.004985798615962267, 0.00498198764398694, 0.00497819110751152, 0.004974403418600559, 0.0049706194549798965, 0.00496684480458498, 0.004963070619851351, 0.0049593085423111916, 0.00495555205270648, 0.004951791372150183, 0.004948044195771217, 0.004944306798279285, 0.00494055962190032, 0.0049368287436664104, 0.004933102056384087, 0.004929375369101763, 0.004925672430545092, 0.0049219559878110886, 0.004918253049254417, 0.004914553835988045, 0.00491086533293128, 0.004907181952148676, 0.0049034953117370605, 0.004899811930954456, 0.004896145313978195, 0.004892486147582531, 0.004888809751719236, 0.004885162226855755, 0.004881523549556732, 0.004877867177128792, 0.004874227102845907, 0.004870608914643526, 0.004866969771683216, 0.004863359499722719, 0.0048597343266010284, 0.004856116138398647, 0.004852514714002609, 0.004848916549235582, 0.004845325835049152, 0.004841725341975689, 0.004838145803660154, 0.004834564868360758, 0.004830995108932257, 0.004827428609132767, 0.004823857918381691, 0.004820303991436958, 0.004816750064492226, 0.004813203122466803, 0.0048096622340381145, 0.004806118551641703, 0.004802593495696783, 0.004799061454832554, 0.00479554571211338, 0.0047920201905071735, 0.004788513295352459, 0.00478500546887517, 0.004781506489962339, 0.004778003320097923, 0.004774508997797966, 0.004771034233272076, 0.004767553880810738, 0.004764073062688112, 0.004760598763823509, 0.0047571416944265366, 0.004753674380481243, 0.004750218242406845, 0.004746775142848492, 0.004743324592709541, 0.004739878699183464, 0.004736447241157293, 0.0047330111265182495, 0.004729589447379112, 0.0047261668369174, 0.0047227549366652966, 0.004719341639429331, 0.004715931136161089, 0.00471253227442503, 0.004709131550043821, 0.004705744795501232, 0.004702357575297356, 0.004698978271335363, 0.004695598967373371, 0.004692218732088804, 0.004688865039497614, 0.004685496911406517, 0.004682138096541166, 0.004678794182837009, 0.0046754442155361176, 0.0046720951795578, 0.004668761044740677, 0.00466542411595583, 0.00466209277510643, 0.004658769816160202, 0.00465544406324625, 0.00465213181450963, 0.004648829344660044, 0.004645520355552435, 0.004642218817025423, 0.004638929385691881, 0.004635631572455168, 0.004632351454347372, 0.004629063419997692, 0.004625784698873758, 0.004622514359652996, 0.004619238432496786, 0.004615978337824345, 0.004612721502780914, 0.0046094683930277824, 0.004606213420629501, 0.004602967295795679, 0.004599733278155327, 0.004596492275595665, 0.0045932685025036335, 0.004590036813169718, 0.004586820490658283, 0.00458359532058239, 0.004580385517328978, 0.004577173851430416, 0.004573964513838291, 0.004570768214762211, 0.004567568190395832, 0.004564371891319752, 0.004561194684356451, 0.004558010026812553, 0.004554837476462126, 0.004551658406853676, 0.004548491910099983, 0.004545321688055992, 0.00454216543585062, 0.004539003595709801, 0.004535853397101164, 0.004532709252089262, 0.004529571160674095, 0.004526419099420309, 0.004523288458585739, 0.00452016107738018, 0.004517039749771357, 0.004513923544436693, 0.004510801285505295, 0.004507682751864195, 0.004504579119384289, 0.00450146896764636, 0.004498376976698637, 0.004495282657444477, 0.004492189269512892, 0.004489099606871605, 0.004486015532165766, 0.004482931923121214, 0.004479861818253994, 0.004476784262806177, 0.004473729524761438, 0.004470666870474815, 0.004467600490897894, 0.0044645508751273155, 0.004461502656340599, 0.004458462353795767, 0.004455415531992912, 0.004452375695109367, 0.004449343308806419, 0.004446316976100206, 0.0044432892464101315, 0.00444027129560709, 0.004437252879142761, 0.004434248432517052, 0.004431235138326883, 0.004428233951330185, 0.004425229504704475, 0.004422229714691639, 0.004419239237904549, 0.004416257608681917, 0.004413270857185125, 0.004410282243043184, 0.004407315980643034, 0.004404345061630011, 0.004401376936584711, 0.004398414399474859, 0.004395452328026295, 0.004392491187900305, 0.004389539826661348, 0.004386590328067541, 0.00438365014269948, 0.004380711819976568, 0.004377778619527817, 0.004374845419079065, 0.004371915943920612, 0.004368987400084734, 0.004366070032119751, 0.004363157320767641, 0.004360240884125233, 0.004357333295047283, 0.004354429896920919, 0.004351525567471981, 0.0043486375361680984, 0.0043457369320094585, 0.004342850763350725, 0.004339957144111395, 0.0043370844796299934, 0.004334202501922846, 0.004331324715167284, 0.0043284534476697445, 0.00432558823376894, 0.004322721157222986, 0.004319870378822088, 0.004317014943808317, 0.00431416928768158, 0.00431131012737751, 0.0043084691278636456, 0.004305627662688494, 0.004302798304706812, 0.004299953579902649, 0.004297131672501564, 0.004294300451874733, 0.004291480407118797, 0.004288668744266033, 0.004285851493477821, 0.004283036570996046, 0.0042802379466593266, 0.0042774309404194355, 0.004274636507034302, 0.004271843936294317, 0.004269039258360863, 0.0042662532068789005, 0.004263469483703375, 0.004260695539414883, 0.004257910419255495, 0.004255139734596014, 0.004252362996339798, 0.0042496006935834885, 0.004246838856488466, 0.004244076553732157, 0.004241325426846743, 0.0042385742999613285, 0.004235823173075914, 0.004233075305819511, 0.004230329301208258, 0.004227601457387209, 0.004224870353937149, 0.0042221322655677795, 0.0042194039560854435, 0.004216685425490141, 0.0042139580473303795, 0.004211234860122204, 0.004208529368042946, 0.004205819219350815, 0.004203109070658684, 0.004200404975563288, 0.004197710659354925, 0.004195008892565966, 0.004192324820905924, 0.004189627710729837, 0.004186946898698807, 0.004184267949312925, 0.004181584808975458, 0.00417891051620245, 0.004176233895123005, 0.004173563327640295, 0.004170895088464022, 0.004168242681771517, 0.004165584221482277, 0.0041629234328866005, 0.004160268232226372, 0.004157631192356348, 0.00415497412905097, 0.004152338951826096, 0.00414970563724637, 0.00414706626906991, 0.0041444264352321625, 0.004141802899539471, 0.004139179363846779, 0.0041365595534443855, 0.00413393834605813, 0.004131323657929897, 0.004128712695091963, 0.004126102663576603, 0.004123499616980553, 0.004120893310755491, 0.004118297714740038, 0.004115699790418148, 0.004113112576305866, 0.004110523033887148, 0.004107933957129717, 0.004105350933969021, 0.004102767910808325, 0.00410020025447011, 0.004097626078873873, 0.004095055628567934, 0.004092490766197443, 0.004089930560439825, 0.004087371286004782, 0.004084801767021418, 0.00408225879073143, 0.004079708829522133, 0.004077162593603134, 0.004074613098055124, 0.004072075709700584, 0.004069541115313768, 0.004066993948072195, 0.004064470995217562, 0.0040619466453790665, 0.004059417173266411, 0.004056899342685938, 0.004054379649460316, 0.004051855765283108, 0.004049350041896105, 0.004046835005283356, 0.00404432462528348, 0.00404182355850935, 0.004039324354380369, 0.004036827478557825, 0.004034343641251326, 0.004031848162412643, 0.004029355943202972, 0.004026869311928749, 0.004024391993880272, 0.004021911416202784, 0.004019427113234997, 0.00401696003973484, 0.004014498088508844, 0.0040120212361216545, 0.004009561147540808, 0.004007095005363226, 0.0040046414360404015, 0.004002185072749853, 0.003999735694378614, 0.003997281659394503, 0.003994845785200596, 0.0039923954755067825, 0.003989954479038715, 0.003987519536167383, 0.003985094837844372, 0.003982651513069868, 0.003980229143053293, 0.003977808635681868, 0.003975386265665293, 0.003972968552261591, 0.003970547113567591, 0.003968134522438049, 0.003965719137340784, 0.003963314462453127, 0.003960914444178343, 0.003958515357226133, 0.003956112079322338, 0.003953714855015278, 0.003951329737901688, 0.003948931582272053, 0.003946543671190739, 0.003944166470319033, 0.003941773436963558, 0.003939406480640173, 0.003937033005058765, 0.0039346604607999325, 0.0039322893135249615, 0.0039299167692661285, 0.0039275577291846275, 0.003925189841538668, 0.0039228410460054874, 0.003920493647456169, 0.003918130416423082, 0.00391578720882535, 0.003913436084985733, 0.003911093343049288, 0.003908755257725716, 0.003906415309756994, 0.003904081182554364, 0.0039017510134726763, 0.0038994168862700462, 0.0038970871828496456, 0.0038947605062276125, 0.003892447566613555, 0.0038901311345398426, 0.0038878126069903374, 0.003885496873408556, 0.003883186262100935, 0.003880887059494853, 0.0038785787764936686, 0.0038762763142585754, 0.0038739782758057117, 0.003871689550578594, 0.0038693943060934544, 0.003867097431793809, 0.0038648112677037716, 0.003862527897581458, 0.003860239638015628, 0.0038579662796109915, 0.0038556938525289297, 0.0038534151390194893, 0.003851146437227726, 0.0038488840218633413, 0.0038466041442006826, 0.0038443508092314005, 0.0038420867640525103, 0.00383983226493001, 0.003837574040517211, 0.0038353290874511003, 0.003833071794360876, 0.003830827074125409, 0.003828583285212517, 0.0038263422902673483, 0.003824104554951191, 0.003821869380772114, 0.0038196356035768986, 0.003817410673946142, 0.0038151752669364214, 0.003812953596934676, 0.0038107316941022873, 0.003808504668995738, 0.0038062878884375095, 0.0038040759973227978, 0.0038018650375306606, 0.0037996619939804077, 0.0037974556908011436, 0.0037952458951622248, 0.0037930470425635576, 0.003790851216763258, 0.0037886514328420162, 0.0037864618934690952, 0.003784267930313945, 0.003782084211707115, 0.003779900958761573, 0.0037777144461870193, 0.003775539342314005, 0.003773351199924946, 0.0037711793556809425, 0.0037690079770982265, 0.0037668340373784304, 0.0037646668497473, 0.003762501524761319, 0.0037603434175252914, 0.003758183214813471, 0.0037560253404080868, 0.0037538735195994377, 0.0037517284508794546, 0.003749563591554761, 0.0037474273703992367, 0.003745279274880886, 0.003743140958249569, 0.003741000546142459, 0.0037388682831078768, 0.0037367427721619606, 0.003734600730240345, 0.0037324661388993263, 0.0037303445860743523, 0.003728220472112298, 0.0037261012475937605, 0.003723978064954281, 0.003721862332895398, 0.003719756845384836, 0.0037176429759711027, 0.003715537255629897, 0.003713421756401658, 0.0037113225553184748, 0.003709223819896579, 0.0037071171682327986, 0.0037050226237624884, 0.0037029392551630735, 0.003700842848047614, 0.003698752261698246, 0.0036966705229133368, 0.0036945862229913473, 0.0036924993619322777, 0.0036904250737279654, 0.0036883431021124125, 0.0036862650886178017, 0.003684196388348937, 0.003682132577523589, 0.0036800571251660585, 0.003677995875477791, 0.0036759248469024897, 0.0036738549824804068, 0.0036718049086630344, 0.0036697506438940763, 0.003667698707431555, 0.0036656439770013094, 0.0036636008881032467, 0.0036615515127778053, 0.003659503534436226, 0.0036574676632881165, 0.0036554252728819847, 0.0036533779930323362, 0.0036513442173600197, 0.0036493095103651285, 0.003647285746410489, 0.0036452533677220345, 0.0036432272754609585, 0.003641207702457905, 0.0036391806788742542, 0.0036371545866131783, 0.003635138040408492, 0.0036331156734377146, 0.003631112864241004, 0.003629101440310478, 0.0036271053832024336, 0.0036250886041671038, 0.003623075783252716, 0.003621078794822097, 0.003619077615439892, 0.0036170827224850655, 0.003615077119320631, 0.0036130857188254595, 0.003611101768910885, 0.0036091154906898737, 0.0036071205977350473, 0.0036051380448043346, 0.0036031478084623814, 0.003601175732910633, 0.003599202958866954, 0.0035972262267023325, 0.0035952541511505842, 0.00359327532351017, 0.003591298358514905, 0.003589337458834052, 0.003587366547435522, 0.0035854089073836803, 0.0035834566224366426, 0.003581494325771928, 0.003579525277018547, 0.0035775804426521063, 0.003575625829398632, 0.0035736828576773405, 0.003571727778762579, 0.0035697827115654945, 0.0035678374115377665, 0.003565895603969693, 0.0035639500711113214, 0.0035620250273495913, 0.003560083219781518, 0.003558150492608547, 0.0035562149714678526, 0.0035542920231819153, 0.0035523567348718643, 0.0035504368133842945, 0.0035485122352838516, 0.003546595573425293, 0.0035446779802441597, 0.0035427603870630264, 0.0035408451221883297, 0.0035389303229749203, 0.003537018783390522, 0.003535109106451273, 0.003533201292157173, 0.0035313009284436703, 0.003529398934915662, 0.003527495777234435, 0.0035255926195532084, 0.003523707389831543, 0.0035218046978116035, 0.0035199164412915707, 0.0035180291160941124, 0.003516140393912792, 0.003514248179271817, 0.003512369003146887, 0.0035104902926832438, 0.0035086097195744514, 0.00350672984495759, 0.0035048555582761765, 0.003502980573102832, 0.0035011086147278547, 0.003499237820506096, 0.0034973653964698315, 0.0034954994916915894, 0.003493637777864933, 0.0034917721059173346, 0.003489923197776079, 0.0034880610182881355, 0.003486201399937272, 0.0034843480680137873, 0.003482497064396739, 0.0034806516487151384, 0.0034788029734045267, 0.0034769601188600063, 0.003475105157122016, 0.0034732650965452194, 0.003471420146524906, 0.0034695849753916264, 0.003467750269919634, 0.0034659230150282383, 0.003464088076725602, 0.0034622522071003914, 0.003460422856733203, 0.0034585981629788876, 0.0034567713737487793, 0.0034549504052847624, 0.003453125711530447, 0.0034513070713728666, 0.0034494956489652395, 0.0034476756118237972, 0.0034458618611097336, 0.003444056725129485, 0.0034422334283590317, 0.003440435277298093, 0.0034386313054710627, 0.003436820115894079, 0.003435020800679922, 0.003433224745094776, 0.003431426826864481, 0.003429633332416415, 0.0034278377424925566, 0.0034260412212461233, 0.0034242537803947926, 0.0034224584233015776, 0.0034206712152808905, 0.0034188912250101566, 0.003417106345295906, 0.0034153240267187357, 0.00341355474665761, 0.0034117656759917736, 0.0034099863842129707, 0.0034082222264260054, 0.0034064408391714096, 0.0034046685323119164, 0.0034028973896056414, 0.003401136491447687, 0.0033993732649832964, 0.003397610504180193, 0.003395848209038377, 0.0033940915018320084, 0.0033923315349966288, 0.003390578320249915, 0.003388826036825776, 0.0033870702609419823, 0.00338532030582428, 0.003383568488061428, 0.0033818252850323915, 0.0033800797536969185, 0.003378335852175951, 0.003376592416316271, 0.003374855499714613, 0.003373126033693552, 0.0033713849261403084, 0.003369644982740283, 0.003367908298969269, 0.0033661862835288048, 0.0033644542563706636, 0.003362729912623763, 0.0033610048703849316, 0.0033592835534363985, 0.0033575627021491528, 0.003355837194249034, 0.003354121930897236, 0.003352406434714794, 0.003350687911733985, 0.003348973346874118, 0.003347266698256135, 0.003345555393025279, 0.003343846881762147, 0.0033421427942812443, 0.003340437775477767, 0.0033387348521500826, 0.003337039379402995, 0.0033353427425026894, 0.00333363632671535, 0.0033319422509521246, 0.0033302498050034046, 0.0033285534009337425, 0.0033268711995333433, 0.0033251759596168995, 0.003323489800095558, 0.0033217985183000565, 0.0033201181795448065, 0.0033184350468218327, 0.003316758666187525, 0.0033150864765048027, 0.0033133989199995995, 0.003311724402010441, 0.0033100515138357878, 0.003308382350951433, 0.0033067038748413324, 0.003305044723674655, 0.00330337998457253, 0.0033017126843333244, 0.0033000472467392683, 0.003298384603112936, 0.0032967214938253164, 0.0032950721215456724, 0.0032934173941612244, 0.0032917545177042484, 0.003290100023150444, 0.003288449952378869, 0.003286802675575018, 0.0032851500436663628, 0.003283509984612465, 0.003281861310824752, 0.0032802210189402103, 0.0032785777002573013, 0.003276936709880829, 0.0032753010746091604, 0.0032736624125391245, 0.0032720190938562155, 0.0032703836914151907, 0.0032687594648450613, 0.0032671287190169096, 0.0032655009999871254, 0.003263872815296054, 0.003262249520048499, 0.0032606327440589666, 0.003259007353335619, 0.003257378237321973, 0.003255759831517935, 0.003254141891375184]\n\n\nThe logistic regression model has been fit perfectly to the training data.\n\nfind_accuracy(X_train, y_train)\n\nAccuracy: 1.0\n\n\nSo we achieved 100% accuracy.\nThen we must initialize the test data with the same parameters as the training data.\n\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nfind_accuracy(X_test, y_test)\n\nAccuracy: 1.0\n\n\nWe can see that the logistic regression model has overfit to the training data and cannot classify the test data with the same accuracy. This it the danger of fitting a model too well to training data, as it is now not generalizable to other data."
  },
  {
    "objectID": "posts/post5/homework5.html#conclusion",
    "href": "posts/post5/homework5.html#conclusion",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "In this blog post, I was able to investigate more about gradient descent and its application in solving the empirical risk minimization problem, specifically focusing on logistic regression. By implementing gradient descent in the logistic regression model, I gained a deeper understanding of how the algorithm works and how it can be customized to suit different classification needs.\nFurthermore, I explored a key variant of gradient descent called momentum, which allows logistic regression to achieve faster convergence. Through experiments and analysis, we observed the impact of momentum on the convergence speed on the logistic regression model.\nOverall, these experiments taught me valuable lessons in optimization techniques for machine learning models. Through this experience I learned the importance of different parameters though changing the value of beta, and the impact of overfitting on our training data. By combining theory with practical implementation and experimentation, I gained a comprehensive understanding of gradient descent and its variants in the context of logistic regression.\nAs we continue to learn various machine learning algorithms and optimization techniques, the knowledge and insights gained from this blog post will help me develop practices to build more complex and efficient models in the future."
  },
  {
    "objectID": "posts/post3/homework3.html#data-prep",
    "href": "posts/post3/homework3.html#data-prep",
    "title": " Dissecting racial bias in an algorithm used to manage the health of populations ",
    "section": "1. Data Prep",
    "text": "1. Data Prep\n\npercent_five_cond = (len(df[df[\"gagne_sum_t\"] &lt;= 5])/ len(df)) * 100\npercent_five_cond\n\n95.53952115447689\n\n\n95.5% of patients in the data have five or fewer chronic conditions. Therefore we can justify focusing on these patients as they represent the vast majority of cases.\nLog-transformed the cost by created a column of the data frame that is the logarithm of the cost column. First removed all zero values as log(0) is undefined. Will use this new column as the target variable.\n\nimport numpy as np\n\ndf = df[df[\"cost_t\"] != 0]\ndf[\"log_cost\"] = np.log(df[\"cost_t\"])\n\n/var/folders/g2/ybhd80ns31sc72zl62c5qcjm0000gn/T/ipykernel_64161/702178963.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"log_cost\"] = np.log(df[\"cost_t\"])\n\n\nI created a dummy (one-hot encoded) column for the qualitative race variable in which 0 means that the patient is White and 1 means that the patient is Black.\n\ndf[\"is_black\"] = df[\"race\"] == \"black\"\n\n/var/folders/g2/ybhd80ns31sc72zl62c5qcjm0000gn/T/ipykernel_64161/3239834182.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"is_black\"] = df[\"race\"] == \"black\"\n\n\nI separate the data into predictor variables X and target variable y (the log-cost). For predictor variables, I uses the dummy columns for race and the number of active chronic conditions.\n\nX_train = df[[\"is_black\", \"gagne_sum_t\"]]\ny_train = df[\"log_cost\"]"
  },
  {
    "objectID": "posts/post3/homework3.html#modeling",
    "href": "posts/post3/homework3.html#modeling",
    "title": " Dissecting racial bias in an algorithm used to manage the health of populations ",
    "section": "2. Modeling",
    "text": "2. Modeling\nFunction that will construct data sets with polynomial features of various sizes:\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nI trained Linear Regressions on varying numbers of features from 1 to 11, comparing the scores to find that the optimal score is using 10 features: [‘is_black’, ‘gagne_sum_t’, ‘poly_1’, ‘poly_2’, ‘poly_3’, ‘poly_4’,‘poly_5’, ‘poly_6’, ‘poly_7’, ‘poly_8’, ‘poly_9’] to predict log cost.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nscore = 0\nbest_score = float('-inf')\n\nfor degrees in range(1,11):\n        \n    X_features = add_polynomial_features(X_train, degrees)\n    cols = X_features.columns\n\n    LR = LinearRegression()\n    LR.fit(X_features, y_train)\n    score = cross_val_score(LR, X_features, y_train, cv=5).mean()\n    \n    if (score &gt; best_score):\n        best_score = score\n        best_degrees = degrees\n        best_cols = cols\n\nprint(f\"{best_cols=}\") \nprint(f\"{best_degrees=}\")\n\nbest_cols=Index(['is_black', 'gagne_sum_t', 'poly_1', 'poly_2', 'poly_3', 'poly_4',\n       'poly_5', 'poly_6', 'poly_7', 'poly_8', 'poly_9'],\n      dtype='object')\nbest_degrees=10\n\n\nI then constructed a copy of the data with the correct number of polynomial features and fit a last linear regression model on the optimal features.\n\nX_features = add_polynomial_features(X_train, 10)\nLR = LinearRegression()\nLR.fit(X_features, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nI determined which coefficients corresponded to each feature of the data.\n\nfeatures = X_features.columns\ncoefs = LR.coef_\n\nfeature_coefs = dict(zip(features, coefs))\nfeature_coefs\n\n{'is_black': -0.26711487503849624,\n 'gagne_sum_t': 0.5088163229433447,\n 'poly_1': 0.5088164579853922,\n 'poly_2': -1.0305677844477263,\n 'poly_3': 0.5880247944191436,\n 'poly_4': -0.1776219992097652,\n 'poly_5': 0.03115568304493425,\n 'poly_6': -0.0032705785216849033,\n 'poly_7': 0.00020192700428754498,\n 'poly_8': -6.7458039893789564e-06,\n 'poly_9': 9.390071161850347e-08}\n\n\nI computed \\(e^{w_b}\\) to find the estimate of the cost incurred by Black patients as a percentage of white patients.\n\nimport math\n\nmath.exp(feature_coefs[\"is_black\"])\n\n0.7655851197936364\n\n\nMy model shows that black patients incur a cost of 76% of that of white patients. This supports the argument of Obermeyer et al. (2019) as it proves that less money is spent on Black patients who have the same level of need. Therefore as the algorithm uses cost as a proxy for need it thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Palmer Penguins \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Newton’s Method for Logistic Regression \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Implementing Logistic Regression \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ‘Optimal’ Decision-Making \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dissecting racial bias in an algorithm used to manage the health of populations \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Implementing the Perceptron Algorithm \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes\n\n\n\n\n\nFinal Project Blog Post\n\n\n\n\n\nMay 9, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Methods, Data Cleaning Document\n\n\n\n\n\nProcess of data cleaning.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Results, Model Creation Document\n\n\n\n\n\nProcess of model creation.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Methods, Exploratory Data Analysis\n\n\n\n\n\nVisualizing the data we used.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "warmups/warmup0411.html",
    "href": "warmups/warmup0411.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch \nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\nn_points = 100\nx = torch.rand(n_points)\ny = 1*((x + 0.3*(torch.rand(n_points) - 0.5)) &gt; 0.5 )\n\ndef plot_1d_classification_data(x, y, ax):\n    \n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(x[ix], torch.zeros_like(x[ix]), s = 40,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.6, marker = markers[i], )\n    ax.set(xlabel = r\"$x$\")\n    \nfig, ax = plt.subplots(figsize = (10, 1))\nplot_1d_classification_data(x, y, ax)\n\n\nx_space = torch.linspace(0, 1, 1001)\n\n\ndef kernel_classifier(x_space, x, y, gamma):\n    \n    # compute difference between every elem in x_space and x\n    diffs = x_space[:, None] - x\n    \n    prod = y * torch.exp(-gamma * diffs**2)\n    \n    s = torch.sum(prod, dim=1)\n    \n    return s\n\n\ns = kernel_classifier(x_space, x, y, gamma=100)\n\nfig, ax = plt.subplots(2, 1, figsize = (5, 4), height_ratios= (0.8, 0.2))\nax[0].plot(x_space, s, color = \"slategrey\")\nax[0].set(ylabel = \"Kernel score\")\nplot_1d_classification_data(x, y, ax[1])\n\n\ns = kernel_classifier(x_space, x, y, gamma=1)\n\nfig, ax = plt.subplots(2, 1, figsize = (5, 4), height_ratios= (0.8, 0.2))\nax[0].plot(x_space, s, color = \"slategrey\")\nax[0].set(ylabel = \"Kernel score\")\nplot_1d_classification_data(x, y, ax[1])\n\n\ns = kernel_classifier(x_space, x, y, gamma=10000)\n\nfig, ax = plt.subplots(2, 1, figsize = (5, 4), height_ratios= (0.8, 0.2))\nax[0].plot(x_space, s, color = \"slategrey\")\nax[0].set(ylabel = \"Kernel score\")\nplot_1d_classification_data(x, y, ax[1])"
  },
  {
    "objectID": "warmups/warmup0325.html",
    "href": "warmups/warmup0325.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef f(z):\n    return 1/(1 + np.exp(-z))\n\nz_val = np.linspace(-1, 1, 100)\n\ny = f(z_val)\n\nplt.plot(z_val, y)\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "warmups/warmup0317.html",
    "href": "warmups/warmup0317.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import math\n\ndef mystery_fun(a, epsilon, alpha, maxsteps=100):\n    \n    # start with initial guess \n    x = a\n    \n    # set x'= 0, j'=0\n    x_prime = 1\n    j = 0\n    \n    while(math.fabs(x_prime - x) &gt; epsilon):\n        \n        if j &gt; maxsteps:\n            break\n        \n        x = x_prime\n        \n        x_prime = x - alpha * (x - a/x)\n        \n        j += 1\n    return x\n        \n\n\nx_val = mystery_fun(a = 9, epsilon = 1e-8, alpha = 0.5)\nprint(x_val)\n\n\nx_val = mystery_fun(a = 9, epsilon = 1e-8, alpha = 2000000)\nprint(x_val)"
  }
]