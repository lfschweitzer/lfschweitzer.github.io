[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post3/homework3.html#data-prep",
    "href": "posts/post3/homework3.html#data-prep",
    "title": " Dissecting racial bias in an algorithm used to manage the health of populations ",
    "section": "1. Data Prep",
    "text": "1. Data Prep\n\npercent_five_cond = (len(df[df[\"gagne_sum_t\"] &lt;= 5])/ len(df)) * 100\npercent_five_cond\n\n95.53952115447689\n\n\n95.5% of patients in the data have five or fewer chronic conditions. Therefore we can justify focusing on these patients as they represent the vast majority of cases.\nLog-transformed the cost by created a column of the data frame that is the logarithm of the cost column. First removed all zero values as log(0) is undefined. Will use this new column as the target variable.\n\nimport numpy as np\n\ndf = df[df[\"cost_t\"] != 0]\ndf[\"log_cost\"] = np.log(df[\"cost_t\"])\n\n/var/folders/g2/ybhd80ns31sc72zl62c5qcjm0000gn/T/ipykernel_64161/702178963.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"log_cost\"] = np.log(df[\"cost_t\"])\n\n\nI created a dummy (one-hot encoded) column for the qualitative race variable in which 0 means that the patient is White and 1 means that the patient is Black.\n\ndf[\"is_black\"] = df[\"race\"] == \"black\"\n\n/var/folders/g2/ybhd80ns31sc72zl62c5qcjm0000gn/T/ipykernel_64161/3239834182.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"is_black\"] = df[\"race\"] == \"black\"\n\n\nI separate the data into predictor variables X and target variable y (the log-cost). For predictor variables, I uses the dummy columns for race and the number of active chronic conditions.\n\nX_train = df[[\"is_black\", \"gagne_sum_t\"]]\ny_train = df[\"log_cost\"]"
  },
  {
    "objectID": "posts/post3/homework3.html#modeling",
    "href": "posts/post3/homework3.html#modeling",
    "title": " Dissecting racial bias in an algorithm used to manage the health of populations ",
    "section": "2. Modeling",
    "text": "2. Modeling\nFunction that will construct data sets with polynomial features of various sizes:\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nI trained Linear Regressions on varying numbers of features from 1 to 11, comparing the scores to find that the optimal score is using 10 features: [‘is_black’, ‘gagne_sum_t’, ‘poly_1’, ‘poly_2’, ‘poly_3’, ‘poly_4’,‘poly_5’, ‘poly_6’, ‘poly_7’, ‘poly_8’, ‘poly_9’] to predict log cost.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nscore = 0\nbest_score = float('-inf')\n\nfor degrees in range(1,11):\n        \n    X_features = add_polynomial_features(X_train, degrees)\n    cols = X_features.columns\n\n    LR = LinearRegression()\n    LR.fit(X_features, y_train)\n    score = cross_val_score(LR, X_features, y_train, cv=5).mean()\n    \n    if (score &gt; best_score):\n        best_score = score\n        best_degrees = degrees\n        best_cols = cols\n\nprint(f\"{best_cols=}\") \nprint(f\"{best_degrees=}\")\n\nbest_cols=Index(['is_black', 'gagne_sum_t', 'poly_1', 'poly_2', 'poly_3', 'poly_4',\n       'poly_5', 'poly_6', 'poly_7', 'poly_8', 'poly_9'],\n      dtype='object')\nbest_degrees=10\n\n\nI then constructed a copy of the data with the correct number of polynomial features and fit a last linear regression model on the optimal features.\n\nX_features = add_polynomial_features(X_train, 10)\nLR = LinearRegression()\nLR.fit(X_features, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nI determined which coefficients corresponded to each feature of the data.\n\nfeatures = X_features.columns\ncoefs = LR.coef_\n\nfeature_coefs = dict(zip(features, coefs))\nfeature_coefs\n\n{'is_black': -0.26711487503849624,\n 'gagne_sum_t': 0.5088163229433447,\n 'poly_1': 0.5088164579853922,\n 'poly_2': -1.0305677844477263,\n 'poly_3': 0.5880247944191436,\n 'poly_4': -0.1776219992097652,\n 'poly_5': 0.03115568304493425,\n 'poly_6': -0.0032705785216849033,\n 'poly_7': 0.00020192700428754498,\n 'poly_8': -6.7458039893789564e-06,\n 'poly_9': 9.390071161850347e-08}\n\n\nI computed \\(e^{w_b}\\) to find the estimate of the cost incurred by Black patients as a percentage of white patients.\n\nimport math\n\nmath.exp(feature_coefs[\"is_black\"])\n\n0.7655851197936364\n\n\nMy model shows that black patients incur a cost of 76% of that of white patients. This supports the argument of Obermeyer et al. (2019) as it proves that less money is spent on Black patients who have the same level of need. Therefore as the algorithm uses cost as a proxy for need it thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care."
  },
  {
    "objectID": "posts/post1/homework1.html",
    "href": "posts/post1/homework1.html",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nTo start my exploration into the penguin data, I first sough to understand the data by plotting it\n\nimport seaborn as sns\n\n# body mass vs specie with colors for gender\n\n# replace the column with the first word in each entry\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nsns.catplot(data=train, x=\"Species\", y=\"Body Mass (g)\", hue=\"Sex\")\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis plot shows penguins as points colored by sex, with species as the x-axis and body mass (in grams) on the y-axis. This plot displays that on average, male penguins are larger than female penguins. It also shows a trend of Gentoo species of penguins larger than Chinstrap or Adelie penguins- who are relatively similar in size. This indicates that Body Mass (g) could potentially be a useful indicator in indicating species type. It is also important to note that the separation of gender allows us to make these assertions. Gento male penguins are generally larger than Adelie male penguins and the same follows with female penguins of these same two species. However, Gentoo female penguins are roughly the same size as Adelie male penguins. Therefore gender might be a useful tool in predicting penguin species.\n\nsns.catplot(data=train, kind=\"violin\", x=\"Delta 13 C (o/oo)\", y=\"Island\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis plot shows the penguins of different species with different colors graphed on Delta 13 C (o/oo) and Island location. Delta 13(o/oo) is an interesting indicator as it is widely used as an indicator of diet especially in reference to vegetation. This therefore shows that there could be a large difference in diet between Chinstrap and the other two species of penguins. Adelie penguins are present in all three islands whereas Gentoo and Chinstrap penguins are isolated to one of the three. Therefore Island could be a helpful indicator of species type, but probably would not be indicative by itself.\n\ntable = train.groupby(['Species', 'Sex']).aggregate({'Culmen Length (mm)' : ['min', 'max','mean']})\nprint(table)\n\n                 Culmen Length (mm)                 \n                                min   max       mean\nSpecies   Sex                                       \nAdelie    FEMALE               34.0  42.2  37.426415\n          MALE                 34.6  46.0  40.404918\nChinstrap FEMALE               40.9  58.0  46.722581\n          MALE                 49.0  55.8  51.334615\nGentoo    .                    44.5  44.5  44.500000\n          FEMALE               40.9  50.5  45.455102\n          MALE                 44.4  55.9  49.006818\n\n\nThis table indicates that on average male penguins have longer Culmen Length than their female counterparts. Male Gentoo penguins have longer Culmens then male Chinstraps who have longer Culmens than male Adelie penguins while the order for female penguins descends for Chinstrap, Gentoo, then Adelie. These distinctions could serve useful in training our model to differentiate species. It also serves to note that the gender distinction matters when comparing species.\n\n\n\nI then attempted to find three features of the data (one qualitative and two quantitative) and train a model on these features that achieves 100% testing accuracy.\nFirst I did data preparation to prepare the quantitative columns of the data\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nI completed an exhaustive iterative search to find the best markers. I used cross validation one each combination of one qualitative and two quantitative column with different models keeping track of the best score for each model. In terms of models I used a Logistic Regression, Decision Tree Classifier, SVC, and Random Forrest Classifier. For the Decision Tree Classifier and the Random Forrest Classifier I used grid search across the amount of parameters to identify the optimal maximum depth. For SVC I used grid search to find the optimal value for gamma across a wide array of numbers. With the best markers of each model, I also saved the columns of the best score.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\",'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)',]\n\nbestLRScore = 0.0\nbestLRCols=[]\ncol_combos = []\n\n\nbestDTCScore = 0.0\nbestDTCCols=[]\n\nbestSVMScore = 0.0\nbestSVMCols=[]\n\nbestRFCScore = 0.0\nbestRFCCols=[]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  \n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n        \n    LRScore = cross_val_score(LR, X_train[cols], y_train, cv=5).mean()\n    \n    if LRScore &gt;= bestLRScore :\n      bestLRScore = LRScore\n      bestLRCols = cols\n        \n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n    \n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCScore  = grid_search.best_score_\n        \n    if(DTCScore &gt; bestDTCScore):\n      bestDTCScore = DTCScore\n      bestDTCCols = cols\n      bestDTCDepth = grid_search.best_params_\n      \n    \n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    RFCScore  = grid_search.best_score_\n        \n    if(RFCScore &gt; bestRFCScore):\n      bestRFCScore = RFCScore\n      bestRFCCols = cols\n      bestRFCDepth = grid_search.best_params_\n    \n    #SVC\n    SVM = SVC()\n    \n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    SVMScore  = grid_search.best_score_   \n      \n    if(SVMScore &gt; bestSVMScore):\n      bestSVMScore = SVMScore\n      bestSVMCols = cols\n        \n\n  \nprint(\"best LR\",bestLRCols, \":\", bestLRScore)\nprint(\"best DTC\",bestDTCCols, \":\", bestDTCScore)\nprint(\"best max depth:\", bestDTCDepth)\nprint(\"best RFC\", bestRFCCols, \":\", bestRFCScore)\nprint(\"best max depth:\", bestRFCDepth)\nprint(\"best SVM\", bestSVMCols, \":\", bestSVMScore)\n\nbest LR ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9883107088989442\nbest DTC ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9804675716440421\nbest max depth: {'max_depth': 5}\nbest RFC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9844645550527904\nbest max depth: {'max_depth': 5}\nbest SVM ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9805429864253394\n\n\n\n\n\nNext I prepared the test data by shorting my Species column and identifying selected columns of the best fitting models- Culmen Length (mm), Culmen Depth (mm), and Sex for Logistic Regression. On my test data I achieved 100% testing accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\n\nselected_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\n\nX_train_selected = X_train[selected_cols]\nX_test_selected = X_test[selected_cols]\n\nLR = LogisticRegression()\nLR.fit(X_train_selected, y_train)\ntestScore = LR.score(X_test_selected, y_test)\n\nprint(testScore)\n\n1.0\n\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nI visualized how my model worked on the training and test data by plotting the data and displaying the decision regions of my model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n        X.columns[0] : XX,\n        X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train_selected, y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test_selected, y_test)\n\n\n\n\n\n\n\n\n\n\n\nFinally, to visualize the successful identification of my model with my three chosen characteristics I used a confusion matrix on the testing data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_selected)\nC = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo."
  },
  {
    "objectID": "posts/post1/homework1.html#introduction",
    "href": "posts/post1/homework1.html#introduction",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/post1/homework1.html#explore",
    "href": "posts/post1/homework1.html#explore",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "To start my exploration into the penguin data, I first sough to understand the data by plotting it\n\nimport seaborn as sns\n\n# body mass vs specie with colors for gender\n\n# replace the column with the first word in each entry\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nsns.catplot(data=train, x=\"Species\", y=\"Body Mass (g)\", hue=\"Sex\")\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis plot shows penguins as points colored by sex, with species as the x-axis and body mass (in grams) on the y-axis. This plot displays that on average, male penguins are larger than female penguins. It also shows a trend of Gentoo species of penguins larger than Chinstrap or Adelie penguins- who are relatively similar in size. This indicates that Body Mass (g) could potentially be a useful indicator in indicating species type. It is also important to note that the separation of gender allows us to make these assertions. Gento male penguins are generally larger than Adelie male penguins and the same follows with female penguins of these same two species. However, Gentoo female penguins are roughly the same size as Adelie male penguins. Therefore gender might be a useful tool in predicting penguin species.\n\nsns.catplot(data=train, kind=\"violin\", x=\"Delta 13 C (o/oo)\", y=\"Island\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis plot shows the penguins of different species with different colors graphed on Delta 13 C (o/oo) and Island location. Delta 13(o/oo) is an interesting indicator as it is widely used as an indicator of diet especially in reference to vegetation. This therefore shows that there could be a large difference in diet between Chinstrap and the other two species of penguins. Adelie penguins are present in all three islands whereas Gentoo and Chinstrap penguins are isolated to one of the three. Therefore Island could be a helpful indicator of species type, but probably would not be indicative by itself.\n\ntable = train.groupby(['Species', 'Sex']).aggregate({'Culmen Length (mm)' : ['min', 'max','mean']})\nprint(table)\n\n                 Culmen Length (mm)                 \n                                min   max       mean\nSpecies   Sex                                       \nAdelie    FEMALE               34.0  42.2  37.426415\n          MALE                 34.6  46.0  40.404918\nChinstrap FEMALE               40.9  58.0  46.722581\n          MALE                 49.0  55.8  51.334615\nGentoo    .                    44.5  44.5  44.500000\n          FEMALE               40.9  50.5  45.455102\n          MALE                 44.4  55.9  49.006818\n\n\nThis table indicates that on average male penguins have longer Culmen Length than their female counterparts. Male Gentoo penguins have longer Culmens then male Chinstraps who have longer Culmens than male Adelie penguins while the order for female penguins descends for Chinstrap, Gentoo, then Adelie. These distinctions could serve useful in training our model to differentiate species. It also serves to note that the gender distinction matters when comparing species."
  },
  {
    "objectID": "posts/post1/homework1.html#model",
    "href": "posts/post1/homework1.html#model",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "I then attempted to find three features of the data (one qualitative and two quantitative) and train a model on these features that achieves 100% testing accuracy.\nFirst I did data preparation to prepare the quantitative columns of the data\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nI completed an exhaustive iterative search to find the best markers. I used cross validation one each combination of one qualitative and two quantitative column with different models keeping track of the best score for each model. In terms of models I used a Logistic Regression, Decision Tree Classifier, SVC, and Random Forrest Classifier. For the Decision Tree Classifier and the Random Forrest Classifier I used grid search across the amount of parameters to identify the optimal maximum depth. For SVC I used grid search to find the optimal value for gamma across a wide array of numbers. With the best markers of each model, I also saved the columns of the best score.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\",'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)',]\n\nbestLRScore = 0.0\nbestLRCols=[]\ncol_combos = []\n\n\nbestDTCScore = 0.0\nbestDTCCols=[]\n\nbestSVMScore = 0.0\nbestSVMCols=[]\n\nbestRFCScore = 0.0\nbestRFCCols=[]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  \n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n        \n    LRScore = cross_val_score(LR, X_train[cols], y_train, cv=5).mean()\n    \n    if LRScore &gt;= bestLRScore :\n      bestLRScore = LRScore\n      bestLRCols = cols\n        \n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n    \n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCScore  = grid_search.best_score_\n        \n    if(DTCScore &gt; bestDTCScore):\n      bestDTCScore = DTCScore\n      bestDTCCols = cols\n      bestDTCDepth = grid_search.best_params_\n      \n    \n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    RFCScore  = grid_search.best_score_\n        \n    if(RFCScore &gt; bestRFCScore):\n      bestRFCScore = RFCScore\n      bestRFCCols = cols\n      bestRFCDepth = grid_search.best_params_\n    \n    #SVC\n    SVM = SVC()\n    \n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    SVMScore  = grid_search.best_score_   \n      \n    if(SVMScore &gt; bestSVMScore):\n      bestSVMScore = SVMScore\n      bestSVMCols = cols\n        \n\n  \nprint(\"best LR\",bestLRCols, \":\", bestLRScore)\nprint(\"best DTC\",bestDTCCols, \":\", bestDTCScore)\nprint(\"best max depth:\", bestDTCDepth)\nprint(\"best RFC\", bestRFCCols, \":\", bestRFCScore)\nprint(\"best max depth:\", bestRFCDepth)\nprint(\"best SVM\", bestSVMCols, \":\", bestSVMScore)\n\nbest LR ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9883107088989442\nbest DTC ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9804675716440421\nbest max depth: {'max_depth': 5}\nbest RFC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9844645550527904\nbest max depth: {'max_depth': 5}\nbest SVM ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9805429864253394"
  },
  {
    "objectID": "posts/post1/homework1.html#test-the-models-on-the-test-data.",
    "href": "posts/post1/homework1.html#test-the-models-on-the-test-data.",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Next I prepared the test data by shorting my Species column and identifying selected columns of the best fitting models- Culmen Length (mm), Culmen Depth (mm), and Sex for Logistic Regression. On my test data I achieved 100% testing accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\n\nselected_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\n\nX_train_selected = X_train[selected_cols]\nX_test_selected = X_test[selected_cols]\n\nLR = LogisticRegression()\nLR.fit(X_train_selected, y_train)\ntestScore = LR.score(X_test_selected, y_test)\n\nprint(testScore)\n\n1.0\n\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "posts/post1/homework1.html#plotting-decision-regions",
    "href": "posts/post1/homework1.html#plotting-decision-regions",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "I visualized how my model worked on the training and test data by plotting the data and displaying the decision regions of my model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n        X.columns[0] : XX,\n        X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train_selected, y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test_selected, y_test)"
  },
  {
    "objectID": "posts/post1/homework1.html#confusion-matrix",
    "href": "posts/post1/homework1.html#confusion-matrix",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Finally, to visualize the successful identification of my model with my three chosen characteristics I used a confusion matrix on the testing data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_selected)\nC = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo."
  },
  {
    "objectID": "posts/post2/hw2.html",
    "href": "posts/post2/hw2.html",
    "title": " ‘Optimal’ Decision-Making ",
    "section": "",
    "text": "In this blog post I trained a Logistic Regression function to predict the outcome of a loan. Through iterative comparison, I found that age, loan percent income, and person home ownership were the optimal features to predict loan status. Using these features to train my model, I then measured the optimal threshold by finding the highest possible gain for the bank. For these calculations I used the cost functions given by Professor Phil that depend on interest rate and loan amount. I then evaluated my model to see how the predictions changed based on different characteristics and calculated fairness based on my chosen definition.\nI found that the bank stood to gain $7931919.35735091 in total, or $1384.0375776218652 per loan. This was based on an optimal threshold of 2.857653939857805. My trained model predicted that older people and people with lower incomes were less likely to be given a loan. It also predicted default rate as highest for debt consolidation, then medical expenses, personal, education, and loan intent. Based on my choice of Error Rate Parity as a definition of model fairness, I determined that my model made fair decisions based on medical expense loan intent."
  },
  {
    "objectID": "posts/post2/hw2.html#introduction",
    "href": "posts/post2/hw2.html#introduction",
    "title": " ‘Optimal’ Decision-Making ",
    "section": "",
    "text": "In this blog post I trained a Logistic Regression function to predict the outcome of a loan. Through iterative comparison, I found that age, loan percent income, and person home ownership were the optimal features to predict loan status. Using these features to train my model, I then measured the optimal threshold by finding the highest possible gain for the bank. For these calculations I used the cost functions given by Professor Phil that depend on interest rate and loan amount. I then evaluated my model to see how the predictions changed based on different characteristics and calculated fairness based on my chosen definition.\nI found that the bank stood to gain $7931919.35735091 in total, or $1384.0375776218652 per loan. This was based on an optimal threshold of 2.857653939857805. My trained model predicted that older people and people with lower incomes were less likely to be given a loan. It also predicted default rate as highest for debt consolidation, then medical expenses, personal, education, and loan intent. Based on my choice of Error Rate Parity as a definition of model fairness, I determined that my model made fair decisions based on medical expense loan intent."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Palmer Penguins \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ‘Optimal’ Decision-Making \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dissecting racial bias in an algorithm used to manage the health of populations \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]