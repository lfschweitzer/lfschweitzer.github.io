[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Link to source code (logistic.py)\nFor this blog post I implemented logistic regression and performed several experiments on my model. The first experiment I conducted was to see vanilla gradient descent. Vanilla gradient descent is when beta=0 and we were able to know that the model works because we saw loss decrease monotonically. The second experiment was to understand the benefits of momentum. By increasing the beta value we were able to see loss decrease faster than the vanilla gradient descent case. Then, finally, we conducted an experiment to see the potential harms of overfitting our data. By reaching 100% accuracy on training data, we could see the drawback directly by a lower accuracy rate on training data. Overall, I was able to learn more about implementing machine learning models and how to test their functionality. I was able to concretely understand the benefits of momentum and the drawbacks of overfitting."
  },
  {
    "objectID": "posts/post5/homework5.html#experiments",
    "href": "posts/post5/homework5.html#experiments",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Before doing any experiments, I had to generate data for a classification problem.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nPlot the data\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# p_dims is 2\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nCode tp graph a straight line\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n\nThe first experiment that I performed was vanilla gradient descent: When p_dim = 2, when alpha is sufficiently small and beta=0.\nWant to see:\n\nGradient descent for logistic regression converges to a weight vector w that looks visually correct\n\nshow this by plot the decision boundary with the data\n\nLoss decreases monotonically: A monotonic function is a function which is either entirely nonincreasing or nondecreasing.\n\nshow this by plotting the loss over iterations\n\n\nFirst implement a training loop with graphs with a dividing line to visualize our progress.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(1000):\n    \n    # for vanilla gradient descent, alpha must be sufficiently small and beta must be 0\n    opt.step(X, y, alpha = 0.3, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n\ndef find_accuracy(X, y):\n\n    predictions = LR.predict(X)\n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nPlot the loss over time over the 1000 iterations.\n\nimport numpy as np\n\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nWe can see that the loss is decreasing monotonically over time through this graph of the loss. The negative slope shows us that the loss is in fact decreasing over time. In other words, our machine learning model is learning!\nPlot the final line separating the data\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nWe can see that the dividing line almost perfectly divides the classes. In time, we could see the logistic regression training could become perfectly accurate."
  },
  {
    "objectID": "posts/post5/homework5.html#benefits-of-momentum",
    "href": "posts/post5/homework5.html#benefits-of-momentum",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Our next experiment was to see the benefits of momentum. On the same data, gradient descent with momentum (e.g. beta=0.9) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with beta=0).\nWe want to see:\n\nA model that learns at a faster rate\n\nshow loss decreasing at a faster rate than when beta was 0\n\n\nFirst implement a training loop with graphs with a dividing line to visualize our progress.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nX, y = classification_data(noise = 0.2)\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(0,1000):\n    \n    # to see the benefits of momentum, alpha must be sufficiently small and beta must be 0.9\n    opt.step(X, y, alpha = 0.3, beta = 0.9)\n    loss = LR.loss(X, y).item()            \n    loss_vec.append(loss)\n\nPlot the loss over time over the 1000 iterations.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nWith the same number of iterations, the final loss is lower. The faster declining slope of the loss shows that the larger beta value does in fact increase the learning speed of the machine learning model.\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n\n\n\n\n\n\n\nWe can see the benefits of increase of momentum by viewing the improved dividing line. The increase in the beta value allows our logistic regression to improve at a much faster rate then when beta=0. We know that because with the same number of iterations, the loss decreased more, or in other words, the model learned to classify at a faster rate.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nA perfect accuracy rate!"
  },
  {
    "objectID": "posts/post5/homework5.html#overfitting",
    "href": "posts/post5/homework5.html#overfitting",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "Our final experiment was to show the danger of overfitting. To show this I need to generate some data where p_dim &gt; n_points and create an instance where the same logistic regression model has a 100% accuracy rate on training data.\nWant to see:\n\nPerfect accuracy for training data\nLess accurate classification for testing data with the exact same parameters\n\nFor overfitting, we need to generate data where p_dim &gt; n_points.\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nGoal to achieve 100% accuracy with the training data.\n\n# initialize a Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(0,100):\n   \n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec.append(loss)\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThe logistic regression model has been fit perfectly to the training data.\n\nfind_accuracy(X_train, y_train)\n\nAccuracy: 1.0\n\n\nSo we achieved 100% accuracy.\nThen we must initialize the test data with the same parameters as the training data.\n\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n\nfind_accuracy(X_test, y_test)\n\nAccuracy: 0.9800000190734863\n\n\nWe can see that the logistic regression model has overfit to the training data and cannot classify the test data with the same accuracy. This it the danger of fitting a model too well to training data, as it is now not generalizable to other data."
  },
  {
    "objectID": "posts/post5/homework5.html#conclusion",
    "href": "posts/post5/homework5.html#conclusion",
    "title": " Implementing Logistic Regression ",
    "section": "",
    "text": "In this blog post, I was able to investigate more about gradient descent and its application in solving the empirical risk minimization problem, specifically focusing on logistic regression. By implementing gradient descent in the logistic regression model, I gained a deeper understanding of how the algorithm works and how it can be customized to suit different classification needs.\nFurthermore, I explored a key variant of gradient descent called momentum, which allows logistic regression to achieve faster convergence. Through experiments and analysis, we observed the impact of momentum on the convergence speed on the logistic regression model.\nOverall, these experiments taught me valuable lessons in optimization techniques for machine learning models. Through this experience I learned the importance of different parameters though changing the value of beta, and the impact of overfitting on our training data. By combining theory with practical implementation and experimentation, I gained a comprehensive understanding of gradient descent and its variants in the context of logistic regression.\nAs we continue to learn various machine learning algorithms and optimization techniques, the knowledge and insights gained from this blog post will help me develop practices to build more complex and efficient models in the future."
  },
  {
    "objectID": "posts/post1/homework1.html",
    "href": "posts/post1/homework1.html",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nTo start my exploration into the penguin data, I first sough to understand the data by plotting it\n\nimport seaborn as sns\n\n# body mass vs specie with colors for gender\n\n# replace the column with the first word in each entry\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\nsns.catplot(data=train, x=\"Species\", y=\"Body Mass (g)\", hue=\"Sex\")\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis plot shows penguins as points colored by sex, with species as the x-axis and body mass (in grams) on the y-axis. This plot displays that on average, male penguins are larger than female penguins. It also shows a trend of Gentoo species of penguins larger than Chinstrap or Adelie penguins- who are relatively similar in size. This indicates that Body Mass (g) could potentially be a useful indicator in indicating species type. It is also important to note that the separation of gender allows us to make these assertions. Gento male penguins are generally larger than Adelie male penguins and the same follows with female penguins of these same two species. However, Gentoo female penguins are roughly the same size as Adelie male penguins. Therefore gender might be a useful tool in predicting penguin species.\n\nsns.catplot(data=train, kind=\"violin\", x=\"Delta 13 C (o/oo)\", y=\"Island\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThis plot shows the penguins of different species with different colors graphed on Delta 13 C (o/oo) and Island location. Delta 13(o/oo) is an interesting indicator as it is widely used as an indicator of diet especially in reference to vegetation. This therefore shows that there could be a large difference in diet between Chinstrap and the other two species of penguins. Adelie penguins are present in all three islands whereas Gentoo and Chinstrap penguins are isolated to one of the three. Therefore Island could be a helpful indicator of species type, but probably would not be indicative by itself.\n\ntable = train.groupby(['Species', 'Sex']).aggregate({'Culmen Length (mm)' : ['min', 'max','mean']})\nprint(table)\n\n                 Culmen Length (mm)                 \n                                min   max       mean\nSpecies   Sex                                       \nAdelie    FEMALE               34.0  42.2  37.426415\n          MALE                 34.6  46.0  40.404918\nChinstrap FEMALE               40.9  58.0  46.722581\n          MALE                 49.0  55.8  51.334615\nGentoo    .                    44.5  44.5  44.500000\n          FEMALE               40.9  50.5  45.455102\n          MALE                 44.4  55.9  49.006818\n\n\nThis table indicates that on average male penguins have longer Culmen Length than their female counterparts. Male Gentoo penguins have longer Culmens then male Chinstraps who have longer Culmens than male Adelie penguins while the order for female penguins descends for Chinstrap, Gentoo, then Adelie. These distinctions could serve useful in training our model to differentiate species. It also serves to note that the gender distinction matters when comparing species.\n\n\n\nI then attempted to find three features of the data (one qualitative and two quantitative) and train a model on these features that achieves 100% testing accuracy.\nFirst I did data preparation to prepare the quantitative columns of the data\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nI completed an exhaustive iterative search to find the best markers. I used cross validation one each combination of one qualitative and two quantitative column with different models keeping track of the best score for each model. In terms of models I used a Logistic Regression, Decision Tree Classifier, SVC, and Random Forrest Classifier. For the Decision Tree Classifier and the Random Forrest Classifier I used grid search across the amount of parameters to identify the optimal maximum depth. For SVC I used grid search to find the optimal value for gamma across a wide array of numbers. With the best markers of each model, I also saved the columns of the best score.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\",'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)',]\n\nbestLRScore = 0.0\nbestLRCols=[]\ncol_combos = []\n\n\nbestDTCScore = 0.0\nbestDTCCols=[]\n\nbestSVMScore = 0.0\nbestSVMCols=[]\n\nbestRFCScore = 0.0\nbestRFCCols=[]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  \n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n        \n    LRScore = cross_val_score(LR, X_train[cols], y_train, cv=5).mean()\n    \n    # keep track of best Logistic Regression Score\n    if LRScore &gt;= bestLRScore :\n      bestLRScore = LRScore\n      bestLRCols = cols\n        \n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n    \n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCScore  = grid_search.best_score_\n        \n    if(DTCScore &gt; bestDTCScore):\n      bestDTCScore = DTCScore\n      bestDTCCols = cols\n      bestDTCDepth = grid_search.best_params_\n      \n    \n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    RFCScore  = grid_search.best_score_\n    \n    # keep track of best RFC Score  \n    if(RFCScore &gt; bestRFCScore):\n      bestRFCScore = RFCScore\n      bestRFCCols = cols\n      bestRFCDepth = grid_search.best_params_\n    \n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    SVMScore  = grid_search.best_score_   \n    \n    # keep track of best SVM Score\n    if(SVMScore &gt; bestSVMScore):\n      bestSVMScore = SVMScore\n      bestSVMCols = cols\n        \n\n  \nprint(\"best LR\",bestLRCols, \":\", bestLRScore)\nprint(\"best DTC\",bestDTCCols, \":\", bestDTCScore)\nprint(\"best max depth:\", bestDTCDepth)\nprint(\"best RFC\", bestRFCCols, \":\", bestRFCScore)\nprint(\"best max depth:\", bestRFCDepth)\nprint(\"best SVM\", bestSVMCols, \":\", bestSVMScore)\n\nbest LR ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9883107088989442\nbest DTC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9765460030165913\nbest max depth: {'max_depth': 7}\nbest RFC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9844645550527904\nbest max depth: {'max_depth': 5}\nbest SVM ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9805429864253394\n\n\nFrom my iterative search through the features and the different models I was able to find that the best score was Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex. This model with these features had a 0.9883107088989442% classification rate. This made sense as I noted in my data exploration that sex was an important distinction to make when comparing different features to classify teh species.\n\n\n\nNext I prepared the test data by shorting my Species column and identifying selected columns of the best fitting models- Culmen Length (mm), Culmen Depth (mm), and Sex for Logistic Regression. On my test data I achieved 100% testing accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\n\nselected_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\n\nX_train_selected = X_train[selected_cols]\nX_test_selected = X_test[selected_cols]\n\nLR = LogisticRegression()\nLR.fit(X_train_selected, y_train)\ntestScore = LR.score(X_test_selected, y_test)\n\nprint(testScore)\n\n1.0\n\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\nI visualized how my model worked on the training and test data by plotting the data and displaying the decision regions of my model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n        X.columns[0] : XX,\n        X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train_selected, y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test_selected, y_test)\n\n\n\n\n\n\n\n\n\n\n\nFinally, to visualize the successful identification of my model with my three chosen characteristics I used a confusion matrix on the testing data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_selected)\nC = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_test_pred, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nThis is a perfect confusion matrix as the model has 100% testing accuracy in classifying the penguins. This result is exactly what we want.\n\n\n\nThis blog post taught me many things. Firstly, it allowed me to gain practice exploring and understanding data. I was able to explore the Palmer Penguin data set by creating tables and graphs. Then I was able to use the knowledge gained by the exploration to train my own classifier. I was able to become familiar with various different machine learning models through this assignment: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, and Support Vector Machines. Through this process, I learned about grid search and how to find optimal parameters. I then got practice training a Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex and finally testing it on our test data to get a 100% accuracy rate. Not only did I become more familiar with this particular data set, but I was able to learn skills in how to analyze data and build and test Machine Learning Models."
  },
  {
    "objectID": "posts/post1/homework1.html#introduction",
    "href": "posts/post1/homework1.html#introduction",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/post1/homework1.html#explore",
    "href": "posts/post1/homework1.html#explore",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Second Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post1/homework1.html#model",
    "href": "posts/post1/homework1.html#model",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "I then attempted to find three features of the data (one qualitative and two quantitative) and train a model on these features that achieves 100% testing accuracy.\nFirst I did data preparation to prepare the quantitative columns of the data\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nI completed an exhaustive iterative search to find the best markers. I used cross validation one each combination of one qualitative and two quantitative column with different models keeping track of the best score for each model. In terms of models I used a Logistic Regression, Decision Tree Classifier, SVC, and Random Forrest Classifier. For the Decision Tree Classifier and the Random Forrest Classifier I used grid search across the amount of parameters to identify the optimal maximum depth. For SVC I used grid search to find the optimal value for gamma across a wide array of numbers. With the best markers of each model, I also saved the columns of the best score.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\",'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)',]\n\nbestLRScore = 0.0\nbestLRCols=[]\ncol_combos = []\n\n\nbestDTCScore = 0.0\nbestDTCCols=[]\n\nbestSVMScore = 0.0\nbestSVMCols=[]\n\nbestRFCScore = 0.0\nbestRFCCols=[]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  \n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n        \n    LRScore = cross_val_score(LR, X_train[cols], y_train, cv=5).mean()\n    \n    # keep track of best Logistic Regression Score\n    if LRScore &gt;= bestLRScore :\n      bestLRScore = LRScore\n      bestLRCols = cols\n        \n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n    \n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCScore  = grid_search.best_score_\n        \n    if(DTCScore &gt; bestDTCScore):\n      bestDTCScore = DTCScore\n      bestDTCCols = cols\n      bestDTCDepth = grid_search.best_params_\n      \n    \n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    RFCScore  = grid_search.best_score_\n    \n    # keep track of best RFC Score  \n    if(RFCScore &gt; bestRFCScore):\n      bestRFCScore = RFCScore\n      bestRFCCols = cols\n      bestRFCDepth = grid_search.best_params_\n    \n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    \n    SVMScore  = grid_search.best_score_   \n    \n    # keep track of best SVM Score\n    if(SVMScore &gt; bestSVMScore):\n      bestSVMScore = SVMScore\n      bestSVMCols = cols\n        \n\n  \nprint(\"best LR\",bestLRCols, \":\", bestLRScore)\nprint(\"best DTC\",bestDTCCols, \":\", bestDTCScore)\nprint(\"best max depth:\", bestDTCDepth)\nprint(\"best RFC\", bestRFCCols, \":\", bestRFCScore)\nprint(\"best max depth:\", bestRFCDepth)\nprint(\"best SVM\", bestSVMCols, \":\", bestSVMScore)\n\nbest LR ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9883107088989442\nbest DTC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9765460030165913\nbest max depth: {'max_depth': 7}\nbest RFC ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)'] : 0.9844645550527904\nbest max depth: {'max_depth': 5}\nbest SVM ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)'] : 0.9805429864253394\n\n\nFrom my iterative search through the features and the different models I was able to find that the best score was Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex. This model with these features had a 0.9883107088989442% classification rate. This made sense as I noted in my data exploration that sex was an important distinction to make when comparing different features to classify teh species."
  },
  {
    "objectID": "posts/post1/homework1.html#test-the-models-on-the-test-data",
    "href": "posts/post1/homework1.html#test-the-models-on-the-test-data",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Next I prepared the test data by shorting my Species column and identifying selected columns of the best fitting models- Culmen Length (mm), Culmen Depth (mm), and Sex for Logistic Regression. On my test data I achieved 100% testing accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nX_test, y_test = prepare_data(test)\n\nselected_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\n\nX_train_selected = X_train[selected_cols]\nX_test_selected = X_test[selected_cols]\n\nLR = LogisticRegression()\nLR.fit(X_train_selected, y_train)\ntestScore = LR.score(X_test_selected, y_test)\n\nprint(testScore)\n\n1.0\n\n\n/Users/lindseyschweitzer/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result("
  },
  {
    "objectID": "posts/post1/homework1.html#plotting-decision-regions",
    "href": "posts/post1/homework1.html#plotting-decision-regions",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "I visualized how my model worked on the training and test data by plotting the data and displaying the decision regions of my model.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n        X.columns[0] : XX,\n        X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train_selected, y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test_selected, y_test)"
  },
  {
    "objectID": "posts/post1/homework1.html#confusion-matrix",
    "href": "posts/post1/homework1.html#confusion-matrix",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "Finally, to visualize the successful identification of my model with my three chosen characteristics I used a confusion matrix on the testing data.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test_selected)\nC = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_test_pred, normalize = \"true\")\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\nThis is a perfect confusion matrix as the model has 100% testing accuracy in classifying the penguins. This result is exactly what we want."
  },
  {
    "objectID": "posts/post1/homework1.html#discussion",
    "href": "posts/post1/homework1.html#discussion",
    "title": " Palmer Penguins ",
    "section": "",
    "text": "This blog post taught me many things. Firstly, it allowed me to gain practice exploring and understanding data. I was able to explore the Palmer Penguin data set by creating tables and graphs. Then I was able to use the knowledge gained by the exploration to train my own classifier. I was able to become familiar with various different machine learning models through this assignment: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, and Support Vector Machines. Through this process, I learned about grid search and how to find optimal parameters. I then got practice training a Logistic Regression with Culmen Length (mm), Culmen Depth (mm), and Sex and finally testing it on our test data to get a 100% accuracy rate. Not only did I become more familiar with this particular data set, but I was able to learn skills in how to analyze data and build and test Machine Learning Models."
  },
  {
    "objectID": "posts/post2/hw2.html",
    "href": "posts/post2/hw2.html",
    "title": " ‘Optimal’ Decision-Making ",
    "section": "",
    "text": "In this blog post I trained a Logistic Regression function to predict the outcome of a loan. Through iterative comparison, I found that age, loan percent income, and person home ownership were the optimal features to predict loan status. Using these features to train my model, I then measured the optimal threshold by finding the highest possible gain for the bank. For these calculations I used the cost functions given by Professor Phil that depend on interest rate and loan amount. I then evaluated my model to see how the predictions changed based on different characteristics and calculated fairness based on my chosen definition.\nI found that the bank stood to gain $7931919.35735091 in total, or $1384.0375776218652 per loan. This was based on an optimal threshold of 2.857653939857805. My trained model predicted that older people and people with lower incomes were less likely to be given a loan. It also predicted default rate as highest for debt consolidation, then medical expenses, personal, education, and loan intent. Based on my choice of Error Rate Parity as a definition of model fairness, I determined that my model made fair decisions based on medical expense loan intent."
  },
  {
    "objectID": "posts/post2/hw2.html#introduction",
    "href": "posts/post2/hw2.html#introduction",
    "title": " ‘Optimal’ Decision-Making ",
    "section": "",
    "text": "In this blog post I trained a Logistic Regression function to predict the outcome of a loan. Through iterative comparison, I found that age, loan percent income, and person home ownership were the optimal features to predict loan status. Using these features to train my model, I then measured the optimal threshold by finding the highest possible gain for the bank. For these calculations I used the cost functions given by Professor Phil that depend on interest rate and loan amount. I then evaluated my model to see how the predictions changed based on different characteristics and calculated fairness based on my chosen definition.\nI found that the bank stood to gain $7931919.35735091 in total, or $1384.0375776218652 per loan. This was based on an optimal threshold of 2.857653939857805. My trained model predicted that older people and people with lower incomes were less likely to be given a loan. It also predicted default rate as highest for debt consolidation, then medical expenses, personal, education, and loan intent. Based on my choice of Error Rate Parity as a definition of model fairness, I determined that my model made fair decisions based on medical expense loan intent."
  },
  {
    "objectID": "posts/post4/homework4.html",
    "href": "posts/post4/homework4.html",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "For this blog post I implemented the perceptron algorithm. I then ran several experiments to visualize the changes to my model each iteration and see the improvement of my loss. I investigated how the perceptron implementation changed when given linearly separable data vs not and 2 dimension vs more dimensional data. I also implemented mini-batch and ran experiments to see how this impacted algorithm outcomes. Through these steps, I learned the perceptron algorithm works and how that functionality can change to accommodate different data. Ultimately, I learned that the perceptron algorithm works well to address many different data forms by continuously updating the model based on misclassified points.\nLink to source code (perceptron.py)\nIn perceptron.py I implemented 5 functions: score, predict, loss, grad, and step. Here I will go through to briefly explain each function.\n\nscore: Calculate the score by taking the cross product of the data input and the weights.\npredict: Calculates y_hat where y_hat is 1 when the score is greater than or equal to 0, and 0 otherwise.\nloss: Finds loss by calculating mean of misclassified data points.\ngrad: Calculates score. If misclassified, returns gradient of cross product of data input and output \\[\\mathbb{1}\\left[s_i y_{i} &lt; 0 \\right] y_{i} \\mathbf{x}_{i}\\] Otherwise, return 0 gradient.\nstep: Calls and returns loss function. Adds gradient from grad function to model.\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptron_minibatch import Perceptron_mini, PerceptronOptimizer_mini\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nFirst, I used data from warmup to create a plot of linearly separable data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSee if perceptron.py code is functional by run the “minimal training loop” code from this section of the notes and eventually achieve loss = 0 on linearly separable data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThen I showed visualizations of the data, the separating line, and the evolution of the loss function during training.\nI copied the graph from notes in class to show the change of the loss in iterations of the perceptron algorithm.\n\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThis shows that the loss gradually decreases over the iterations and eventually reaches 0 (possible as the data is linearly separable).\nWe can then measure the accuracy of our predictions.\n\ndef find_accuracy(X, y):\n\n    predictions = p.predict(X)\n    \n    # convert predictions from {0, 1} to {-1, 1}\n    predictions = 2*predictions - 1\n    \n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nOur model has perfect accuracy.\n\n\n\nI changed class notes code to include 50 points that overlap the existing classes. This will mean that the data is not necessarily linearly separable like the data before.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data_overlap(n_points=300, noise=0.2, p_dims=2, overlap_points=50):\n   \n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # Add overlapping points within the range of existing points\n    overlap_X = torch.rand(overlap_points, p_dims) * (X.max() - X.min()) + X.min()\n    overlap_X = torch.cat((overlap_X, torch.ones((overlap_X.shape[0], 1))), 1)\n    X = torch.cat((X, overlap_X), dim=0)\n\n    # Convert y from {0, 1} to {-1, 1}\n    y = torch.cat((2 * y - 1, torch.ones(overlap_points, dtype=torch.long)))  # Label the overlapping points as class 1\n\n    return X, y\n\ndef plot_perceptron_data_overlap(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s=20, c=y[ix], facecolors=\"none\", edgecolors=\"darkgrey\", cmap=\"BrBG\", vmin=-2, vmax=2, alpha=0.5, marker=markers[i])\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data_overlap()\nplot_perceptron_data_overlap(X, y, ax)\nplt.show()\n\n\n\n\n\n\n\n\nNext, I reran the perceptron algorithm. The difference with not linearly separable data is that I cannot run the algorithm until the loss is 0 as it will never be 0 for this data. Instead, I ran the data for 1000 iterations based on Pr.Chodrow’s advice in the blog post description. I also including code to visualize model updates.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nX, y = perceptron_data_overlap()\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(0, 1000):\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # make an optimization step -- this is where the update actually happens\n    local_loss = opt.step(x_i, y_i)\n\n    # also add the new loss to loss_vec for plotting below\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nI then reran code to show the updates of the loss.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nHere we can see that the loss decreases over time but does not reach 0 like in the linearly separable case.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9628571271896362\n\n\nClose, but not perfect, accuracy.\n\n\n\nThe only difference here is that I created data with more than 5 dimensions.\n\nX, y = perceptron_data_overlap(n_points = 300, noise = 0.2, p_dims = 5)\n\nI ran the perceptron algorithm for 1000 iterations, without knowing if the data is linearly separable.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_dimen = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    local_loss = opt.step(x_i, y_i)\n    \n    if (local_loss &gt; 0):\n\n        loss = p.loss(X, y).item()\n        loss_vec_dimen.append(loss)\n        \n\nI then visualized the loss over time.\n\nplot_loss(loss_vec_dimen)\n\n\n\n\n\n\n\n\nBecause the visualization of the loss vector shows that data never fully reaches a loss of 0 after 1000 iterations, we can conclude that the data is probably not linearly separable.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9571428298950195\n\n\nClose, but not perfect, accuracy with non-linearly separable data.\n\n\n\nI then modified my perceptron.grad() method so that it accepts a submatrix of the feature matrix X of size k*p. I changed the code from:\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n\n   # if misclassified, calculate update\n    if s*y &lt;= 0:            \n        update_val = X*y\n        return update_val[0,:]\n    else:\n        return torch.zeros_like(self.w)\nto\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n    \n    # choose random learning rate\n    learning_rate = 0.3\n\n    # if misclassified, calculate update\n    misclass = s*y &lt;= 0\n    update_val_row = X*y[:,None]\n    \n    update_val = update_val_row * misclass[:,None]\n    \n    r = learning_rate * torch.mean(update_val, 0)\n    \n    return r\nThis change allowed the grad function to accept submatrices of the feature matrix X instead of just a single point.\n\n\n\nI then performed experiments and create visualizations to demonstrate the following:\nFor linearly separable data, when k = 1, mini-batch perceptron performs similarly to regular perceptron.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nLoss goes to 0 after many iterations. And accuracy is perfect.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nStill with k=1, the perceptron with mini-batch performs similarly to the normal perceptron with overlapping data.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nfor index in range(0, 1500):\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nWe can see that the perceptron reaches a loss close to, but never equal to, zero. We also have nearly perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.954285740852356\n\n\nFor linearly separable data when k = 10, mini-batch perceptron can still find a separating line in 2d.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nwhile loss &gt; 0:\n    \n    # K is 10\n    k = 10\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nHere the loss reaches zero with linearly separable data. We also have 100% accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nWhen k = n (that is, the batch size is the size of the entire data set), mini-batch perceptron can converge even when the data is not linearly separable, provided that the learning rate is small enough.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    # K is n\n    k = n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nEven with overlapping data (not linearly separable) the algorithm converges with k=n at very close to 0 loss. Close to perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9485714435577393\n\n\n\n\n\n\n\nFor a single iteration of the perceptron algorithm the dot product is taken between w and one row of the feature matrix X, as each row represents a data point. The size of the row of the feature matrix is p, the number of dimensions of the data. Therefore a single iteration’s runtime is O(p).\n\n\n\nWith the mini-batch algorithm, we take the dot product between each data point in the batch instead of just one for each iteration. Therefore it is O(kp) for each algorithm iteration.\n\n\n\n\nThrough this blog post, I learned many important lessons in Machine Learning and computing in general. Firstly, I was able to implement the perceptron algorithm and understand the steps that go into updating a machine learning model. Through my experiments, I validates that my implementation were correct. I learned how to analyze the updates of perceptron and see how the algorithm functions on different data. I developed my abilities to visualize data and use those visualizations to understand both the input data and the algorithm itself. Ultimately, I was able to successfully implement and use the perceptron algorithm for linearly separable data, non-linearly separable data, and data with more than 2 dimensions. With mini-batch, I was able to demonstrate the algorithm functioned similarly to a single point with k=1, can still find a separating line in linearly separable data with k=10, and when k=n converge even without linearly separable data."
  },
  {
    "objectID": "posts/post4/homework4.html#abstract",
    "href": "posts/post4/homework4.html#abstract",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "For this blog post I implemented the perceptron algorithm. I then ran several experiments to visualize the changes to my model each iteration and see the improvement of my loss. I investigated how the perceptron implementation changed when given linearly separable data vs not and 2 dimension vs more dimensional data. I also implemented mini-batch and ran experiments to see how this impacted algorithm outcomes. Through these steps, I learned the perceptron algorithm works and how that functionality can change to accommodate different data. Ultimately, I learned that the perceptron algorithm works well to address many different data forms by continuously updating the model based on misclassified points.\nLink to source code (perceptron.py)\nIn perceptron.py I implemented 5 functions: score, predict, loss, grad, and step. Here I will go through to briefly explain each function.\n\nscore: Calculate the score by taking the cross product of the data input and the weights.\npredict: Calculates y_hat where y_hat is 1 when the score is greater than or equal to 0, and 0 otherwise.\nloss: Finds loss by calculating mean of misclassified data points.\ngrad: Calculates score. If misclassified, returns gradient of cross product of data input and output \\[\\mathbb{1}\\left[s_i y_{i} &lt; 0 \\right] y_{i} \\mathbf{x}_{i}\\] Otherwise, return 0 gradient.\nstep: Calls and returns loss function. Adds gradient from grad function to model.\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptron_minibatch import Perceptron_mini, PerceptronOptimizer_mini\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/post4/homework4.html#implement-perceptron-on-linearly-separable-data",
    "href": "posts/post4/homework4.html#implement-perceptron-on-linearly-separable-data",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "First, I used data from warmup to create a plot of linearly separable data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nSee if perceptron.py code is functional by run the “minimal training loop” code from this section of the notes and eventually achieve loss = 0 on linearly separable data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nThen I showed visualizations of the data, the separating line, and the evolution of the loss function during training.\nI copied the graph from notes in class to show the change of the loss in iterations of the perceptron algorithm.\n\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThis shows that the loss gradually decreases over the iterations and eventually reaches 0 (possible as the data is linearly separable).\nWe can then measure the accuracy of our predictions.\n\ndef find_accuracy(X, y):\n\n    predictions = p.predict(X)\n    \n    # convert predictions from {0, 1} to {-1, 1}\n    predictions = 2*predictions - 1\n    \n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nOur model has perfect accuracy."
  },
  {
    "objectID": "posts/post4/homework4.html#implement-perceptron-on-data-that-is-not-linearly-separable",
    "href": "posts/post4/homework4.html#implement-perceptron-on-data-that-is-not-linearly-separable",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "I changed class notes code to include 50 points that overlap the existing classes. This will mean that the data is not necessarily linearly separable like the data before.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data_overlap(n_points=300, noise=0.2, p_dims=2, overlap_points=50):\n   \n    y = torch.arange(n_points) &gt;= int(n_points / 2)\n    X = y[:, None] + torch.normal(0.0, noise, size=(n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # Add overlapping points within the range of existing points\n    overlap_X = torch.rand(overlap_points, p_dims) * (X.max() - X.min()) + X.min()\n    overlap_X = torch.cat((overlap_X, torch.ones((overlap_X.shape[0], 1))), 1)\n    X = torch.cat((X, overlap_X), dim=0)\n\n    # Convert y from {0, 1} to {-1, 1}\n    y = torch.cat((2 * y - 1, torch.ones(overlap_points, dtype=torch.long)))  # Label the overlapping points as class 1\n\n    return X, y\n\ndef plot_perceptron_data_overlap(X, y, ax):\n    targets = [-1, 1]\n    markers = [\"o\", \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix, 0], X[ix, 1], s=20, c=y[ix], facecolors=\"none\", edgecolors=\"darkgrey\", cmap=\"BrBG\", vmin=-2, vmax=2, alpha=0.5, marker=markers[i])\n    ax.set(xlabel=r\"$x_1$\", ylabel=r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data_overlap()\nplot_perceptron_data_overlap(X, y, ax)\nplt.show()\n\n\n\n\n\n\n\n\nNext, I reran the perceptron algorithm. The difference with not linearly separable data is that I cannot run the algorithm until the loss is 0 as it will never be 0 for this data. Instead, I ran the data for 1000 iterations based on Pr.Chodrow’s advice in the blog post description. I also including code to visualize model updates.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nX, y = perceptron_data_overlap()\n\n# initialize for main loop\nloss_vec = []\n\nfor index in range(0, 1000):\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # make an optimization step -- this is where the update actually happens\n    local_loss = opt.step(x_i, y_i)\n\n    # also add the new loss to loss_vec for plotting below\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nI then reran code to show the updates of the loss.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nHere we can see that the loss decreases over time but does not reach 0 like in the linearly separable case.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9628571271896362\n\n\nClose, but not perfect, accuracy."
  },
  {
    "objectID": "posts/post4/homework4.html#implement-perceptron-on-data-with-more-than-2-dimensions",
    "href": "posts/post4/homework4.html#implement-perceptron-on-data-with-more-than-2-dimensions",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "The only difference here is that I created data with more than 5 dimensions.\n\nX, y = perceptron_data_overlap(n_points = 300, noise = 0.2, p_dims = 5)\n\nI ran the perceptron algorithm for 1000 iterations, without knowing if the data is linearly separable.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_dimen = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    local_loss = opt.step(x_i, y_i)\n    \n    if (local_loss &gt; 0):\n\n        loss = p.loss(X, y).item()\n        loss_vec_dimen.append(loss)\n        \n\nI then visualized the loss over time.\n\nplot_loss(loss_vec_dimen)\n\n\n\n\n\n\n\n\nBecause the visualization of the loss vector shows that data never fully reaches a loss of 0 after 1000 iterations, we can conclude that the data is probably not linearly separable.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9571428298950195\n\n\nClose, but not perfect, accuracy with non-linearly separable data."
  },
  {
    "objectID": "posts/post4/homework4.html#implement-minibatch",
    "href": "posts/post4/homework4.html#implement-minibatch",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "I then modified my perceptron.grad() method so that it accepts a submatrix of the feature matrix X of size k*p. I changed the code from:\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n\n   # if misclassified, calculate update\n    if s*y &lt;= 0:            \n        update_val = X*y\n        return update_val[0,:]\n    else:\n        return torch.zeros_like(self.w)\nto\ndef grad(self, X, y): # should correctly return the “update” part of the perceptron update\n    s = self.score(X)\n    \n    # choose random learning rate\n    learning_rate = 0.3\n\n    # if misclassified, calculate update\n    misclass = s*y &lt;= 0\n    update_val_row = X*y[:,None]\n    \n    update_val = update_val_row * misclass[:,None]\n    \n    r = learning_rate * torch.mean(update_val, 0)\n    \n    return r\nThis change allowed the grad function to accept submatrices of the feature matrix X instead of just a single point."
  },
  {
    "objectID": "posts/post4/homework4.html#minibatch-perceptron-experiments",
    "href": "posts/post4/homework4.html#minibatch-perceptron-experiments",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "I then performed experiments and create visualizations to demonstrate the following:\nFor linearly separable data, when k = 1, mini-batch perceptron performs similarly to regular perceptron.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nLoss goes to 0 after many iterations. And accuracy is perfect.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nStill with k=1, the perceptron with mini-batch performs similarly to the normal perceptron with overlapping data.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nfor index in range(0, 1500):\n    \n    # K is 1\n    k = 1\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nWe can see that the perceptron reaches a loss close to, but never equal to, zero. We also have nearly perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.954285740852356\n\n\nFor linearly separable data when k = 10, mini-batch perceptron can still find a separating line in 2d.\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nwhile loss &gt; 0:\n    \n    # K is 10\n    k = 10\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nHere the loss reaches zero with linearly separable data. We also have 100% accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 1.0\n\n\nWhen k = n (that is, the batch size is the size of the entire data set), mini-batch perceptron can converge even when the data is not linearly separable, provided that the learning rate is small enough.\n\nX, y = perceptron_data_overlap()\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    # K is n\n    k = n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\n\nplot_loss(loss_vec_mini)\n\n\n\n\n\n\n\n\nEven with overlapping data (not linearly separable) the algorithm converges with k=n at very close to 0 loss. Close to perfect accuracy.\n\nfind_accuracy(X, y)\n\nAccuracy: 0.9485714435577393"
  },
  {
    "objectID": "posts/post4/homework4.html#discussion-question",
    "href": "posts/post4/homework4.html#discussion-question",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "For a single iteration of the perceptron algorithm the dot product is taken between w and one row of the feature matrix X, as each row represents a data point. The size of the row of the feature matrix is p, the number of dimensions of the data. Therefore a single iteration’s runtime is O(p).\n\n\n\nWith the mini-batch algorithm, we take the dot product between each data point in the batch instead of just one for each iteration. Therefore it is O(kp) for each algorithm iteration."
  },
  {
    "objectID": "posts/post4/homework4.html#conclusion",
    "href": "posts/post4/homework4.html#conclusion",
    "title": " Implementing the Perceptron Algorithm ",
    "section": "",
    "text": "Through this blog post, I learned many important lessons in Machine Learning and computing in general. Firstly, I was able to implement the perceptron algorithm and understand the steps that go into updating a machine learning model. Through my experiments, I validates that my implementation were correct. I learned how to analyze the updates of perceptron and see how the algorithm functions on different data. I developed my abilities to visualize data and use those visualizations to understand both the input data and the algorithm itself. Ultimately, I was able to successfully implement and use the perceptron algorithm for linearly separable data, non-linearly separable data, and data with more than 2 dimensions. With mini-batch, I was able to demonstrate the algorithm functioned similarly to a single point with k=1, can still find a separating line in linearly separable data with k=10, and when k=n converge even without linearly separable data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "The Palmer Penguins data set was collected by Dr. Kristen Gorman and the Palmer Station, Anartica LTER, a member of the Long Term Ecological Research Network. It contains physiological measurements from three species of penguins and is widely used in data analysis. My goal with homework 1 is to identify three characteristics (two quantitative and one qualitative) and a model, which I then train, to identify penguin species with 100% testing accuracy.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  }
]